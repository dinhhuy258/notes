{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>The repository for sharing the knowledge of computer science</p>"},{"location":"algorithms/articulation_points_and_bridges/","title":"Articulation Points and Bridges","text":""},{"location":"algorithms/articulation_points_and_bridges/#1-articulation-point","title":"1. Articulation Point","text":"<p>In a graph, a vertex is called an articulation point if removing it and all the edges associated with it results in the increase of the number of connected components in the graph</p> <p></p> <p>Algorithm Steps:</p> <ul> <li>Pick an arbitrary vertex of the graph root and run DFS from it.</li> <li>For each node maintain two time values:</li> <li><code>disc</code>: The time in which the node was first reached</li> <li><code>low</code>: The low time of a node is the lowest discovery time of all of its adjacent nodes</li> <li>A node is an articulation point if satisfy any of the following properties:</li> <li>If node is root node and node has 2 children</li> <li>If node's low time is lower than the low time of all other adjacent nodes (using <code>lows</code>)</li> </ul> <pre><code>int n; // number of nodes\nvector&lt;vector&lt;int&gt;&gt; adj; // adjacency list of graph\n\nvector&lt;bool&gt; visited;\nvector&lt;int&gt; disc, low;\nint timer;\n\nvoid dfs(int v, int p = -1) {\n  visited[v] = true;\n  disc[v] = low[v] = timer++;\n  int children = 0;\n  for (int to : adj[v]) {\n    if (to == p) {\n      continue;\n    }\n\n    if (visited[to]) {\n      low[v] = min(low[v], disc[to]);\n    } else {\n      dfs(to, v);\n      low[v] = min(low[v], low[to]);\n\n      if (low[to] &gt;= disc[v] &amp;&amp; p != -1) {\n        IS_CUTPOINT(v);\n      }\n\n      ++children;\n    }\n  }\n  if (p == -1 &amp;&amp; children &gt; 1) {\n    IS_CUTPOINT(v);\n  }\n}\n\nvoid find_cutpoints() {\n  timer = 0;\n  visited.assign(n, false);\n  disc.assign(n, -1);\n  low.assign(n, -1);\n\n  for (int i = 0; i &lt; n; ++i) {\n    if (!visited[i]) {\n      dfs(i);\n    }\n  }\n}\n</code></pre> <p>The time complexity of the algorithm is O(E + V).</p>"},{"location":"algorithms/articulation_points_and_bridges/#2-bridge","title":"2. Bridge","text":"<p>An edge in a graph between vertices u and v is called a Bridge, if after removing it, there will be no path left between  u and v.</p> <p></p> <p>Algorithm Steps:</p> <pre><code>int n; // number of nodes\nvector&lt;vector&lt;int&gt;&gt; adj; // adjacency list of graph\n\nvector&lt;bool&gt; visited;\nvector&lt;int&gt; disc, low;\nint timer;\n\nvoid dfs(int v, int p = -1) {\n  visited[v] = true;\n  disc[v] = low[v] = timer++;\n  for (int to : adj[v]) {\n    if (to == p) {\n      continue;\n    }\n\n    if (visited[to]) {\n      low[v] = min(low[v], disc[to]);\n    } else {\n      dfs(to, v);\n      low[v] = min(low[v], low[to]);\n\n      if (low[to] &gt; disc[v]) {\n        IS_BRIDGE(v, to);\n      }\n    }\n  }\n}\n\nvoid find_bridges() {\n  timer = 0;\n  visited.assign(n, false);\n  disc.assign(n, -1);\n  low.assign(n, -1);\n\n  for (int i = 0; i &lt; n; ++i) {\n    if (!visited[i]) {\n      dfs(i);\n    }\n  }\n}\n</code></pre>"},{"location":"algorithms/binary_indexed_tree/","title":"Binary Indexed Tree (Fenwick Tree)","text":""},{"location":"algorithms/binary_indexed_tree/#problem-statement","title":"Problem statement","text":"<p>Given an array v, we have to evaluate two operations:</p> <ul> <li>query(l, r): Find the sum of all elements in a given interval [l, r].</li> <li>update(i, val): Change the value of position i of the array to val.</li> </ul> <p>The naive solution would take O(1) time complexity for an update operation and O(n) time complexity for a query operation. Another naive solution is to use prefix sum would take O(1) time complexity for a query operation and O(n) time complexity for an update operation (rebuild prefix sum array).</p> <p>The first solution has a good update but a terrible query. The second one has a good query but a terrible update.</p>"},{"location":"algorithms/binary_indexed_tree/#binary-indexed-tree","title":"Binary Indexed Tree","text":"<p>A Fenwick tree or binary indexed tree is a data structure that can efficiently update elements and calculate prefix sums in a table of numbers.</p>"},{"location":"algorithms/binary_indexed_tree/#isolating-the-last-set-bit","title":"Isolating the last set bit","text":"<p>Before moving to the concept of the binary indexed tree, we need to find the solution to retrieve the last set bit which will help us in binary indexed tree implementation later.</p> <p>For example a number x = (1110)\u2082 then the last set bit should be (0010)\u2082</p> <p>How to isolate?</p> <p><code>x &amp; (-x)</code> gives the last set bit in a number x, but how?</p> <p>Let\u2019s say x = (a1b)\u2082 is the number whose last set bit we want to isolate.</p> <p>Here a is some binary sequence of any length of 1\u2019s and 0\u2019s and b is some sequence of any length but of 0\u2019s only</p> <p>-x = 2\u2019s complement of x = (a1b)\u2019 + 1 = a\u20190b\u2019 + 1 = a\u20190(0\u2026.0)\u2019 + 1 = a\u20190(1...1) + 1 = a\u20191(0\u20260) = a\u20191b</p> <p><code>x &amp; (-x)</code> = (a1b) &amp; (a'1b)</p> <p>We have a &amp; a' = 0 and b contains zero bits only then the result of <code>x &amp; (-x)</code> will isolate the last bit of <code>x</code></p>"},{"location":"algorithms/binary_indexed_tree/#the-conception-of-the-tree","title":"The conception of the tree","text":"<p>In Fenwick tree the root node has the sum of the entire array. Its left child has the sum of the first half and the right child of the second half.</p> <p></p> <p>How can we use that new structure to sum the first n elements?</p> <p></p> <p>As you can see in the image above, the even indices is completely not used then we can ignore these indices.</p> <p></p>"},{"location":"algorithms/binary_indexed_tree/#query-in-the-tree","title":"Query in the tree","text":"<p>For example, we want to calculate prefix sum from index 1 to index 13 (For sake of simplicity we will use 1-indexed array)</p> <p>13 = (1101)\u2082 = 8 + 4 + 0 + 1. This structure divides the sum of first 13 elements into 3 blocks.</p> <p></p> <p>Generally, each set bit will be responsible for one part of the summation.</p> <p>In our example, to sum up the first 13 = (1101)\u2082 elements, we sum the values on position 13 = (1101)\u2082, 12 = (1100)\u2082, and 8= (1000)\u2082.</p> <p>A cleaner way to look at the construction we built</p> <p></p>"},{"location":"algorithms/binary_indexed_tree/#update-in-the-tree","title":"Update in the tree","text":"<p>For the update operation, we need to change the value of the tree from the left node to all nodes above it.</p> <p></p> <p>To define those concepts better, we will need to understand how we can walk on the tree. Let represent the array as tree-shape structure.</p> <p></p> <p>By looking closely into the tree. We can notice that all nodes in the same level having the same suffix. The prefix merely indicates the order of that index in the level. In other words, the prefixes of the same level form the sequence 0, 1, 2, 3\u2026</p> <p>The Fenwick Tree arranges binary numbers in two dimensions. Assuming we always start at the root, it makes perfect sense to represent one digit as <code>go to the left</code> and the other as <code>go to the right</code>.</p> <p></p> <p>For now everything is clear, let move on to update operation in binary indexed tree.</p> <p>Let <code>A</code> and <code>B</code> be two nodes in the tree, if len(prefix(A)) &lt; len(prefix(B)) then <code>A</code> is an ancestor of <code>B</code>.</p> <p>For example:</p> <ul> <li>6=(001|10)\u2082 is an ancestor of 5=(0010|1)\u2082</li> <li>7=(0011|1)\u2082, and 11=(0101|1)\u2082 is a decedent of 10=(010|10)\u2082, 12=(01|100)\u2082, 8=(0|1000)\u2082</li> </ul> <p>To update a value of the array, we need to move from a given node B to node A immediately above it. Although not intuitive, this is precisely the sum of an index with its least significant bit.</p> <p></p>"},{"location":"algorithms/binary_indexed_tree/#implementation","title":"Implementation","text":"<pre><code>class FenwickTree {\nprivate:\n    std::vector&lt;int&gt; nodes;\n\npublic:\n    FenwickTree(int size): nodes(size + 1, 0) {\n    }\n\n    void update(int i) {\n        for (; i &lt; nodes.size(); i += i &amp; -i) {\n            nodes[i] += 1;\n        }\n    }\n\n    int query(int i) {\n        int result = 0;\n        for (; i &gt; 0; i -= i &amp; -i) {\n            result += nodes[i];\n        }\n        return result;\n    }\n};\n</code></pre>"},{"location":"algorithms/binary_search/","title":"Binary search","text":"<p>Binary Search is a searching algorithm for finding an element's position in a sorted array.</p> <p>Binary search compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array.</p> <p></p> <p>Algorithm</p> <pre><code>int binarySearch(std::vector&lt;int&gt; array, int x) {\n  int low = 0;\n  int high = array.size() - 1;\n\n  while (low &lt;= high) {\n    int mid = (low + high) / 2;\n\n    if (array[mid] == x) {\n      return mid;\n    } else if (array[mid] &lt; x) {\n      low = mid + 1;\n    } else {\n      high = mid - 1;\n    }\n  }\n\n  // Not found\n  return -1;\n}\n</code></pre>"},{"location":"algorithms/binary_search/#examples","title":"Examples","text":""},{"location":"algorithms/binary_search/#find-first-and-last-position-of-element-in-sorted-array","title":"Find First and Last Position of Element in Sorted Array","text":"<p>Problem</p> <p>Approach</p> <p>In this problem, we'll search for x as we are searching normally. When we find the element, then there are 2 solutions:</p> <ul> <li>To find the first position, we keep searching for element in the left part of the array</li> <li>To find the last position, we keep searching for element in the right part of the array</li> </ul> <pre><code>int findPosition(std::vector&lt;int&gt; &amp;nums, int target, bool findFirst) {\n  int leftIdx = 0;\n  int rightIdx = nums.size() - 1;\n  int position = NOT_FOUND;\n  while (leftIdx &lt;= rightIdx) {\n    int midIdx = (leftIdx + rightIdx) / 2;\n    if (nums[midIdx] == target) {\n      position = midIdx;\n      if (findFirst) {\n        // Continue to find in the left part\n        rightIdx = midIdx - 1;\n      } else {\n        // Continue to find in the right part\n        leftIdx = midIdx + 1;\n      }\n    } else if (nums[midIdx] &gt; target) {\n      rightIdx = midIdx - 1;\n    } else {\n      leftIdx = midIdx + 1;\n    }\n  }\n\n  return position;\n}\n</code></pre>"},{"location":"algorithms/binary_search/#find-floor-ceil-in-a-sorted-array","title":"Find floor/ ceil in a sorted array","text":"<p>floor(arr, x): Return the largest integer in array <code>arr</code> that is smaller or equal to <code>x</code></p> <p>Problem</p> <pre><code>int findFloor(vector&lt;int&gt; arr, int target) {\n  int lowArrIdx = 0;\n  int highArrIdx = arr.size() - 1;\n  int floorIdx = -1;\n  while (lowArrIdx &lt;= highArrIdx) {\n    int midArrIdx = (lowArrIdx + highArrIdx) / 2;\n    if (arr[midArrIdx] &lt;= target) {\n      floorIdx = midArrIdx;\n      lowArrIdx = midArrIdx + 1;\n    } else {\n      // arr[midArrIdx] &gt; target\n      highArrIdx = midArrIdx - 1;\n    }\n  }\n\n  return floorIdx;\n}\n</code></pre> <p>ceil(arr, x): Return the smallest integer in array <code>arr</code> that is greater or equal to <code>x</code></p> <pre><code>int findCeil(vector&lt;int&gt; arr, int target) {\n  int lowArrIdx = 0;\n  int highArrIdx = arr.size() - 1;\n  int ceilIdx = -1;\n  while (lowArrIdx &lt;= highArrIdx) {\n    int midArrIdx = (lowArrIdx + highArrIdx) / 2;\n    if (arr[midArrIdx] &gt;= target) {\n      ceilIdx = midArrIdx;\n      highArrIdx = midArrIdx - 1;\n    } else {\n      // arr[midArrIdx] &lt; target\n      lowArrIdx = midArrIdx + 1;\n    }\n  }\n\n  return ceilIdx;\n}\n</code></pre>"},{"location":"algorithms/binary_search/#finding-the-peak-of-an-array","title":"Finding the peak of an array","text":"<p>Problem</p> <pre><code>int findPeakElement(vector&lt;int&gt;&amp; nums) {\n  int leftNumsIdx = 0;\n  int rightNumsIdx = nums.size() - 1;\n\n  while (leftNumsIdx &lt;= rightNumsIdx) {\n    int midNumsIdx = (leftNumsIdx + rightNumsIdx) / 2;\n    if (midNumsIdx == 0 || nums[midNumsIdx - 1] &lt; nums[midNumsIdx]) {\n      if (midNumsIdx == nums.size() - 1 || nums[midNumsIdx + 1] &lt; nums[midNumsIdx]) {\n          return midNumsIdx;\n      }\n\n      // We can find answer in the right part\n      // We have a contrain nums[n] = -INF\n      leftNumsIdx = midNumsIdx + 1;\n    } else {\n      // nums[midNumsIdx - 1] &gt; nums[midNumsIdx]\n      rightNumsIdx = midNumsIdx - 1;\n    }\n  }\n\n  // Not reachable\n  return -1;\n}\n</code></pre>"},{"location":"algorithms/bloom_filter/","title":"Bloom filter","text":""},{"location":"algorithms/bloom_filter/#what-is-bloom-filter","title":"What is Bloom filter","text":"<p>A Bloom filter is a data structure designed to tell you, rapidly and memory-efficiently, whether an element is present in a set.</p> <p>The price paid for this efficiency is that a Bloom filter is a probabilistic data structure: it tells us that the element either definitely is not in the set or may be in the set.</p> <p>For example is x in the list?</p> <p>Yes (90% correct) No (100% correct)</p>"},{"location":"algorithms/bloom_filter/#how-does-a-bloom-filter-work","title":"How does a Bloom filter work?","text":"<p>An empty Bloom filter is a Bit Vector with all bits set to zero. In the image below, each cell represents a bit. The number below the bit is its index for a 10-bit vector.</p> <p></p> <p>In order to add an element, it must be hashed using multiple hash functions. Bits are set at the index of the hashes in the bit vector.</p> <p>For example, let\u2019s assume we need to add the james@gmail.com element using three efficient hash functions:</p> <ul> <li>H1(james@gmail.com) = 12021</li> <li>H2(james@gmail.com) = 23324</li> <li>H3(james@gmail.com) = 23237</li> </ul> <p>We can take the mod of 10 for all these values to get an index within the bounds of the bit vector:</p> <ul> <li>12021 % 10 = 1</li> <li>23324 % 10 = 4</li> <li>23237 % 10 = 7</li> </ul> <p></p> <p>For an item whose membership needs to be tested, it is also hashed via the same hash functions. If all the bits are already set for this, the element may exist in the set.</p> <p>If any bit is not set, the element is definitely not in the set.</p> <p></p>"},{"location":"algorithms/bloom_filter/#why-bloom-filters-give-false-positive-results","title":"Why Bloom filters give false positive results?","text":"<p>Let\u2019s assume we have added the two members below to the bloom filter.</p> <ul> <li>Monkey with Hash Output H(\u201cMonkey\u201d) = {1,2,5}</li> <li>Lion with Hash Output H(\u201cLion\u201d) = {7,4,3}</li> </ul> <p>Now, if we want to check whether or not Tiger exists in the set, we can hash it via the same hash functions.</p> <ul> <li>H(\u201cTiger\u201d) = {2,7,3}</li> </ul> <p>We have not added \u201cTiger\u201d to the bloom filter, but all the bits at index {2,7,3} have already been set by the previous two elements; thus, Bloom Filter claims that \u201cTiger\u201d is present in the set. This is a false positive result.</p>"},{"location":"algorithms/bloom_filter/#bloom-filter-applications","title":"Bloom filter applications","text":"<ul> <li>Medium uses Bloom filters in its Recommendation module to avoid showing those posts that have already been seen by the user.</li> <li>Cassandra uses bloom filters to optimize the search of data in an SSTable on the disk.</li> <li>CDNs use bloom filters to avoid caching items that are rarely searched.</li> </ul>"},{"location":"algorithms/bucket_sort/","title":"Bucket sort","text":"<p>Bucket sort is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using different sorting algorithm.</p> <p>Bucket sort is mainly useful when input is uniformly distributed over a range.</p>"},{"location":"algorithms/bucket_sort/#1-how-does-bucket-sort-work","title":"1. How does bucket sort work?","text":""},{"location":"algorithms/bucket_sort/#2-pseudocode","title":"2. Pseudocode","text":"<p>Step 1: Create <code>n</code> empty buckets</p> <p>Step 2: Place every element in their corresponding bucket</p> <p>Step 3: Sort individual buckets (Using: quick sort or insertion sort)</p> <p>Step 4: Merge all the bucket</p>"},{"location":"algorithms/bucket_sort/#3-example","title":"3. Example","text":""},{"location":"algorithms/bucket_sort/#sort-a-large-set-of-floating-point-numbers-which-are-in-range-from-00-to-10-and-are-uniformly-distributed-across-the-range","title":"Sort a large set of floating point numbers which are in range from 0.0 to 1.0 and are uniformly distributed across the range.","text":"<pre><code>void bucketSort(std::vector&lt;float&gt;&amp; arr, int n) {\n    // Create n empty buckets\n    std::vector&lt;std::vector&lt;float&gt;&gt; buckets;\n    buckets.resize(n);\n\n    // Put array elements in different buckets\n    for (int i = 0; i &lt; arr.size(); ++i) {\n        int bucketIndex = arr[i] * n; // the value is in range from 0.0 to 1.0\n        buckets[i].push_back(arr[i]);\n    }\n\n    // Sort individual buckets\n    for (int i = 0; i &lt; n; ++i) {\n        std::sort(buckets[i].begin(), buckets[i].end());\n    }\n\n    // Concatenate all buckets into arr[]\n    int index = 0;\n    for (int i = 0; i &lt; n; ++i) {\n        for (int j = 0; j &lt; buckets[i].size(); ++j) {\n            arr[index++] = bucket[i][j];\n        }\n    }\n}\n</code></pre>"},{"location":"algorithms/bucket_sort/#bucket-sort-for-numbers-having-integer-part","title":"Bucket sort for numbers having integer part:","text":"<p>Step 1: Find maximum and minimum elements of the array</p> <p>Step 2: Calculate the range of each bucket</p> <pre><code>auto range = (max - min) / n; // n is the number of buckets\n</code></pre> <p>Step 3: Create n buckets of calculated range</p> <p>Step 4: Scatter the array elements to these buckets</p> <pre><code>auto bucketIndex = (arr[i] - min) / range;\n</code></pre> <p>Step 5: Sort each bucket individually</p> <p>Step 6: Gather the sorted elements from buckets to original array</p>"},{"location":"algorithms/bucket_sort/#4-time-complexity","title":"4. Time complexity","text":"Time Complexity Best O(n + k) Worst O(n<sup>2</sup>) Average O(n) Space Complexity O(n + k) <p>Where k is the number of buckets</p>"},{"location":"algorithms/huffman_coding/","title":"Huffman coding","text":"<p>Huffman Coding is a technique of compressing data to reduce its size without losing any of the details.</p> <p>Huffman Coding is generally useful to compress the data in which there are frequently occurring characters.</p>"},{"location":"algorithms/huffman_coding/#1-how-huffman-coding-works","title":"1. How huffman coding works","text":"<p>Suppose the following stringis to be sent over a network: BCAADDDCCACACAC</p> <p>Each character occupies <code>8</code> bits. There are a total of <code>15</code> characters in the above string. Thus, a total of <code>8 * 15 = 120</code> bits are required to send this string.</p> <p>Using the Huffman Coding technique, we can compress the string to a smaller size.</p> <p>Huffman coding is done with the help of the following steps.</p> <p>Step 1: Calculate the frequency of each character in the string.</p> B C A D 1 6 5 3 <p>Step 2: Create a leaf node for each unique character and sort all nodes in increasing order of the frequency. We can store these nodes in a min heap.</p> B D A C 1 3 5 6 <pre><code>[B - 1] [D - 3] [A - 5] [C - 6]\n</code></pre> <p>Step 3: Extract 2 nodes with the minimum frequency from the min heap, create a new internode with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap.</p> * A C 4 5 6 <pre><code>    [* - 4]\n    /     \\\n[B - 1] [D - 3] [A - 5] [C - 6]\n</code></pre> <p>Step 4: Repeat step 2 and step 3 util the min heap contains only one node. The remaining node is the root node and the tree is complete.</p> C * 6 9 <pre><code>         [* - 9]\n        /       \\\n    [* - 4]      \\\n    /     \\       \\\n[B - 1] [D - 3] [A - 5] [C - 6]\n</code></pre> * 15 <pre><code>              [* - 15]\n              /      \\\n          [C - 6]     [* - 9]\n                     /       \\\n                 [* - 4]      \\\n                 /     \\       \\\n             [B - 1] [D - 3] [A - 5]\n</code></pre> <p>Step 5: For each non-leaf node, assign 0 to the left edge and 1 to the right edge</p> <pre><code>              [* - 15]\n              /      \\\n             0        1\n            /          \\\n          [C - 6]     [* - 9]\n                      /     \\\n                     0       \\\n                    /         \\\n                 [* - 4]       1\n                  /    \\        \\\n                 0      1        \\\n                /        \\        \\\n             [B - 1] [D - 3] [A - 5]\n</code></pre> <p>For sending the above string over a network, we have to send the tree as well as the above compressed-code. The total size is given by the table below.</p> Character Frequency Code Size A 5 11 5 * 2 = 10 B 1 100 1 * 3 = 3 C 6 0 6 *1 = 6 D 3 101 3 * 3 = 9 4 * 8 = 32 bits 15 bits 28 bits <p>Without encoding, the total size of the string was <code>120</code> bits. After encoding the size is reduced to <code>32 + 15 + 28 = 75</code>.</p>"},{"location":"algorithms/huffman_coding/#2-decoding-the-code","title":"2. Decoding the code","text":"<p>For decoding the code, we can take the code and traverse through the tree to find the character.</p> <p>Let 101 is to be decoded, we can traverse from the root as in the figure below.</p> <pre><code>              [* - 15]\n              /      \\\n             0       (1)\n            /          \\\n          [C - 6]     [* - 9]\n                      /     \\\n                    (0)      \\\n                    /         \\\n                 [* - 4]       1\n                  /    \\        \\\n                 0     (1)       \\\n                /        \\        \\\n             [B - 1] [D - 3] [A - 5]\n</code></pre> <p>101 ~ D</p>"},{"location":"algorithms/huffman_coding/#3-huffman-coding-complexity","title":"3. Huffman Coding Complexity","text":"<p>The time complexity for encoding each unique character based on its frequency is O(nlog n).</p> <p>Extracting minimum frequency from the priority queue takes place 2*(n-1) times and its complexity is O(log n). Thus the overall complexity is O(nlog n).</p>"},{"location":"algorithms/kmp/","title":"Knuth\u2013Morris\u2013Pratt (KMP)","text":"<p>KMP (Knuth\u2013Morris\u2013Pratt algorithm) is a nice algorithm for solving some string matching problems (match a pattern in a text).</p>"},{"location":"algorithms/kmp/#naive-solution","title":"Naive solution","text":"<p>A naive way to solve this problem would be to compare each character of W with T. Every time there is a mismatch, we shift W to the right by 1, then we start comparing again.</p> <p>Example:</p> <p></p> <p>One naive way to solve this problem would be to compare each character of W with T. Every time there is a mismatch, we shift W to the right by 1, then we start comparing again.</p> <p>But we can do it better by using KMP algorithm.</p>"},{"location":"algorithms/kmp/#kmp","title":"KMP","text":"<p>As you can see in the above image, there is a mismatch at index 3. According to naive approach next step would be to shift W by 1. Since all letters in W are different, we can actually shift W by the index where mismatch occurred (3 in this case). We can say for sure there won\u2019t be any match in between.</p> <p>Let take another example:</p> <p></p> <p>In the above image the green cells in the left substring is equal to the green cells in the right substring. It is actually the largest prefix which is also equal to the suffix of the substring till index 4 of the word \u201cdeadeye\u201d</p> <p>Now let\u2019s see how it works by taking an abstract example:</p> <p></p> <p>str1 = str2 (green cells) and str2 = str3. When there is a mismatch after str2, we can directly shift the word till after str1 as you can see in the image. Green cells actually tell us the index from where it should start comparing next, if there is a mismatch.</p>"},{"location":"algorithms/kmp/#calculate-lps-longest-proper-prefix-which-is-also-suffix-table","title":"Calculate LPS (Longest Proper Prefix which is also Suffix) table","text":"<p>Prefix table (also known as LPS/ Longest Prefix Suffix) is an array data structure which captures the longest prefix which is also a suffix for every substring starting at index 0.</p> <p></p> <p></p>"},{"location":"algorithms/kmp/#source-code","title":"Source code","text":"<p>Example code to compute LPS table</p> <pre><code>std::vector&lt;int&gt; computeLPS(std::string pat) {\n  int patSize = pat.size();\n  std::vector&lt;int&gt; lsp(patSize, 0);\n  int left = 0;\n  int right = 1;\n  lsp[0] = 0;\n  while (right &lt; patSize) {\n    if (patt[left] == pat[right]) {\n      lsp[right] = left + 1;\n      ++left;\n      ++right;\n    } else {\n      if (left != 0) {\n        left = lsp[left - 1];\n      } else {\n        lsp[right] = 0;\n        ++right;\n      }\n    }\n  }\n}\n</code></pre> <p>Example code to do KMS search</p> <pre><code>int kmpSearch(std::string str, std::string pat) {\n  auto strSize = str.size();\n  auto patSize = pat.size();\n  auto lsp = computeLPS(pat);\n\n  int strIdx = 0;\n  int patIdx = 0;\n\n  while (strIdx &lt; strSize) {\n    if (str[strIdx] == pat[patIdx]) {\n      ++strIdx;\n      ++patIdx;\n    } else {\n      if (patIdx != 0) {\n        patIdx = lsp[patIdx - 1];\n      } else {\n        ++strIdx;\n      }\n    }\n\n    if (patIdx == patSize - 1) {\n      return strIdx - patIdx;\n    }\n  }\n\n  return -1;\n}\n</code></pre>"},{"location":"algorithms/kmp/#timespace-complexity","title":"Time/space complexity","text":"<p>Time complexity: O(n + m) Space complexity: O(m)</p> <p>where:</p> <ul> <li>n is the size of the string</li> <li>m is the size of the pattern</li> </ul>"},{"location":"algorithms/minimum_spanning_tree/","title":"Minimum Spanning Tree","text":""},{"location":"algorithms/minimum_spanning_tree/#1-what-is-a-spanning-tree","title":"1. What is a Spanning Tree?","text":"<p>In an undirected and connected graph G=(V,E), a spanning tree is a subgraph that is a tree which includes all of the vertices of G, with minimum possible number of edges. A graph may have several spanning trees. The cost of the spanning tree is the sum of the weights of all the edges in the tree</p>"},{"location":"algorithms/minimum_spanning_tree/#2-what-is-a-minimum-spanning-tree","title":"2. What is a Minimum Spanning Tree?","text":"<p>A minimum spanning tree (M- ST) is the spanning tree where the cost is minimum among all the spanning trees.</p> <p></p>"},{"location":"algorithms/minimum_spanning_tree/#3-prims-algorithm","title":"3. Prim\u2019s Algorithm","text":"<ul> <li>Prim\u2019s algorithm is a greedy algorithm that works well on dense graphs.</li> <li>It finds a minimum spanning tree for a weighted UNDIRECTED graph.</li> </ul> <p>Algorithm Steps:</p> <ol> <li>Choose any arbitrary node s as root node</li> <li>Enqueues all edges incident to s into a Priority Queue (PQ)</li> <li>Repeatedly do the following greedy steps until PQ is empty: If the vertex v with edge e (w -&gt; v) in the PQ has not been visited then add e to MST and enqueue all edges connected to v into the PQ.</li> </ol> <p>Visualising</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;queue&gt;\n\nstd::vector&lt;std::vector&lt;std::pair&lt;int, int&gt;&gt;&gt; adj;\nstd::vector&lt;bool&gt; visited;\nstd::priority_queue&lt;std::pair&lt;int, int&gt;, std::vector&lt;std::pair&lt;int, int&gt;&gt;, std::greater&lt;std::pair&lt;int, int&gt;&gt;&gt; queue;\n\nvoid addEdges(int s) {\n    visited[s] = true;\n    for (int i = 0; i &lt; adj[s].size(); ++i) {\n        if (visited[adj[s][i].second]) {\n            continue;\n        }\n\n        queue.push(adj[s][i]);\n    }\n}\n\nint main() {\n    int n, m, a, b, w;\n    std::cin &gt;&gt; n &gt;&gt; m;\n    adj = std::vector&lt;std::vector&lt;std::pair&lt;int, int&gt;&gt;&gt;(n + 1, std::vector&lt;std::pair&lt;int, int&gt;&gt;{});\n    visited = std::vector&lt;bool&gt;(n + 1, false);\n\n    for (int i = 0; i &lt; m; ++i) {\n        std::cin &gt;&gt; a &gt;&gt; b &gt;&gt; w;\n        adj[a].push_back(std::make_pair(w, b));\n        adj[b].push_back(std::make_pair(w, a));\n    }\n\n    int edgeCount = 0;\n    int mstCost = 0;\n    addEdges(1);\n\n    while(!queue.empty() &amp;&amp; edgeCount != n - 1) {\n        auto cost = queue.top().first;\n        auto des = queue.top().second;\n        queue.pop();\n\n        if (visited[des]) {\n            continue;\n        }\n\n        mstCost += cost;\n        ++edgeCount;\n\n        addEdges(des);\n    }\n\n    if (edgeCount != n - 1) {\n        // No MST found\n        std::cout &lt;&lt; \"0\";\n    } else {\n        std::cout &lt;&lt; mstCost;\n    }\n}\n</code></pre> <p>The time complexity of Prim's algorithm is O(E log V).</p>"},{"location":"algorithms/minimum_spanning_tree/#4-kruskals-algorithm","title":"4. Kruskal\u2019s Algorithm","text":"<ul> <li>Kruskal\u2019s algorithm is a greedy algorithm that works well on dense graphs.</li> </ul> <p>Algorithm Steps:</p> <ol> <li>Sort the set of edges E in increasing order</li> <li>Start adding edges to the MST from the edge with the smallest weight until the edge of the largest weight.</li> <li>Only add edges which doesn't form a cycle , edges which connect only disconnected components.</li> </ol> <p>So now the question is how to check if  vertices are connected or not ?</p> <p>This could be done using DFS but DFS will make time complexity large. So the best solution is <code>Disjoint Set</code></p> <pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;algorithm&gt;\n\nclass DisjointSet {\nprivate:\n    std::vector&lt;int&gt; parents;\npublic:\n    DisjointSet(int n) {\n        parents = std::vector&lt;int&gt;(n + 1, -1);\n\n        for (int i = 1; i &lt;= n; ++i) {\n            parents[i] = i;\n        }\n    }\n\n    int find(int x) {\n        while (parents[x] != x) {\n            parents[x] = parents[parents[x]];\n            x = parents[x];\n        }\n\n        return parents[x];\n    }\n\n    void unionSet(int x, int y) {\n        auto parentX = find(x);\n        auto parentY = find(y);\n\n        parents[parentY] = parentX;\n    }\n};\n\nint main() {\n    int n, m, a, b, w;\n    std::cin &gt;&gt; n &gt;&gt; m;\n    std::vector&lt;std::pair&lt;int, std::pair&lt;int, int&gt;&gt;&gt; edges;\n\n    for (int i = 0; i &lt; m; ++i) {\n        std::cin &gt;&gt; a &gt;&gt; b &gt;&gt; w;\n        edges.push_back(std::make_pair(w, std::make_pair(a, b)));\n    }\n\n    DisjointSet disjointSet(n);\n    std::sort(edges.begin(), edges.end());\n    int mst = 0;\n    for (int i = 0; i &lt; edges.size(); ++i) {\n        auto cost = edges[i].first;\n        auto s = edges[i].second.first;\n        auto d = edges[i].second.second;\n\n        if (disjointSet.find(s) != disjointSet.find(d)) {\n            disjointSet.unionSet(s, d);\n\n            mst += cost;\n        }\n    }\n\n    std::cout &lt;&lt; mst &lt;&lt; std::endl;\n}\n</code></pre>"},{"location":"algorithms/monotonic_stack/","title":"Monotonic stack","text":"<p>Stack is a very simple data structure. The logical sequence of first in and last out conforms to the some characteristics of some problems, such as function call stack.</p> <p>Monotonic stack is actually a stack. It just uses some ingenious logic to keep the elements in the stack orderly (monotone increasing or monotone decreasing) after each new element putting into the stack.</p> <p>Well,sounds like a heap? No, monotonic stack is not widely used. It only deals with one typical problem, which is called <code>Next Greater Element</code></p> <p>If a problem is suitable to use monotonic stack, it must has at least three characters:</p> <ol> <li>It is a <code>range queries in an array</code> problem.</li> <li>The minima/maxima element or the monotonic order of elements in a range is useful to get answer of every range query.</li> <li>When a element is popped from the monotonic stack, it will never be used again</li> </ol>"},{"location":"algorithms/monotonic_stack/#example","title":"Example","text":""},{"location":"algorithms/monotonic_stack/#next-greater-element","title":"Next greater element","text":"<p>Give you an array,and return an array of equal length.The corresponding index stores the next larger element, if there is no larger element, store -1. It's not easy to explain clearly in words. Let's take a direct example:</p> <p>Give you an array [2,1,2,4,3],and you return an array [4,2,4,-1,-1].</p> <p>This problem can be thought abstractly: think of the elements in the array as people standing side by side, and the size of the elements as the height of an adult. These people stand in line before you. How to find the Next Greater Number of element \"2\"? Very simply, if you can see the element \"2\", then the first person you can see behind him is the Next Greater Number of \"2\". Because the element smaller than \"2\" is not tall enough and it is blocked by \"2\",the first one not being blocked is the answer.</p> <p></p> <p>This is a very understandable situation,huh? With this abstract scenario in mind, let's look at the code first.</p> <pre><code>vector&lt;int&gt; nextGreaterElement(vector&lt;int&gt;&amp; nums) {\n    vector&lt;int&gt; ans(nums.size()); // array to store answer\n    stack&lt;int&gt; s;\n    for (int i = nums.size() - 1; i &gt;= 0; i--) { // put it into the stack back to front\n        while (!s.empty() &amp;&amp; s.top() &lt;= nums[i]) { // determine by height\n            s.pop(); // short one go away while blocked\n        }\n        ans[i] = s.empty() ? -1 : s.top(); // the first tall behind this element\n        s.push(nums[i]); // get into the queue and wait for later height determination\n    }\n    return ans;\n}\n</code></pre>"},{"location":"algorithms/segment_tree/","title":"Segment Tree","text":"<p>A segment tree is a data structure used to store information about array segments and answer segment queries efficiently:</p> <ul> <li>range(i, j): gives the sum of the array elements starting at index i and ending at index j.</li> <li>update(i, val): updates the value at index i to the val in the original array and updates the segment tree accordingly.</li> </ul>"},{"location":"algorithms/segment_tree/#structure-of-the-segment-tree","title":"Structure of the Segment Tree","text":"<p>Consider an array <code>arr</code> of size <code>n</code> and a corresponding Segment Tree <code>T</code>:</p> <ul> <li>The root of <code>T</code> will represent the whole array <code>arr[0..n-1]</code></li> <li>Each leaf in the segment tree <code>T</code> represent a single element arr[i] such that <code>0 &lt;= i &lt; n</code></li> <li>The internal nodes in the segment tree <code>T</code> represent the union of elementary intervals arr[i..j] where <code>0 &lt;= i &lt; j &lt; n</code></li> </ul> <p>We can take a divide-and-conquer approach when it comes to array segments. We compute and store the sum of the elements of the whole array, i.e. the sum of the segment <code>arr[0..n-1]</code>. We then split the array into two halves <code>arr[0..n / 2 - 1]</code> and <code>arr[n / 2..n - 1]</code> and compute the sum of each halve and store them. Each of these two halves in turn are split in half, and so on until all segments reach size <code>1</code>.</p> <p></p>"},{"location":"algorithms/segment_tree/#construction","title":"Construction","text":"<p>Before constructing the segment tree, we need to decide:</p> <ul> <li>The value that gets stored at each node of the segment tree. For example, in a sum segment tree, a node would store the sum of the elements in its range <code>[l..r]</code>.- The merge operation that merges two siblings in a segment tree. For example, in a sum segment tree, the two nodes corresponding to the ranges <code>arr[l1..r1]</code> and <code>arr[l2..r2]</code> would be merged into a node corresponding to the range <code>arr[l1..r2]</code> by adding the values of the two nodes.</li> </ul> <p>Note that a vertex is a leaf vertex, if its corresponding segment covers only one value in the original array. It is present at the lowermost level of a segment tree. Its value would be equal to the (corresponding) element <code>arr[i]</code></p> <p>We can use an array to represent Segment Trees. For each node at index i</p> <ul> <li>The left child is at index (2 * i + 1)</li> <li>The right child at (2 * i + 2)</li> <li>The parent is at floor((i \u2013 1) / 2).</li> </ul> <p>Segment trees have some nice properties:</p> <ul> <li>If the underlying array has <code>n</code> elements, the segment tree has exactly <code>(2n - 1)</code> nodes \u2014 <code>n</code> leaves and <code>(n - 1)</code> internal nodes</li> <li>The height of the tree is <code>O(logn)</code></li> <li>Each segment can be split into <code>O(log n)</code> non-intersecting segments that correspond to the nodes of the segment tree</li> </ul> <p>When <code>n</code> is not a perfect power of two, not all levels are filled entirely \u2014 the last layer may be incomplete.</p> <pre><code>void build(std::vector&lt;int&gt;&amp; arr, int node, int start, int end) {\n  if (start == end) {\n    // leaf node will have a single element\n    segmentTree[node] = arr[start];\n  } else {\n    int mid = (start + end) / 2;\n    // recurse on the start child\n    build(arr, node * 2, start, mid);\n    // recurse on the end child\n    build(arr, node * 2 + 1, mid + 1, end);\n    // internal node will have the sum of both of its children\n    segmentTree[node] = segmentTree[node * 2] + segmentTree[node * 2 + 1];\n  }\n}\n</code></pre> <ul> <li>arr: the input array</li> <li>node: the index of the current vertex</li> <li>left and right: the boundaries of the current segment</li> </ul> <p>In the main program this function will be called with the parameters <code>node = 1</code>, <code>left = 0</code>, <code>right = n - 1</code></p> <p>How to calculate segmentTree array size</p> <p>If we have array of <code>n</code> elements, then the segment tree will have a leaf node for each of these <code>n</code> entries. Thus, we have <code>n</code> leaf nodes, and also <code>(n - 1)</code> internal nodes.</p> <p>Total number of nodes = <code>n + (n - 1) = 2n - 1</code>. Now, we know its a full binary tree and thus the height is: <code>std::ceil(std::log2(n)) + 1</code></p> <p>Total no. of nodes = 2<sup>0</sup> + 2<sup>1</sup> + ... + 2<sup>h</sup></p> <p>where h is the height of the segment tree</p> <p>We have Sum of n Terms in GP:</p> <p>Sn = a * r<sup>0</sup> + a * <sup>r</sup> + a * r<sup>2</sup> + ... + a * r<sup>n - 1</sup> = a * (r<sup>n</sup> - 1) / (r - 1)</p> <p>Apply the formula we have</p> <p>Total no. of nodes = 2 * 2 <sup>std::ceil(std::log2(n))</sup> - 1</p> <pre><code>auto heighSegmentTree = (int)(std::ceil(std::log2(n)));\nauto segmentTreeSize = 2 * std::pow(2, heighSegmentTree) - 1;\n</code></pre> <p>Time complexity: <code>O(n)</code>.</p>"},{"location":"algorithms/segment_tree/#update","title":"Update","text":"<p>Now we want to modify a specific element in the array, let's say we want to do the assignment <code>arr[i] = x</code> And we have to rebuild the Segment Tree, such that it correspond to the new, modified array.</p> <p>It is easy to see, that the update request can be implemented using a recursive function. The function gets passed the current tree vertex, and it recursively calls itself with one of the two child vertices (the one that contains <code>arr[i]</code> in its segment), and after that recomputes its sum value, similar how it is done in the build method (that is as the sum of its two children).</p> <pre><code>void update(int node, int start, int end, int pos, int newVal) {\n  if (start == end) {\n    // leaf node\n    segmentTree[node] = newVal;\n    return;\n  }\n\n  int mid = (start + end) / 2;\n  if (pos &lt;= mid) {\n    // if idx is in the left child, recurse on the left child\n    update(node * 2, start, mid, pos, newVal);\n  } else {\n    // if idx is in the right child, recurse on the end child\n    update(node * 2 + 1, mid + 1, end, pos, newVal);\n  }\n\n  // internal node will have the sum of both of its children\n  segmentTree[node] = segmentTree[node * 2] + segmentTree[node * 2 + 1];\n}\n</code></pre> <p>Time complexity: <code>O(logn)</code>.</p>"},{"location":"algorithms/segment_tree/#query","title":"Query","text":"<p>For now we are going to answer sum queries. As an input we receive two integers <code>l</code> and <code>r</code>, and we have to compute the sum of the segment <code>arr[l..r]</code> in <code>O(logn)</code> time.</p> <p>There are three possible cases.</p> <ul> <li>The easiest case is when the segment <code>arr[l..r]</code> is equal to the corresponding segment of the current vertex. Eg: <code>arr[l..r] = segmentTree[tl..tr]</code>, then we are finished and can return the precomputed sum that is stored in the vertex.</li> <li>The segment of the query can fall completely into the domain of either the left or the right child. In this case we can simply go to the child vertex, which corresponding segment covers the query segment.</li> <li>And then there is the last case, the query segment intersects with both children. In this case we have no other option as to make two recursive calls, one for each child. First we go to the left child, compute a partial answer for this vertex, then go to the right child, compute the partial answer using that vertex, and then combine the answers by adding them.</li> </ul> <pre><code>int query(int node, int start, int end, int left, int right) {\n  if (right &lt; start || end &lt; left || start &gt; end || left &gt; right) {\n    // range represented by a node is completely outside the given range\n    return 0;\n  }\n\n  if (left &lt;= start &amp;&amp; end &lt;= right) {\n    // range represented by a node is completely inside the given range\n    return segmentTree[node];\n  }\n\n  // range represented by a node is partially inside and partially outside the given range\n  int mid = (start + end) / 2;\n\n  return query(node * 2, start, mid, left, right)\n         + query(node * 2 + 1, mid + 1, end, left, right);\n}\n</code></pre> <p>Time complexity: <code>O(logn)</code>.</p>"},{"location":"algorithms/segment_tree/#updating-an-interval-lazy-propagation","title":"Updating an interval (Lazy Propagation)","text":"<p>Sometimes problems will ask you to update an interval from <code>l</code> to <code>r</code>, instead of a single element. One solution is to update all the elements one by one. Complexity of this approach will be <code>O(N)</code> per operation since where are <code>N</code> elements in the array and updating a single element will take <code>O(logN)</code> time.</p> <p>To avoid multiple call to update function, we can modify the update function to work on an interval.</p> <p>Let's be Lazy i.e., do work only when needed. How ? When we need to update an interval, we will update a node and mark its child that it needs to be updated and update it when needed. For this we need an array <code>lazy[]</code> of the same size as that of segment tree.</p> <p>Initially all the elements of the <code>lazy[]</code> array will be <code>0</code> representing that there is no pending update. If there is non-zero element <code>lazy[k]</code> then this element needs to update node <code>k</code> in the segment tree before making any query operation.</p> <p>To update an interval we will keep 3 things in mind.</p> <ul> <li>If current segment tree node has any pending update, then first add that pending update to current node.</li> <li>If the interval represented by current node lies completely in the interval to update, then update the current node and update the lazy[] array for children nodes.</li> <li>If the interval represented by current node overlaps with the interval to update, then update the nodes as the earlier update function</li> </ul> <pre><code>void updateRange(int node, int start, int end, int left, int right, int val)\n{\n  if(lazy[node] != 0) {\n    // this node needs to be updated\n    // update it\n    segmentTree[node] += (end - start + 1) * lazy[node];\n    if(start != end) {\n        // mark child as lazy\n        lazy[node * 2] += lazy[node];\n        lazy[node * 2 + 1] += lazy[node];\n    }\n\n    // reset it\n    lazy[node] = 0;\n  }\n\n  if(start &gt; end || start &gt; right || end &lt; left) {\n    // current segment is not within range [left, right]\n    return;\n  }\n\n  if(left &lt;= start &amp;&amp; end &lt;= right) {\n    // segment is fully within range\n    segmentTree[node] += (end - start + 1) * val;\n    if(start != end) {\n        // not leaf node\n        lazy[node * 2] += val;\n        lazy[node * 2 + 1] += val;\n    }\n    return;\n  }\n\n  int mid = (start + end) / 2;\n\n  // updating left child\n  updateRange(node * 2, start, mid, left, right, val);\n  // updating right child\n  updateRange(node * 2 + 1, mid + 1, end, left, right, val);\n\n  // updating root with max value\n  segmentTree[node] = segmentTree[node * 2] + segmentTree[node * 2 + 1];\n}\n</code></pre> <p>Query</p> <pre><code>int queryRange(int node, int start, int end, int left, int right) {\n  if(start &gt; end || start &gt; right || end &lt; left) {\n    // out of range\n    return 0;\n  }\n\n  if(lazy[node] != 0) {\n    // this node needs to be updated\n    // update it\n    segmentTree[node] += (end - start + 1) * lazy[node];\n    if(start != end) {\n      // mark child as lazy\n      lazy[node * 2] += lazy[node];\n      // mark child as lazy\n      lazy[node * 2 + 1] += lazy[node];\n    }\n\n    // reset it\n    lazy[node] = 0;\n  }\n\n  if(left &lt;= start &amp;&amp; end &lt;= right) {\n    // current segment is totally within range [l, r]\n    return tree[node];\n  }\n\n  int mid = (start + end) / 2;\n\n  // query the left child\n  int p1 = queryRange(node*2, start, mid, left, right);\n  // query the right child\n  int p2 = queryRange(node*2 + 1, mid + 1, end, left, right);\n\n  return (p1 + p2);\n}\n</code></pre>"},{"location":"algorithms/segment_tree/#advanced-versions-of-segment-trees","title":"Advanced versions of Segment Trees","text":""},{"location":"algorithms/segment_tree/#finding-the-maximum","title":"Finding the maximum","text":"<p>Let us slightly change the condition of the problem described above: instead of querying the sum, we will now make maximum queries.</p> <p>The tree will have exactly the same structure as the tree described above. We only need to change the way <code>segmentTree[node]</code> is computed in the <code>build</code> and <code>update</code> functions. <code>segmentTree[node]</code> will now store the maximum of the corresponding segment. And we also need to change the calculation of the returned value of the <code>query</code> function (replacing the summation by the maximum).</p>"},{"location":"algorithms/segment_tree/#finding-the-maximum-and-the-number-of-times-it-appears","title":"Finding the maximum and the number of times it appears","text":"<p>This task is very similar to the previous one. In addition of finding the maximum, we also have to find the number of occurrences of the maximum.</p> <p>To solve this problem, we store a pair of numbers at each vertex in the tree: In addition to the maximum we also store the number of occurrences of it in the corresponding segment.</p>"},{"location":"algorithms/segment_tree/#compute-the-greatest-common-divisor-least-common-multiple","title":"Compute the greatest common divisor / least common multiple","text":"<p>In this problem we want to compute the GCD / LCM of all numbers of given ranges of the array.</p> <p>This interesting variation of the Segment Tree can be solved in exactly the same way as the Segment Trees we derived for sum / minimum / maximum queries: it is enough to store the GCD / LCM of the corresponding vertex in each vertex of the tree. Combining two vertices can be done by computing the GCD / LCM of both vertices.</p>"},{"location":"algorithms/segment_tree/#counting-the-number-of-zeros-searching-for-the-k-th-zero","title":"Counting the number of zeros, searching for the k-th zero","text":"<p>In this problem we want to find the number of zeros in a given range, and additionally find the index of the <code>k-th</code> zero using a second function.</p> <p>Again we have to change the store values of the tree a bit: This time we will store the number of zeros in each segment in <code>segmentTree[]</code>. It is pretty clear, how to implement the <code>build</code>, <code>update</code> and <code>countZero</code> functions, we can simply use the ideas from the sum query problem. Thus we solved the first part of the problem.</p> <p>Now we learn how to solve the problem of finding the <code>k-th</code> zero in the array <code>arr</code>. To do this task, we will descend the Segment Tree, starting at the root vertex, and moving each time to either the left or the right child, depending on which segment contains the <code>k-th</code> zero. In order to decide to which child we need to go, it is enough to look at the number of zeros appearing in the segment corresponding to the left vertex. If this precomputed count is greater or equal to <code>k</code>, it is necessary to descend to the left child, and otherwise descent to the right child. Notice, if we chose the right child, we have to subtract the number of zeros of the left child from <code>k</code>.</p> <pre><code>int findKth(int node, int start, int end, int k) {\n  if (k &gt; segmentTree[node]) {\n    return -1;\n  }\n\n  if (start == end) {\n    return start;\n  }\n\n  int mid = (start + end) / 2;\n  if (segmentTree[node * 2] &gt;= k) {\n    return findKth(node * 2, start, mid, k);\n  } else {\n    return findKth(node * 2 + 1, mid + 1, end, k - segmentTree[node * 2]);\n  }\n}\n</code></pre>"},{"location":"algorithms/segment_tree/#searching-for-an-array-prefix-with-a-given-amount","title":"Searching for an array prefix with a given amount","text":"<p>The task is as follows: for a given value <code>x</code> we have to quickly find smallest index <code>i</code> such that the sum of the first <code>i</code> elements of the array <code>arr</code> is greater or equal to <code>x</code> (assuming that the array <code>arr</code> only contains non-negative values).</p> <p>This task can be solved using binary search, computing the sum of the prefixes with the Segment Tree. However this will lead to a <code>O(logn*logn)</code> solution</p> <p>Instead we can use the same idea as in the previous section, and find the position by descending the tree: by moving each time to the left or the right, depending on the sum of the left child. Thus finding the answer in <code>O(logn)</code> time</p>"},{"location":"algorithms/segment_tree/#searching-for-the-first-element-greater-than-a-given-amount","title":"Searching for the first element greater than a given amount","text":"<p>The task is as follows: for a given value <code>x</code> and a range <code>[l..r]</code> find the smallest <code>i</code> in the range <code>arr[l..r]</code>, such that <code>arr[i]</code> is greater than <code>x</code>.</p> <p>This task can be solved using binary search over max prefix queries with the Segment Tree. However, this will lead to a <code>O(logn*logn)</code> solution.</p> <p>Instead, we can use the same idea as in the previous sections, and find the position by descending the tree: by moving each time to the left or the right, depending on the maximum value of the left child. Thus finding the answer in <code>O(logn)</code> time</p>"},{"location":"algorithms/segment_tree/#finding-subsegments-with-the-maximal-sum","title":"Finding subsegments with the maximal sum","text":"<p>Here again we receive a range <code>arr[l..r]</code> for each query, this time we have to find a subsegment <code>arr[l'..r']</code> such that <code>l &lt;= l'</code> and <code>r' &lt;= r</code> and the sum of the elements of this segment is maximal</p> <p>This time we will store four values for each vertex:</p> <ul> <li>the sum of the segment</li> <li>the maximum prefix sum</li> <li>the maximum suffix sum</li> <li>the sum of the maximal subsegment in it</li> </ul> <p>You can find more here</p>"},{"location":"algorithms/shortest_path_algorithms/","title":"Shortest Path Algorithms","text":"<p>The shortest path problem is about finding a path between 2 vertices in a graph such that the total sum of the edges weights is minimum.</p> BFS Dijkstra Bellman Ford Floyd Warshall Complexity O(V+E) O(V + E log(V)) O(VE) O(V3) Recommended grapth size Large Large/Medium Medium/Small Small Good for APSP Only works on unweighted graphs Ok Bad Yes Can detect negative cycles No No Yes Yes SP on graph with weighted edges Incorrect SP answer Best algorithm Works Bad in general SP on graph with unweighted edges Best algorithm Ok Bad Bad in general"},{"location":"algorithms/shortest_path_algorithms/#1-bellman-fords-algorithm","title":"1. Bellman Ford's Algorithm","text":"<p>Bellman Ford's algorithm is used to find the shortest paths from the source vertex to all other vertices in a weighted graph.</p> <p>Algorithm Steps:</p> <ul> <li>Initialize an array D to keep track of the shortest path from <code>s</code> to all of the nodes</li> <li>Set every entry in D to +\u221e</li> <li>The outer loop traverses from: 0 -&gt; n - 1 .</li> <li>Loop over all edges, check if the D[next] &gt; D[current] + weight, in this case update D[next] = D[current] + weight.</li> <li>Repeat the step 4 to find nodes caught in a negative cycle</li> </ul> <pre><code>std::vector&lt;int&gt; dist(n, std::numeric_limits&lt;int&gt;::max());\ndist[0] = 0;\n\nfor (int i = 0; i &lt; n - 1; ++i) {\n  for (int j = 0; j &lt; m; ++j){\n    auto from = edges[j][0];\n    auto to = edges[j][1];\n    auto w = edges[j][2];\n\n    if (dist[from] != std::numeric_limits&lt;int&gt;::max() &amp;&amp; dist[from] + w &lt; dist[to]) {\n      dist[to] = dist[from] + w;\n    }\n  }\n}\n\n// Repeat to find nodes caught in a negative cycle\nfor (int i = 0; i &lt; n - 1; ++i) {\n  for (int j = 0; j &lt; m; ++j){\n    auto from = edges[j][0];\n    auto to = edges[j][1];\n    auto w = edges[j][2];\n\n    if (dist[from] != std::numeric_limits&lt;int&gt;::max() &amp;&amp; dist[from] + w &lt; dist[to]) {\n      dist[to] = std::numeric_limits&lt;int&gt;::min();\n    }\n  }\n}\n</code></pre> <p>The time complexity of the algorithm is O(VE).</p>"},{"location":"algorithms/shortest_path_algorithms/#2-dijkstras-algorithm","title":"2. Dijkstra's Algorithm","text":"<p>Dijkstra's algorithm is used to find the shortest paths from the source vertex to all other vertices in a weighted graph.</p> <p>One contraint for Dijkstra's algorithm is that the graph must only contain non-negative edge weight. This contraint is imposed to ensure that once a node has been visited its optimal distance can not be improved.</p> <p>Algorithm Steps:</p> <ul> <li>Maintain a <code>dist</code> array where the distance to every node is positive infinity, mark the distance to the start node <code>s</code> to be <code>0</code></li> <li>Maintain a PQ of key-value pairs of (node_idx, distance) pairs and insert (s, 0) in to the PQ</li> <li>Pull out the pair (node_idx, distance) from PQ</li> <li>Update the distance of the connected vertices to node in <code>node_idx</code> in case of <code>current vertex distance + edge weight &lt; next vertex distance</code>, then push them to PQ</li> <li>If the popped vertex is visited before, just continue without using it.</li> <li>Apply the same algorithm again until the PQ is empty.</li> </ul> <pre><code>std::vector&lt;int&gt; dist(n, std::numeric_limits&lt;int&gt;::max());\nstd::vector&lt;bool&gt; visited(n, false);\nstd::priority_queue&lt;std::pair&lt;int, int&gt;, std::vector&lt;std::pair&lt;int, int&gt;&gt;, std::greater&lt;std::pair&lt;int, int&gt;&gt;&gt; queue;\n\ndist[0] = 0;\nqueue.push(std::make_pair(0, 0));\n\nwhile(!queue.empty()) {\n  auto w = queue.top().first;\n  auto node_idx = queue.top().second;\n  queue.pop();\n\n  if (visited[node_idx]) {\n    continue;\n  }\n\n  visited[node_idx] = true;\n\n  if (dist[node_idx] &lt; w) {\n    continue;\n  }\n\n  for (int i = 0; i &lt; adj[node_idx].size(); ++i) {\n    auto to = adj[node_idx][i].first;\n    auto cost = adj[node_idx][i].second;\n\n    if (dist[node_idx] + cost &lt; dist[to]) {\n      dist[to] = dist[node_idx] + cost;\n      queue.push(std::make_pair(dist[to], to));\n    }\n  }\n}\n</code></pre> <p>The time complexity of the algorithm is O(V + E log(V)).</p>"},{"location":"algorithms/shortest_path_algorithms/#3-bfs-0-1","title":"3. BFS 0-1","text":"<p>We know that we can use Dijkstra algorithm with time complexity O(V + E log(V)) to solve SSSP problems.</p> <p>However if the weights are more constrained we can do better, in this case we can use BFS to solve the SSSP problem in O(E), if weight of each edge is either 0 or 1.</p> <p>The algorithm is based on the Dijkstra algorithm</p> <pre><code>std::vector&lt;int&gt; d(n, INF);\nd[s] = 0;\nstd::deque&lt;int&gt; q;\nq.push_front(s);\n\nwhile (!q.empty()) {\n  int v = q.front();\n  q.pop_front();\n\n  for (auto edge : adj[v]) {\n    int u = edge.first;\n    int w = edge.second;\n    if (d[v] + w &lt; d[u]) {\n      d[u] = d[v] + w;\n      if (w == 1) {\n        q.push_back(u);\n      }\n      else {\n        q.push_front(u);\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"algorithms/shortest_path_algorithms/#4-floyd-warshalls-algorithm","title":"4. Floyd Warshall's Algorithm","text":"<p>Floyd Warshall's Algorithm is used to find the shortest paths between between all pairs of vertices in a graph, where each edge in the graph has a weight which is positive or negative.</p> <p>Algorithm Steps:</p> <ul> <li>Initialize the shortest paths between any 2 vertices with Infinity.</li> <li>Find all pair shortest paths that use 0 intermediate vertices, then find the shortest paths that use 1 intermediate vertex and so on.. until using all V vertices as intermediate nodes.</li> <li>Minimize the shortest paths between any 2 pairs in the previous operation.</li> <li>For any 2 vertices (i,j), one should actually minimize the distances between this pair using the first K nodes, so the shortest path will be: min(dist[i][k]+dist[k][j], dist[i][j])</li> <li>Repeat to find nodes caught in a negative cycle</li> </ul> <pre><code>for(int k = 1; k &lt;= n; k++){\n  for(int i = 1; i &lt;= n; i++){\n    for(int j = 1; j &lt;= n; j++){\n      dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j]);\n    }\n  }\n}\n\n// Detect negative cycle\nfor(int k = 1; k &lt;= n; k++){\n  for(int i = 1; i &lt;= n; i++){\n    for(int j = 1; j &lt;= n; j++){\n      dist[i][j] = std::numeric_limits&lt;int&gt;::min();\n    }\n  }\n}\n</code></pre> <p>The time complexity of the algorithm is O(V^3).</p>"},{"location":"algorithms/sieve-of-eratosthenes/","title":"Sieve of Eratosthenes","text":"<p>Sieve of Eratosthenes is an algorithm for finding all the prime numbers in a segment [1..n] using O(nloglogn).</p> <p>Algorithm Steps:</p> <pre><code>std::vector&lt;bool&gt; primes(n, true);\nprimes[1] = false;\nfor (int i = 2; i * i &lt; n; ++i) {\n  if (!primes[i]) {\n    continue;\n  }\n\n  for (int j = i * i; j &lt; n; j = j + i) {\n    primes[j] = false;\n  }\n}\n</code></pre> <p>Time Complexity: O(nloglogn)</p>"},{"location":"algorithms/string_hashing/","title":"String Hashing","text":"<p>Hashing is used for efficient comparison of strings by converting them into integers and then comparing those strings on the basis of their integer values.</p>"},{"location":"algorithms/string_hashing/#string-hashing_1","title":"String Hashing","text":""},{"location":"algorithms/string_hashing/#formula","title":"Formula","text":"<p>The good and widely used way to define the hash of a string <code>s</code> of length <code>n</code> is</p> <p>hash(s) = s[0] * p<sup>0</sup> + s[1] * p<sup>1</sup> + s[2] * p<sup>2</sup> + ... + s[n - 1] * p <sup>n - 1</sup> mod m</p> <p>where <code>p</code> and <code>m</code> are some chosen, positive numbers. It is called a polynomial rolling hash function.</p> <p>It is reasonable to make <code>p</code> a prime number roughly equal to the number of characters in the input alphabet.</p> <p>For example:</p> <ul> <li>If the input is composed of only lowercase letters of the English alphabet, p = 31 is a good choice</li> <li>If the input may contain both uppercase and lowercase letters then p = 53 is a good choice</li> </ul> <p>Obviously <code>m</code> should be a large number since the probability of two random strings colliding is about is about \u2248 1/m. Sometimes m = 2<sup>64</sup> is chosen, since the integer overflow of 64-bits integers work exactly like the modulo operation.</p> <p>However, there exists a method, which generates colliding strings (which work independently from the choice of ). So in practice m = 2<sup>64</sup> is not recommended. A good choice for <code>m</code> is some large prime number.</p> <p>The code in this article will just use m = 10<sup>9</sup> + 9. This is a large number, but still small enough so that we can perform multiplication of two values using 64-bit integers.</p>"},{"location":"algorithms/string_hashing/#example","title":"Example","text":"<pre><code>long long compute_hash(string const&amp; s) {\n  const int p = 31;\n  const int m = 1e9 + 9;\n  long long hash_value = 0;\n  long long p_pow = 1;\n  for (char c : s) {\n      hash_value = (hash_value + (c - 'a' + 1) * p_pow) % m;\n      p_pow = (p_pow * p) % m;\n  }\n\n  return hash_value;\n}\n</code></pre>"},{"location":"algorithms/string_hashing/#rolling-hash","title":"Rolling Hash","text":"<p>Rolling hash is used to prevent rehashing the whole string while calculating hash values of the substrings of a given string. In rolling hash,the new hash value is rapidly calculated given only the old hash value.Using it, two strings can be compared in constant time.</p>"},{"location":"algorithms/string_hashing/#formula_1","title":"Formula","text":"<p>In general,the hash H can be defined as:-</p> <p>H = (c<sub>1</sub>a<sup>k-1</sup> + c<sub>2</sub>a<sup>k-2</sup> + c<sub>3</sub>a<sup>k-3</sup> +...+ c<sub>k</sub>a<sup>0</sup>) % m where:</p> <ul> <li>a is a constant</li> <li>c<sub>1</sub>, c<sub>2</sub>, ... c<sub>k</sub> are the input characters</li> <li>m is a large prime number (eg: 1e9 + 7)</li> </ul> <p>The probability of two random strings colliding is about \u2248 1/m.</p> <p>The hash value of next substring H<sub>nxt</sub> using rolling hash can be defined as:-</p> <p>H<sub>nxt</sub> = ((H - c<sub>1</sub>a<sup>k-1</sup>) * a + c<sub>k+1</sub>a<sup>0</sup>) % m</p>"},{"location":"algorithms/string_hashing/#example_1","title":"Example","text":"<p>Consider the string <code>abcd</code> and we have to find the hash values of substrings of this string having length 3 ,i.e.,<code>abc</code> and <code>bcd</code>.</p> <p>For simplicity let us take 5 as the base but in actual scenarios we should mod it with a large prime number to avoid overflow.</p> <p>So,the hash value of first substring,H(abc) :-</p> <pre><code>H(abc) =&gt; a*(5^2) + b*(5^1) + c*(5^0) = 97*25 + 98*5 + 99*1 = 3014\n</code></pre> <p>And the hash value of the second substring,H(bcd) :-</p> <pre><code>H(bcd) =&gt; b*(5^2) + c*(5^1) + d*(5^0) = 98*25 + 99*5 + 100*1 = 3045\nH(bcd) =(H(abc)-a*(5^2))*5 + d*(5^0)=(3014-97*25)*5 + 100*1 = 3045\n</code></pre>"},{"location":"algorithms/string_hashing/#time-complexity","title":"Time Complexity","text":"<p>The rolling hash takes O(n) time complexity to find hash values of all substrings of a length k of a string of length n.</p> <p>Computing hash value for the first substring will take O(k) as we have to visit every character of the substring and then for each of the n-k characters we can find the hash in O(1) so the total time complexity would be O(k+n-k)</p>"},{"location":"algorithms/submask_enumeration/","title":"Submask Enumeration","text":""},{"location":"algorithms/submask_enumeration/#enumerating-all-submasks-of-a-given-mask","title":"Enumerating all submasks of a given mask","text":"<p>Given a bitmask <code>m</code>, you want to efficiently iterate through all of its submasks, that is, masks <code>s</code> in which only bits that were included in mask <code>m</code> are set.</p> <p>1101 -&gt; [1101, 1100, 1001, 1000, 0101, 0100, 0001]</p> <pre><code>for (int s = m; s &gt; 0; s = (s - 1) &amp; m) {\n ... you can use s ...\n}\n\n\nint s = m;\nwhile (s &gt; 0) {\n ... you can use s ...\n s = (s - 1) &amp; m;\n}\n</code></pre> <p>Why does it work?</p> <p>Suppose we have a current bitmask <code>s</code>, and we want to move on to the next bitmask. By subtracting from the mask <code>s</code> one unit <code>(s - 1)</code>, we will remove the rightmost set bit and all bits to the right of it will become <code>1</code>. Then we remove all the <code>extra</code> one bits that are not included in the mask <code>m</code> and therefore can't be a part of a submask. We do this removal by using the bitwise operation <code>(s-1) &amp; m</code>. As a result, we cut mask <code>s - 1</code> to determine the highest value that it can take, that is, the next submask after <code>s</code> in descending order.</p>"},{"location":"algorithms/submask_enumeration/#iterating-through-all-masks-with-their-submasks","title":"Iterating through all masks with their submasks","text":"<p>In many problems, especially those that use bitmask dynamic programming, you want to iterate through all bitmasks and for each mask, iterate through all of its submasks:</p> <pre><code>for (int m = 0; m &lt; (1 &lt;&lt; n); ++m)\n  for (int s = m; s &gt; 0; s = (s - 1) &amp; m)\n    ... s and m ...\n</code></pre> <p>Time complexity</p> <p>O(3<sup>n</sup>)</p>"},{"location":"algorithms/ternary_search/","title":"Ternary Search","text":"<p>We are given a function \\(f(x)\\) which is unimodal on an interval \\([l, r]\\). By unimodal function, we mean one of two behaviors of the function:  1. The function strictly increases first, reaches a maximum (at a single point or over an interval), and then strictly decreases. 2. The function strictly decreases first, reaches a minimum, and then strictly increases.</p> <p>The task is to find the maximum of function \\(f(x)\\) on the interval \\([l, r]\\).</p>"},{"location":"algorithms/ternary_search/#algorithm","title":"Algorithm","text":"<p>Consider any 2 points \\(m_1\\), and \\(m_2\\) in this interval: \\(l &lt; m_1 &lt; m_2 &lt; r\\). We evaluate the function at \\(m_1\\) and \\(m_2\\), i.e. find the values of \\(f(m_1)\\) and \\(f(m_2)\\). Now, we get one of three options:</p> <ul> <li> <p>\\(f(m_1) &lt; f(m_2)\\)</p> <p>The desired maximum can not be located on the left side of \\(m_1\\), i.e. on the interval \\([l, m_1]\\), since either both points \\(m_1\\) and \\(m_2\\) or just \\(m_1\\) belong to the area where the function increases. In either case, this means that we have to search for the maximum in the segment \\([m_1, r]\\).</p> </li> <li> <p>\\(f(m_1) &gt; f(m_2)\\)</p> <p>This situation is symmetrical to the previous one: the maximum can not be located on the right side of \\(m_2\\), i.e. on the interval \\([m_2, r]\\), and the search space is reduced to the segment \\([l, m_2]\\).</p> </li> <li> <p>\\(f(m_1) = f(m_2)\\)</p> <p>We can see that either both of these points belong to the area where the value of the function is maximized, or \\(m_1\\) is in the area of increasing values and \\(m_2\\) is in the area of descending values (here we used the strictness of function increasing/decreasing). Thus, the search space is reduced to \\([m_1, m_2]\\). To simplify the code, this case can be combined with any of the previous cases.</p> </li> </ul> <p>How to pick \\(m_1\\) and \\(m_2\\)</p> \\[m_1 = l + \\frac{(r - l)}{3}\\] \\[m_2 = r - \\frac{(r - l)}{3}\\]"},{"location":"algorithms/ternary_search/#implementation","title":"Implementation","text":"<pre><code>double ternary_search(double l, double r) {\n  double eps = 1e-9; // set the error limit here\n\n  while (r - l &gt; eps) {\n    double m1 = l + (r - l) / 3;\n    double m2 = r - (r - l) / 3;\n    double f1 = f(m1); // evaluates the function at m1\n    double f2 = f(m2); // evaluates the function at m2\n    if (f1 &lt; f2)\n      l = m1;\n    else\n      r = m2;\n  }\n  return f(l); // return the maximum of f(x) in [l, r]\n}\n</code></pre>"},{"location":"algorithms/ternary_search/#time-complexity","title":"Time complexity","text":"\\[T(n) = T({2n}/{3}) + 1 = \\Theta(\\log n)\\] <p>It can be visualized as follows: every time after evaluating the function at points \\(m_1\\) and \\(m_2\\), we are essentially ignoring about one third of the interval, either the left or right one. Thus the size of the search space is \\({2n}/{3}\\) of the original one.</p> <p>Applying Master's Theorem, we get the desired complexity estimate.</p>"},{"location":"algorithms/topological_sort/","title":"Topological Sort","text":"<p>A topological ordering is an ordering of the nodes in a directed graph where for each directed edge from node A to node B, node A appears before node B in the ordering.</p> <p>For example:</p> <p></p> <p>A topological sorting of this graph is: 1 2 3 4 5</p> <p>Topological ordering are NOT unique. For the graph given above one another topological sorting is: 1 2 3 4 5</p> <p>Note: Not every graph can have a topological ordering. A graph which contains a cycle can not have a valid ordering.</p>"},{"location":"algorithms/topological_sort/#1-kahns-algorithm","title":"1. Kahn's algorithm","text":"<p>Algorithm Steps:</p> <ul> <li>Compute in-degree (number of incoming edges) for each of the vertex present in the DAG and initialize the count of visited nodes as 0.</li> <li>Pick all the vertices with in-degree as 0 and add them into a queue (Enqueue operation)</li> <li>Remove a vertex from the queue (Dequeue operation) and then.</li> <li>Push node to topological array.</li> <li>Decrease in-degree by 1 for all its neighbouring nodes.</li> <li>If in-degree of a neighbouring nodes is reduced to zero, then add it to the queue.</li> <li>Repeat Step 3 until the queue is empty.</li> <li>If size of topological array is not equal to the number of nodes in the graph then the topological sort is not possible for the given graph.</li> </ul> <pre><code>std::vector&lt;int&gt; inDegree(n, 0);\nfor (int i = 0; i &lt; n; ++i) {\n  for (int j = 0; j &lt; adj[i].size(); ++j) {\n    ++inDegree[adj[i][j]];\n  }\n}\n\nstd::queue&lt;int&gt; queue;\nfor (int i = 0; i &lt; n; ++i) {\n  if (inDegree[i] == 0) {\n    queue.push(i);\n  }\n}\n\nstd::vector&lt;int&gt; topologicalOrdering;\nwhile (!queue.empty()) {\n  auto node = queue.front();\n  queue.pop();\n  topologicalOrdering.push_back(node);\n\n  for (int i = 0; i &lt; adj[node].size(); ++i) {\n    --inDegree[adj[node][i]];\n    if (inDegree[adj[node][i]] == 0) {\n      queue.push(adj[node][i]);\n    }\n  }\n}\n\nif (topologicalOrdering.size() != n) {\n  // Graph contains a cycle\n} else {\n  // The topological ordering is in this vector `topologicalOrdering`\n}\n</code></pre>"},{"location":"algorithms/topological_sort/#2-dfs","title":"2. DFS","text":"<p>Algorithm Steps:</p> <ul> <li>Pick an unvisited node</li> <li>Begin with the selected node, do a DFS exploring only unvisited nodes</li> <li>On the recursive callback of the DFS, add the current node to the topological ordering in the reverse order</li> </ul> <pre><code>T = []\nvisited = []\n\ntopological_sort(cur_vert, N, adj[][]){\n  visited[cur_vert] = true\n  for i = 0 to N\n      if adj[cur_vert][i] is true and visited[i] is false\n        topological_sort(i)\n  T.insert_in_beginning(cur_vert)\n}\n</code></pre>"},{"location":"algorithms/union_find/","title":"Union find (Disjoint set)","text":""},{"location":"algorithms/union_find/#1-disjoint-set-in-math","title":"1. Disjoint set in math","text":"<p>In mathematics, two sets are said to be <code>disjoint sets</code> if they have no element in common. Equivalently, disjoint sets are sets whose intersection is the empty set. For example, {1, 2, 3} and {4, 5, 6} are disjoint sets, while {1, 2, 3} and {3, 4, 5} are not.</p>"},{"location":"algorithms/union_find/#2-disjoint-set-data-structure","title":"2. Disjoint set data structure","text":"<p>Union find(Disjoint set) is a data structure that stores a collection of disjoint (non-overlapping) sets. Its has 2 primary operations:</p> <ul> <li>Find: Determine which subset a particular element is in. This can be used for determining if two elements are in the same subset.</li> <li>Union: Join two subsets into a single subset</li> </ul> <p>Below is the sample code which implements union-find algorithms</p> <pre><code>class DisjointSet {\nprivate:\n  std::vector&lt;int&gt; parents;\n\npublic:\n  DisjointSet(int n) {\n    parents.resize(n);\n    for (int i = 0; i &lt; n; ++i) {\n      parents[i] = i;\n    }\n  }\n\n  int find(int x) {\n    if (parents[x] == x) {\n      return x;\n    }\n\n    return find(parents[x]);\n  }\n\n  void unionSet(int x, int y) {\n    int parentX = find(x);\n    int parentY = find(y);\n\n    parents[parentY] = parentX;\n  }\n};\n</code></pre> <p></p>"},{"location":"algorithms/union_find/#3-optimization","title":"3. Optimization","text":""},{"location":"algorithms/union_find/#path-compression","title":"Path compression","text":"<p>This optimization is designed for speeding up <code>find</code> method.</p> <p>The naive find() method is read-only, when find() is called for an element i, root of the tree is returned, the find() operation traverses up from i to find root.</p> <p>The idea of path compression is to make the found root as parent of i so that we don\u2019t have to traverse all intermediate nodes again. If i is root of a subtree, then path (to root) from all nodes under i also compresses.</p> <p></p> <p>Below is the optimized find() method with Path Compression.</p> <pre><code>int find(int x) {\n  if (parents[x] != x) {\n    parents[x] = find(parents[x]);\n  }\n\n  return parents[x];\n}\n</code></pre> <p>We can also implement <code>find</code> without recursion.</p> <pre><code>int find(int x) {\n  while (parents[x] != x) {\n    parents[x] = parents[parents[x]];\n    x = parents[x];\n  }\n\n  return parents[x];\n}\n</code></pre>"},{"location":"algorithms/union_find/#union-by-rank","title":"Union by rank","text":"<p>In this optimization we will change the <code>unionSet</code> method.</p> <p>In the naive implementation the second tree always got attached to the first one. In practice that can lead to trees containing chains of length O(n).</p> <p></p> <p>The solution is to always attach smaller depth tree under the root of the deeper tree.</p> <p></p> <p>Below is the optimized <code>unionSet()</code> method with Union by rank.</p> <pre><code>void unionSet(int x, int y) {\n  int parentX = find(x);\n  int parentY = find(y);\n  if (parentX == parentY) {\n    return;\n  }\n  if (ranks[parentX] &lt; ranks[parentY]) {\n    parents[parentX] = parentY;\n  }\n  else if (ranks[parentX] &gt; ranks[parentY]) {\n    parents[parentY] = parentX;\n  }\n  else {\n    // If ranks are the same\n    parents[parentX] = parentY;\n    ++ranks[parentY]; \n  }\n}\n</code></pre>"},{"location":"blogs/","title":"List of valuable blog posts","text":"<p>How Discord Stores Billions of Messages </p>"},{"location":"database/btree_vs_b_plus_tree/","title":"Btree vs B+tree","text":""},{"location":"database/btree_vs_b_plus_tree/#btree","title":"Btree","text":"<ul> <li>Balanced Data structure for fast traversal</li> <li>In B-Tree of <code>m</code> degree can have up to <code>m</code> child nodes</li> <li>Node has up to <code>m - 1</code> elements (each element has a key and a value)</li> <li>The value is usually data pointer to the row (data pointer can point to primary key or tuple)</li> </ul>"},{"location":"database/btree_vs_b_plus_tree/#limitation","title":"Limitation","text":"<ul> <li>Elements in all nodes store both the key and the VALUE</li> <li>Internal nodes take more space thus require more IO and can slow down traversal (Hard to fit internal nodes in memory)</li> <li>Range queries are slow because of random access (give me all values 1-5)</li> </ul>"},{"location":"database/btree_vs_b_plus_tree/#btree_1","title":"B+Tree","text":"<ul> <li>Exactly like B-Tree but only stores keys in internal nodes</li> <li>Values are only stored in leaf nodes</li> <li>Internal nodes are smaller since they only store keys and they can fit more elements</li> <li>Leaf nodes are \u201clinked\u201d so once you find a key you can find all values before and after that key.</li> <li>Great for range queries</li> </ul>"},{"location":"database/btree_vs_b_plus_tree/#btree-storage-cost","title":"B+Tree Storage cost","text":""},{"location":"database/cassandra/","title":"Cassandra","text":"<p>Apache Cassandra is a highly-scalable partitioned row store. Rows are organized into tables with a required primary key.</p> <ul> <li>Partitioning means that Cassandra can distribute your data across multiple machines</li> <li>Row store means that like relational databases, Cassandra organizes data by rows and columns.</li> </ul>"},{"location":"database/cassandra/#1-cassandra-architecture","title":"1. Cassandra Architecture","text":"<p>Some of the features of Cassandra architecture are as follows:</p> <ul> <li>Cassandra is designed such that it has no master or slave nodes.</li> <li>It has a ring-type architecture, that is, its nodes are logically distributed like a ring.</li> <li>Data is automatically distributed across all the nodes.</li> <li>Similar to HDFS, data is replicated across the nodes for redundancy.</li> <li>Data is kept in memory and lazily written to the disk.</li> <li>Hash values of the keys are used to distribute the data among nodes in the cluster.</li> </ul> <p></p> <p>Additional features of Cassandra architecture are:</p> <ul> <li>Cassandra architecture supports multiple data centers.</li> <li>Data can be replicated across data centers.</li> </ul> <p>You can keep three copies of data in one data center and the fourth copy in a remote data center for remote backup. Data reads prefer a local data center to a remote data center.</p> <p></p>"},{"location":"database/cassandra/#node","title":"Node","text":"<p>Node is the place where data is stored. It is the basic component of Cassandra.</p>"},{"location":"database/cassandra/#rack","title":"Rack","text":"<p>A rack is a group of machines housed in the same physical box.</p> <p></p> <ul> <li>All machines in the rack are connected to the network switch of the rack</li> <li>The rack\u2019s network switch is connected to the cluster.</li> <li>All machines on the rack have a common power supply. It is important to notice that a rack can fail due to two reasons: a network switch failure or a power supply failure.</li> <li>If a rack fails, none of the machines on the rack can be accessed. So it would seem as though all the nodes on the rack are down.</li> </ul>"},{"location":"database/cassandra/#datacenter","title":"Datacenter","text":"<p>A datacenter is a logical set of racks/ nodes. A common use case is AWS-EAST vs AWS-WEST...</p>"},{"location":"database/cassandra/#cluster","title":"Cluster","text":"<p>The cluster is the collection of many data centers.</p> <p></p>"},{"location":"database/cassandra/#2-data-distribution-and-replication","title":"2. Data distribution and replication","text":""},{"location":"database/cassandra/#21-data-partitions","title":"2.1 Data Partitions","text":"<p>A partition key is converted to a token by a partitioner. The tokens are signed integer values between -2^63 to +2^63-1, and this range is referred to as token range.</p> <p>If we consider there are only 100 tokens used for a Cassandra cluster with three nodes. Each node is assigned approximately 33 tokens like</p> <pre><code>node1: 0-33\nnode2: 34-66\nnode3: 67-99\n</code></pre> <p>If there are nodes added or removed, the token range distribution should be shuffled to suit the new topology. This process takes a lot of calculation and configuration change for each cluster operation.</p> <p></p> <p>If one node is removed, data in removed node is placed on the next neighbor node in clockwise manner.</p> <p></p>"},{"location":"database/cassandra/#22-virtual-nodesvnodes","title":"2.2 Virtual nodes/Vnodes","text":"<p>Virtual nodes in a Cassandra cluster are also called vnodes. Vnodes can be defined for each physical node in the cluster. Each node in the ring can hold multiple virtual nodes.</p> <p>The default number of Vnodes owned by a node in Cassandra is <code>256</code>, which is set by <code>num_tokens</code> property. When a node is added into a cluster, the token allocation algorithm allocates tokens to the node. The algorithm selects random token values to ensure uniform distribution.</p> <p>In your case you have 6 nodes, each set with 256 token ranges so you have 6*256 token ranges and each psychical node contains 256 token ranges.</p> <p></p>"},{"location":"database/cassandra/#23-replication","title":"2.3 Replication","text":"<p>The data in each keyspace is replicated with a replication factor. There is one primary replica of data that resides with the token owner node as explained in the data partitioning section. The remainder of replicas is placed by Cassandra on specific nodes using the replica placement strategy.</p> <p>The total number of replicas for a keyspace across a Cassandra cluster is referred to as the keyspace's replication factor. A replication factor of one means that there is only one copy of each row in the Cassandra cluster. A replication factor of two means there are two copies of each row, where each copy is on a different node. All replicas are equally important; there is no primary or master replica.</p> <p>There are two settings that mainly impact replica placement:</p> <ul> <li>First is snitch, which determines the data center, and the rack a Cassandra node belongs to, and it is set at the node level</li> <li>The second setting is the replication strategy. The replication strategy is set at the keyspace level. There are two strategies: SimpleStrategy and NetworkTopologyStrategy.</li> </ul> <p>SimpleStrategy: does not consider racks and multiple data centers. It places data replicas on nodes sequentially. NetworkTopologyStrategy: is rack aware and data center aware</p> <p></p>"},{"location":"database/cassandra/#24-consistency-level","title":"2.4 Consistency level","text":"<p>The Cassandra consistency level is defined as the minimum number of Cassandra nodes that must acknowledge a read or write operation before the operation can be considered successful. Different consistency levels can be assigned to different Edge keyspaces.</p> <p>You can find all cassandra's consistency level here</p> <p></p> <p>Write Consistency</p> <ol> <li>A client sends a write request to the coordinator.</li> <li>The coordinator forwards the write request (INSERT, UPDATE or DELETE) to all replica nodes whatever write CL you have set.</li> <li>The coordinator waits for n number of replica nodes to respond. n is set by the write CL.</li> <li>The coordinator sends the response back to the client.</li> </ol> <p>Read Consistency</p> <ol> <li>A client sends a read request to the coordinator.</li> <li>The coordinator forwards the read (SELECT) request to n number of replica nodes. n is set by the read CL.</li> <li>The coordinator waits for n number of replica nodes to respond.</li> <li>The coordinator then merges (finds out most recent copy of written data) the n number of responses to a single response and sends response to the client.</li> </ol>"},{"location":"database/cassandra/#3-data-storage","title":"3. Data storage","text":"<p>Cassandra processes data at several stages on the write path, starting with the immediate logging of a write and ending in with a write of data to disk:</p> <ul> <li>Logging data in the commit log</li> <li>Writing data to the memtable</li> <li>Flushing data from the memtable</li> <li>Storing data on disk in SSTables</li> </ul> <p></p> <ul> <li>Commitlogs are an append only log of all mutations local to a Cassandra node. Any data written to Cassandra will first be written to a commit log before being written to a memtable. This provides durability in the case of unexpected shutdown. On startup, any mutations in the commit log will be applied to memtables.</li> <li>Memtables are in-memory structures where Cassandra buffers writes. In general, there is one active memtable per table. Eventually, memtables are flushed onto disk and become immutable SSTables. This can be triggered in several ways:</li> <li>The memory usage of the memtables exceeds the configured threshold (see <code>memtable_cleanup_threshold</code>)</li> <li>The <code>commit-log</code> approaches its maximum size, and forces memtable flushes in order to allow commitlog segments to be freed</li> <li>SSTables are the immutable data files that Cassandra uses for persisting data on disk.</li> </ul>"},{"location":"database/cassandra/#4-data-model","title":"4. Data Model","text":"<p>The Cassandra data model uses the same terms as Google BigTable, for example, column family, column, row, etc. Some of these terms also exist in the relational data model but have different meanings.</p> <p>Keyspace</p> <p>A keyspace is analogous to a schema or database in a relational model. Each Cassandra cluster has a system keyspace to store system-wide metadata. Keyspace contains replication settings that control how data is distributed and replicated in clusters.</p> <p>Syntax:</p> <pre><code>Create keyspace KeyspaceName with replication={'class':strategy name, 'replication_factor': No of replications on different nodes};\n</code></pre> <p>Strategy:</p> <ul> <li>Simple Strategy: Simple strategy is used when you have just one data center. In this strategy, the first replica is placed on the node selected by the partitioner. Remaining nodes are placed in the clockwise direction in the ring without considering rack or node location.</li> <li>Network Topology Strategy: Network topology strategy is used when you have more than one data centers. In this strategy, you have to provide replication factor for each data center separately. Network topology strategy places replicas in nodes in the clockwise direction in the same data center. This strategy attempts to place replicas in different racks.</li> </ul> <p>Replication Factor:</p> <p>Replication factor is the number of replicas of data placed on different nodes. For no failure, 3 is good replication factor. More than two replication factor ensures no single point of failure. Sometimes, the server can be down, or network problem can occur, then other replicas provide service with no failure.</p> <p>Eg:</p> <pre><code>Create keyspace University with replication={'class':SimpleStrategy,'replication_factor': 3};\n</code></pre> <p>Column Family</p> <p></p> <p>A row key in the column family must be unique and be used to identify rows. Although not the same, the column family can be analogous to a table in a relational database. Column families provide greater flexibility by allowing different columns in different rows.</p> <p>Syntax:</p> <pre><code>Create table KeyspaceName.TableName\n(\nColumnName DataType,\nColumnName DataType,\nColumnName DataType\n...\nPrimary key(ColumnName)\n) with PropertyName=PropertyValue;\n</code></pre> <ul> <li>Single Primary Key</li> </ul> <pre><code>Primary key (ColumnName)\n</code></pre> <p>In the single primary key, there is only a single column. That column is also called partitioning key. Data is partitioned on the basis of that column.</p> <ul> <li>Compound Primary Key</li> </ul> <pre><code>Primary key(ColumnName1,ColumnName2 . . .)\n</code></pre> <p>In above syntax, ColumnName1 is the partitioning key and ColumnName2 is the Clustering key. Data will be partitioned on the basis of ColumnName1 and data will be clustered on the basis of ColumnName2. Clustering is the process that sorts data in the partition.</p> <ul> <li>Compound Partitioning key</li> </ul> <pre><code>Primary Key((ColumnName1,ColumnName2),ColumnName3...))\n</code></pre> <p>In above syntax, ColumnName1 and ColumnName2 are the compound partition key. Data will be partitioned on the basis of both columns ColumnName1 and ColumnName2 and data will be clustered on the basis of the ColumnName3.</p> <p>Cassandra index</p> <p>Cassandra creates indexes on the data during the \u2018create index\u2019 statement execution.</p> <ul> <li>After creating an index, Cassandra indexes new data automatically when data is inserted.</li> <li>The index cannot be created on primary key as a primary key is already indexed.</li> <li>Indexes on collections are not supported in Cassandra.</li> <li>Without indexing on the column, Cassandra can\u2019t filter that column unless it is a primary key.</li> </ul> <p>That\u2019s why, for filtering columns in Cassandra, indexes needs to be created.</p> <p>Syntax</p> <pre><code>Create index IndexName on KeyspaceName.TableName(ColumnName);\n</code></pre> <p>Row</p> <p>Each row consists of a row key \u2014 also known as the primary key \u2014 and a set of columns, as shown in the following figure.</p> <p></p> <p>Each row may have different column names. That is why Cassandra is row-oriented and column-oriented. There are no timestamps for the row.</p> <p>Column</p> <p>A column is the smallest data model element in Cassandra. Although it also exists in a relational database, the column in Cassandra is different. The figure below shows that each column consists of a column name, column value, timestamp, and TTL ( Time-To-Live ).</p> <p></p> <p>The timestamp is used for conflict resolution by client applications during write operations. Time-To-Live is an optional expiration value that is used to mark columns that are deleted after expiration.</p> <p>Time-To-Live:</p> <p>During data insertion, you have to specify \u2018ttl\u2019 value in seconds. \u2018ttl\u2019 value is the time to live value for the data. After that particular amount of time, data will be automatically removed.</p> <p>For example, specify ttl value 100 seconds during insertion. Data will be automatically deleted after 100 seconds. When data is expired, that expired data is marked with a tombstone.</p> <p>A tombstone exists for a grace period. After data is expired, data is automatically removed after compaction process.</p> <p>Syntax:</p> <pre><code>Insert into KeyspaceName.TableName(ColumnNames) values(ColumnValues) using ttl TimeInseconds;\n</code></pre> <p>Cassandra Data Model Rules</p> <p>Cassandra does not support joins, group by, OR clause, aggregations, etc. So you have to store your data in such a way that it should be completely retrievable. So these rules must be kept in mind while modelling data in Cassandra.</p> <p>What cassandra does not support:</p> <ol> <li>CQL does not support aggregation queries like max, min, avg.</li> <li>CQL does not support group by, having queries.</li> <li>CQL does not support joins.</li> <li>CQL does not support OR queries.</li> <li>CQL does not support wildcard queries.</li> <li>CQL does not support Union, Intersection queries.</li> <li>Table columns cannot be filtered without creating the index.</li> <li>Greater than (&gt;) and less than (&lt;) query is only supported on clustering column.</li> </ol> <p>Cassandra query language is not suitable for analytics purposes because it has so many limitations.</p>"},{"location":"database/cassandra/#5-data-modeling-in-cassandra-example","title":"5. Data Modeling in Cassandra example","text":""},{"location":"database/cassandra/#51-facebook-posts","title":"5.1 Facebook Posts","text":"<p>Suppose that we are storing Facebook posts of different users in Cassandra. One of the common query patterns will be fetching the top <code>N</code> posts made by a given user. Thus we need to store all data for a particular user on a single partition</p> <pre><code>CREATE TABLE posts_facebook (\n  user_id uuid,\n  post_id timeuuid,\n  content text,\n  PRIMARY KEY (user_id, post_id) )\nWITH CLUSTERING ORDER BY (post_id DESC);\n</code></pre> <p>Now, let's write a query to find the top 20 posts for the user Anna:</p> <pre><code>SELECT content FROM posts_facebook WHERE user_id = \"Anna_id\" LIMIT 20\n</code></pre>"},{"location":"database/cassandra/#52-gyms-across-the-country","title":"5.2 Gyms Across the Country","text":"<p>Suppose that we are storing the details of different partner gyms across the different cities and states of many countries and we would like to fetch the gyms for a given city.</p> <p>Also, let's say we need to return the results having gyms sorted by their opening date.</p> <pre><code>CREATE TABLE gyms_by_city (\n country_code text,\n state text,\n city text,\n gym_name text,\n opening_date timestamp,\n PRIMARY KEY (\n   (country_code, state_province, city),\n   (opening_date, gym_name))\n WITH CLUSTERING ORDER BY (opening_date ASC, gym_name ASC);\n</code></pre> <p>Now, let's look at a query that fetches the first ten gyms by their opening date for the city of Phoenix within the U.S. state of Arizona:</p> <pre><code>SELECT * FROM gyms_by_city\n  WHERE country_code = \"us\" AND state = \"Arizona\" AND city = \"Phoenix\"\n  LIMIT 10\n</code></pre> <p>Next, let\u2019s see a query that fetches the ten most recently-opened gyms in the city of Phoenix within the U.S. state of Arizona:</p> <pre><code>SELECT * FROM gyms_by_city\n  WHERE country_code = \"us\" and state = \"Arizona\" and city = \"Phoenix\"\n  ORDER BY opening_date DESC\n  LIMIT 10\n</code></pre> <p>Note: As the last query's sort order is opposite of the sort order defined during the table creation, the query will run slower as Cassandra will first fetch the data and then sort it in memory.</p>"},{"location":"database/cassandra/#53-e-commerce-customers-and-products","title":"5.3 E-commerce Customers and Products","text":"<p>Let's say we are running an e-commerce store and that we are storing the Customer and Product information within Cassandra. Let's look at some of the common query patterns around this use case:</p> <ol> <li>Get Customer info</li> <li>Get Product info</li> <li>Get all Customers who like a given Product</li> <li>Get all Products a given Customer likes</li> </ol> <p>We will start by using separate tables for storing the Customer and Product information. However, we need to introduce a fair amount of denormalization to support the 3rd and 4th queries shown above.</p> <p>We will create two more tables to achieve this \u2013 \u201cCustomer_by_Product\u201d and \u201cProduct_by_Customer\u201c.</p> <pre><code>CREATE TABLE Customer (\n  cust_id text,\n  first_name text,\n  last_name text,\n  registered_on timestamp,\n  PRIMARY KEY (cust_id));\n\nCREATE TABLE Product (\n  prdt_id text,\n  title text,\n  PRIMARY KEY (prdt_id));\n\nCREATE TABLE Customer_By_Liked_Product (\n  liked_prdt_id text,\n  liked_on timestamp,\n  title text,\n  cust_id text,\n  first_name text,\n  last_name text,\n  PRIMARY KEY (prdt_id, liked_on));\n\nCREATE TABLE Product_Liked_By_Customer (\n  cust_id text,\n  first_name text,\n  last_name text,\n  liked_prdt_id text,\n  liked_on timestamp,\n  title text,\n  PRIMARY KEY (cust_id, liked_on));\n</code></pre> <p>Query to find the ten Customers who most recently liked the product \u201cPepsi\u201c:</p> <pre><code>SELECT * FROM Customer_By_Liked_Product WHERE title = \"Pepsi\" LIMIT 10\n</code></pre> <p>Query that finds the recently-liked products (up to ten) by a customer named \u201cAnna\u201c:</p> <pre><code>SELECT * FROM Product_Liked_By_Customer WHERE first_name = \"Anna\" LIMIT 10\n</code></pre>"},{"location":"database/lsm_tree/","title":"LSM Trees","text":"<p>A log-structured merge-tree (LSM tree) is a data structure typically used when dealing with write-heavy workloads.</p> <p>The write path is optimized by only performing sequential writes. LSM trees are the core data structure behind many databases, including BigTable, Cassandra, Scylla, and RocksDB.</p>"},{"location":"database/lsm_tree/#hash-indexes","title":"Hash Indexes","text":"<p>Hash indexes are basically key-value hash maps.</p> <p>Let\u2019s say our data storage is just appending to a file, then the simplest possible indexing strategy is this: keep an in-memory hash map where every key is mapped to a byte offset in the data file</p> <p></p> <p>Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote (this works both for inserting new keys and for updating existing keys)</p> <p>When you want to look up a value, use the hash map to find the offset in the data file, seek to that location and read the value.</p> <p>This may sound simplistic, but it is a viable approach. In fact, this is essentially what Bitcask (the default storage engine in Riak) does. Bitcask offers high-performance reads and writes, subject to the requirement that all the keys fit in the available RAM, since the hash map is kept completely in memory.</p> <p>A storage engine like Bitcask is well suited to situations where the value for each key is updated frequently. For example, the key might be the URL of a cat video, and the value might be the number of times it has been played (incremented every time someone hits the play button). In this kind of workload, there are a lot of writes, but there are not too many distinct keys\u2014you have a large number of writes per key, but it\u2019s feasible to keep all keys in memory.</p> <p>There are many optimizations to reclaim needlessly occupied space by log files, for example, we can use multiple log files, so when one log file goes out of capacity, it gets frozen and new writes are done in a new log file. Frozen log files can later be compacted (duplicate keys are removed and only the most recent value of a key is kept).</p> <p>Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to find the value for a key, we first check the most recent segment\u2019s hash map; if the key is not present we check the second-most-recent segment, and so on. The merging process keeps the number of segments small, so lookups don\u2019t need to check many hash maps.</p> <p>However, the hash table index also has limitations:</p> <ul> <li>The hash table must fit in memory, so if you have a very large number of keys, you\u2019re out of luck</li> <li>Range queries are not efficient. For example, you cannot easily scan over all keys between kitty00000 and kitty99999 \u2014you\u2019d have to look up each key individually in the hash maps.</li> </ul>"},{"location":"database/lsm_tree/#sstables","title":"SSTables","text":"<p>As we saw in hash tables, new logs are added to the end of the table, so logs at the end of the table take precedence over logs that came earlier in the table and this is how hash tables detect updates. Other than that, the order of keys in the table is irrelevant. As we explained in the last part of the previous section, hash tables have two major limitations, namely, having to entirely fit in memory and not being able to serve range queries efficiently.</p> <p>Now let's consider this, what if we can store all segments on disk and only store a few keys in memory and build a mechanism that would allow us to use the few keys stored in memory to know which segment(s) to load from disk in search of some key? That would eliminate the first limitation of having to fit all segments in memory.</p> <p>This is exactly what Sorted Strings Table (SSTables) do, they store all segments on disk and only keep a few keys in memory, but with a minor twist, those keys kept in memory will have to be sorted.</p> <p></p> <p>In the previous diagram, let's say we are looking for the key <code>handiwork</code>, because the keys in the SSTable are sorted, we know it must fall between <code>handbag</code> and <code>handsome</code>, so we can start from the offset of <code>handbag</code> and read all segments until we reach <code>handsome</code> to locate the needed key or determine if it doesn't exist at all.</p> <p>This new structure also helps us to solve the second limitation of not being able to perform range queries efficiently, since we sort keys now.</p>"},{"location":"database/lsm_tree/#constructing-and-maintaining-sstables","title":"Constructing and maintaining SSTables","text":"<p>The main question is how can we construct the SSTable so that all keys are in sorted order. Luckily, we have a data structure can maintain the sorted key: AVL tree or read-black tree</p> <p>We can now make our storage engine work as follows:</p> <ul> <li>When a write comes in, add it to an in-memory balanced tree data structure (for example, a red-black tree). This in-memory tree is sometimes called a memtable.</li> <li>When the memtable gets bigger than some threshold, typically a few megabytes, write it out to disk as an SSTable file. This can be done efficiently because the tree already maintains the key-value pairs sorted by key.</li> <li>In order to serve a read request, first try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc.</li> <li>From time to time, run a merging and compaction process in the background to combine segment files and to discard overwritten or deleted values.</li> </ul> <p>This scheme looks good so far, the only limitation that's clear now, is what if the database crashes? The recent writes that are still in the memtable but not yet persisted on disk will be lost.</p> <p>To solve this, most databases maintain what's called a write-ahead-log (WAL) which is a separate file (on disk) to which new writes are written (in the order they come, just appends).</p> <p>Now if the database crashes, the WAL file can be read and the recent writes that were in the memtable but not on disk can be recovered. In order to keep this WAL file at a constant size, when each memtable instance is flushed the disk, the corresponding WAL file is deleted because it's no longer needed.</p>"},{"location":"database/lsm_tree/#writing-data","title":"Writing Data","text":"<p>When a write comes in, add it to an in-memory balanced tree data structure (memtable). When the memtable gets bigger than some threshold, typically a few megabytes, write it out to disk as an SSTable file.</p> <p>We can also apply write-ahead-log (WAL) technique to avoid the case memtable not persisted on disk when the server crash.</p> <p></p>"},{"location":"database/lsm_tree/#reading-data","title":"Reading Data","text":"<p>In order to serve a read request, first try to find the key in the memtable, then in the on-disk segments. But how can we identify if the segment contains our record. This is something that a bloom filter can help us out with. A bloom filter is a space-efficient data structure that can tell us if a value is missing from our data.</p>"},{"location":"database/lsm_tree/#deleting-data","title":"Deleting Data","text":"<p>How do you delete data from the SSTable when the segment files are considered immutable? Deletes actually follow the exact same path as writing data.</p> <p>Whenever a delete request is received, a unique marker called a tombstone is written for that key.</p> <p></p>"},{"location":"database/lsm_tree/#compaction","title":"Compaction","text":"<p>Over time, this system will accumulate more segment files as it continues to run. These segment files need to be cleaned up and maintained in order to prevent the number of segment files from getting out of hand. This is the responsibility of a process called compaction.</p> <p>Compaction is a background process that is continuously combining old segments together into newer segments. Since we sort keys now, we can make segments also sorted by key during compaction. Notice that during compaction, we can remove duplicate keys (perform updates) while writing the new compacted segment. This can be easily done while segments are resident on disk using an algorithm that's very similar to a normal merge sort</p> <p></p>"},{"location":"database/lsm_tree/#disadvantages-of-lsm-tree","title":"Disadvantages of LSM Tree","text":"<ul> <li>Compaction process sometime interfere with the performance of ongoing reads and writes.</li> <li>Although use of bloom filter some how increase the performance in case of key not present, but if key exist then each key may exist at multiple places and thus checking if a key doesn\u2019t exist needs all segments to be scanned.</li> </ul>"},{"location":"database/mysql/","title":"MySQL","text":""},{"location":"database/mysql/#1-clustered-index-and-non-clustered-index","title":"1. Clustered index and non-clustered index","text":"<p>A clustered index is a B-Tree index whose leaf nodes are the actual data blocks on disk. In cluseted index the data are stored physically on the disk in the same order as the clustered index. Therefore, there can be only one clustered index.</p> <p>A non-clustered index is a B-Tree index whose leaf nodes point to the clustered index key. We can have multiple non-clustered indexes per table.</p> <p>Clustered indexes are faster than non-clustered indexes since they don\u2019t involve any extra lookup step. In clustered index we just only traverse the tree once however in non-clustered index we need to do it twice. The first one for getting clustered index key in non-clustered B-tree and the second one for getting actual data from clustered B-tree.</p> <p>When you define a primary key for an InnoDB table, MySQL uses the primary key as the clustered index. If you do not have a primary key for a table, MySQL will search for the first UNIQUE index where all the key columns are NOT NULL and use this UNIQUE index as the clustered index. In case the InnoDB table has no primary key or suitable UNIQUE index, MySQL internally generates a hidden clustered index named GEN_CLUST_INDEX on a synthetic column that contains the row ID values.</p>"},{"location":"database/mysql/#2-mysql-innodb-vs-myisam","title":"2. MySQL InnoDB vs MyISAM","text":"InnnoDB MyISAM Row level locking Table level locking Supports foreign key Does not support relationship constraints Transactional (Rollback, commit) Non-transactional ACID compliant Not ACID compliant Row data stored in pages as per primary key order No particular order for data stored"},{"location":"database/nosql/","title":"NoSQL","text":"<p>NoSQL DB is a database used to manage huge sets of unstructured data, where the data is not stored in tabular relations like relational databases.</p>"},{"location":"database/nosql/#1-base-properties","title":"1. BASE properties","text":"<p>The relational databases strongly follow the ACID properties while the NoSQL databases follow BASE principles. In comparison with the CAP Theorem, BASE chooses availability over consistency.</p> <ul> <li>Basic Availability: The system guarantees availability. There will be a response to any request (can be failure too).</li> <li>Soft state: The data stored in the system may change because of the eventual consistency model (even without an input, the system state may change).</li> <li>Eventual consistency: The system will eventually become consistent once it stops receiving input.</li> </ul>"},{"location":"database/offset_is_bad/","title":"Offset is bad","text":"<p>We often use <code>offset</code> in our database queries to fetch paginated data. This helps in implementing server-side pagination where it is not convenient to fetch all the data at once for performance reasons.</p>"},{"location":"database/offset_is_bad/#why-offset-is-bad","title":"Why offset is bad?","text":"<p>Performing offset on your database affects performance because of the way database fetches the rows. Offset happens after the database has fetched all the matching rows.</p> <p></p> <p>As shown above, the number of rows that database fetches is <code>offset + limit</code> instead of just limit.</p>"},{"location":"database/offset_is_bad/#how-to-avoid-offset","title":"How to avoid offset?","text":"<p>As shown in this article</p> <pre><code>SELECT someCol, anotherCol\n  FROM someTable\n WHERE 'some_condition' &gt; 1\n   AND id &lt; ?last_seen_id\n ORDER BY id DESC\n FETCH FIRST 10 ROWS ONLY\n</code></pre> <p>This is called <code>keyset_pagination</code> and it offers the advantages of being faster than offset and also prevents the strange duplication of resuls, or other frustrating anomalies.</p> <p>There are some downsides however. You can\u2019t go to an arbitrary page for example, so it\u2019s a method better suited to things like infinite scrolling in a way similar to Instagram, than it is to clicking on page numbers. It\u2019s a judgment call as to what will fit your use-case the best.</p>"},{"location":"database/offset_is_bad/#references","title":"References","text":"<ul> <li>https://mysql.rjweb.org/doc.php/pagination </li> <li>https://use-the-index-luke.com/no-offset </li> </ul>"},{"location":"database/sql_cheat_sheet/","title":"SQL cheat sheet","text":""},{"location":"database/transactions/","title":"Transactions","text":""},{"location":"database/transactions/#1-acid-properties","title":"1. ACID properties","text":"<p>ACID is a set of properties of database transaction intended to guarantee data validity despite errors.</p>"},{"location":"database/transactions/#atomicity","title":"Atomicity","text":"<p>Transactions are often composed of multiple stataments. Atomicity guarantees that each transaction is treated as a single unit, which either succeeds completely or fails completely. If any of the statements constituting a transaction fails to complete, the entire transaction fails and the database is left unchanged.</p>"},{"location":"database/transactions/#consistency","title":"Consistency","text":"<p>The data is in consistent state before and after transaction.</p> <p>Eg: We have a transaction to transfer money from account A to account B. The total amount before and after the transaction must be the same.</p> <p>The Consistency in data is defined by the user. Eg: unique constraint, foreign key constraint</p>"},{"location":"database/transactions/#isolation","title":"Isolation","text":"<p>If the multiple transactions are running concurrently, they should not be affected by each other.</p> <p>(It depends on the isolation level that we are using)</p>"},{"location":"database/transactions/#durability","title":"Durability","text":"<p>Changes that have been committed to the database should remain even in the case of software and system failure.</p>"},{"location":"database/transactions/#2-read-phenomena","title":"2. Read phenomena","text":""},{"location":"database/transactions/#dirty-read","title":"Dirty read","text":"<p>A dirty read is the situation when a transaction reads a uncommitted data from other transactions.</p>"},{"location":"database/transactions/#non-repeatable-read","title":"Non-repeatable read","text":"<p>Non repeatable read occurs when a transaction reads same row twice, and get a different value each time.</p> <p>For example, suppose transaction T1 reads data. Due to concurrency, another transaction T2 updates the same data and commit, Now if transaction T1 rereads the same data, it will retrieve a different value.</p>"},{"location":"database/transactions/#phantom-read","title":"Phantom read","text":"<p>Phantom read occurs when two same range queries are executed, but the number of rows retrieved by the two, are different.</p> <p>For example, suppose transaction T1 retrieves a set of rows that satisfy some search criteria. Now, Transaction T2 generates some new rows that match the search criteria for transaction T1. If transaction T1 re-executes the statement that reads the rows, it gets a different set of rows this time.</p>"},{"location":"database/transactions/#3-isolation-levels","title":"3. Isolation levels","text":"<p>Serializalble: This is the highest isolation level. Concurrent transactions are guaranteed to be executed in sequence.</p> <p>Repeatable Read: Data read during the transaction stays the same as the transaction starts.</p> <p>Read Committed: Data modification can only be read after the transaction is committed.</p> <p>Read Uncommitted: The data modification can be read by other transactions before a transaction is committed.</p> <p>The isolation is guaranteed by MVCC (Multi-Version Consistency Control) and locks.</p> <p> </p> <p>The diagram above takes Repeatable Read as an example to demonstrate how MVCC works:</p> <ul> <li>There are two hidden columns for each row: transaction_id and roll_pointer</li> <li>When transaction A starts, a new Read View with transaction_id=201 is created.</li> <li>Shortly afterward, transaction B starts, and a new Read View with transaction_id=202 is created. </li> <li>Now transaction A modifies the balance to 200, a new row of the log is created, and the roll_pointer points to the old row</li> <li>Before transaction A commits, transaction B reads the balance data. Transaction B finds that transaction_id 201 is not committed, it reads the next committed record(transaction_id=200).</li> <li>Even when transaction A commits, transaction B still reads data based on the Read View created when transaction B starts. So transaction B always reads the data with balance=100. </li> </ul> <p>Note: In <code>Repeatable Reads</code> isolation or higher if we are updating row data at the same time another concurrent transaction is also updating the same row then we are getting an error <code>ERROR: could not serialize access due to concurrent update</code> (In MySQL we need Serializable isolation level).</p>"},{"location":"database/dynamodb/data_model/","title":"DynamoDB Data Model","text":""},{"location":"database/dynamodb/data_model/#tables","title":"Tables","text":"<p>Tables are the basic store of data. We store data as items in our tables. However, tables in DynamoDB are schema-less. There is no fixed column structure that we have to specify and follow. We can literally put anything inside our table, as long as the primary key is kept unique.</p>"},{"location":"database/dynamodb/data_model/#items","title":"Items","text":"<p>Items in DynamoDB are similar to the rows in relational databases. An item belongs to a table and can contain multiple attributes. An item in DynamoDB can also be represented as a JSON object (a collection of key-value pairs). Size of the item should not exceed 400KB.</p>"},{"location":"database/dynamodb/data_model/#attributes","title":"Attributes","text":"<p>Each individual key-value pair of an item is known as an attribute. We can think of attributes as the properties of the item when we think of the item as a JSON object.</p>"},{"location":"database/dynamodb/data_model/#primary-key","title":"Primary Key","text":"<p>Each table in DynamoDB contains a primary key. A primary key is a special set of attributes. Its value is unique for every item and is used to identify the item in the database. Under the hood, it is used to partition and store the data in order.</p> <p>There are two types of primary keys:</p> <ul> <li>Partition key: Here, we have a unique key of scalar type (string, number, boolean), which determines the storage partition the item will go into.</li> <li>Partition key and Sort key: Here, we have two keys. The partition key determines the partition where the item goes into the storage and the sort key determines the rank of the item in the partition. Their combination should be unique.</li> </ul>"},{"location":"database/dynamodb/data_model/#sort-key","title":"Sort key","text":"<p>Sort key is also known as range key. Ideally, we do not need a range key. It is optional. However, it comes in handy when:</p> <ul> <li>Our partition key is not unique by itself. In this case, we can use a combination of partition key and range key as the primary key.</li> <li>Our access patterns require us to get values in some range. For example, if we have an order table partitioned on the <code>customer_id</code>, we may have an access pattern to get all the orders with less than $400 amount. Now to get this access pattern, if we only rely on the customer_id you will be in trouble. But if we make the amount as the sort key, we can easily get the results.</li> </ul>"},{"location":"database/dynamodb/data_model/#data-types","title":"Data types","text":"<p>Scalar</p> <ul> <li>S, String</li> <li>N, Number</li> <li>BOOL, Boolean</li> <li>B, Byte Buffer</li> <li>Null, Null</li> </ul> <p>Document</p> <ul> <li>M, Map</li> <li>L, List</li> </ul> <p>Set</p> <ul> <li>SS, String Set</li> <li>NS, Number Set</li> <li>BS, Binary Set</li> </ul>"},{"location":"database/dynamodb/data_model/#time-to-live-ttl","title":"Time to Live (TTL)","text":"<p>Time To Live (TTL) for DynamoDB is a cost-effective method for deleting items that are no longer relevant. TTL allows you to define a per-item expiration timestamp that indicates when an item is no longer needed. DynamoDB automatically deletes expired items within a few days of their expiration time, without consuming write throughput.</p> <p>To use TTL, first enable it on a table and then define a specific attribute to store the TTL expiration timestamp. The timestamp must be stored in Unix epoch time format at the seconds granularity. Each time an item is created or updated, you can compute the expiration time and save it in the TTL attribute.</p>"},{"location":"database/dynamodb/data_model/#capacity-units","title":"Capacity Units","text":"<p>Amazon DynamoDB has two read/write capacity modes for processing reads and writes on your tables:</p> <ul> <li>On-demand</li> <li>Provisioned (default, free-tier eligible)</li> </ul> <p>The read/write capacity mode controls how you are charged for read and write throughput and how you manage capacity. You can set the read/write capacity mode when creating a table or you can change it later.</p> <p>Write capacity</p> <ul> <li>One write request unit represents one write for an item up to 1 KB in size.</li> <li>Transactional write requests require two write request units to perform one write for items up to 1 KB</li> </ul> <p>Eg:</p> <ul> <li>Size of one item (KB): 21KB</li> <li>Write capacity: 150</li> <li>Strong consistency write per second: 150 / (21 * 2) = 3</li> <li>Eventually consistency write per second: 150 / 21 = 7</li> </ul> <p>Read capacity</p> <p>DynamoDB read requests can be either strongly consistent, eventually consistent, or transactional.</p> <ul> <li>A strongly consistent read request of an item up to 4 KB requires one read request unit.</li> <li>An eventually consistent read request of an item up to 4 KB requires one-half (1/2) read request unit.</li> <li>A transactional read request of an item up to 4 KB requires two read request units.</li> </ul> <p>If you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units required depends on the item size, and whether you want an eventually consistent or strongly consistent read.</p> <p>Eg: </p> <ul> <li>Size of one item (KB): 21KB</li> <li>Read capacity: 150</li> <li>Strong consistency write per second: 150 / (21 / 4) = 28</li> <li>Eventually consistency read per second: 150 / (21 / 4) * (1 / 2) = 57</li> </ul> <p>Sample command to create DynamoDB table</p> <pre><code>aws dynamodb create-table \\\n    --table-name Blog \\\n    --attribute-definitions \\\n    AttributeName=Author,AttributeType=S \\\n    AttributeName=Topic_Title,AttributeType=S \\\n    --key-schema \\\n    AttributeName=Author,KeyType=HASH \\\n    AttributeName=Topic_Title,KeyType=RANGE \\\n    --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1\n</code></pre> <p>In this table, I have used the table-name as Blog. And I have used the following primary keys:</p> <ul> <li>Partition key: Author</li> <li>Range key: Topic_Title (The range key is the concatenation of the topic of the post and its title. Both the keys are of string datatype)</li> </ul>"},{"location":"database/dynamodb/read_write_data/","title":"Read and Write data to DynamoDB","text":""},{"location":"database/dynamodb/read_write_data/#write-data","title":"Write data","text":"<p>PutItem</p> <pre><code>aws dynamodb put-item \\\n    --table-name Blog \\\n    --item \"{ \\\n        \\\"Author\\\" : { \\\"S\\\" : \\\"Seth Godin\\\"}, \\\n        \\\"Topic_Title\\\" : { \\\"S\\\" : \\\"Marketing, This is Marketing\\\" } }\" \\\n    --return-consumed-capacity TOTAL\n</code></pre> <p>--return-consumed-capacity determines the level of detail about either provisioned or on-demand throughput consumption that is returned in the response:</p> <ul> <li>INDEXES - The response includes the aggregate ConsumedCapacity for the operation, together with ConsumedCapacity for each table and secondary index that was accessed. Note that some operations, such as GetItem and BatchGetItem, do not access any indexes at all. In these cases, specifying INDEXES will only return ConsumedCapacity information for table(s).</li> <li>TOTAL - The response includes only the aggregate ConsumedCapacity for the operation.</li> <li>NONE - No ConsumedCapacity details are included in the response.</li> </ul> <p>Note: If the primary key matches with a primary key of an item already present, the new item will REPLACE the old item.</p> <p>BatchWriteItem</p> <pre><code>{\n  \"Blog\" : [\n    {\n      \"PutRequest\": {\n        \"Item\": {\n          \"Author\" : {\"S\":\"Huy\"},\n          \"Topic_Title\" : {\"S\":\"DynamoDB, Hello world\"}\n        }\n      }\n    },\n    ...\n  ]\n}\n</code></pre> <pre><code>aws dynamodb batch-write-item \\\n    --request-items file://batch.json \\\n    --return-consumed-capacity INDEXES \\\n    --return-item-collection-metrics SIZE\n</code></pre>"},{"location":"database/dynamodb/read_write_data/#read-data","title":"Read data","text":"<p>DynamoDB offers two ways to access information stored: Query and Scan.</p> <p>Query</p> <p>If our table doesn\u2019t have a sort key, i.e the partition key that uniquely identifies every item in the table, we can only query one item at a time. We need to specify the partition key to query each item.</p> <p>If the table also has a sort key, we can query multiple items by using a range-over-sort key. To query multiple items, we need to specify the partition key and a range-over-sort key. All items you want to query should be from the same partition. All the items from the given partition, that belong to the given range, are returned in a sorted order.</p> <pre><code>aws dynamodb query \\\n  --table-name Blog \\\n  --select SPECIFIC_ATTRIBUTES \\\n  --projection-expression \"Author\" \\\n  --key-condition-expression 'Author= :author AND Topic_Title = :topic_title' \\\n  --expression-attribute-values \\\n    \"{\n      \\\":author\\\" : {\\\"S\\\" : \\\"Huy\\\"},\n      \\\":topic_title\\\" : {\\\"S\\\" : \\\"Hello world\\\"}\n    }\"\n</code></pre> <p>Amazon DynamoDB returns all the item attributes by default. To get only some, rather than all of the attributes, use a <code>projection expression</code>.</p> <p>Scan</p> <p>Scan reads every item in a table, which can be inefficient and costly for large datasets. You can fetch a maximum of 1 MB of data in one go and you can also apply filters to refine the items you get, all while scanning</p> <pre><code>aws dynamodb scan \\\n    --table-name Blog \\\n    --filter-expression 'Author = :author' \\\n    --expression-attribute-values \\\n    \"{\n        \\\":author\\\" : {\\\"S\\\" : \\\"Huy\\\"}\n    }\"\n</code></pre> <p> </p>"},{"location":"database/dynamodb/secondary_index/","title":"Secondary Index","text":"<p>AWS DynamoDB being a No SQL database doesn\u2019t support queries such as SELECT with a condition such as the following query.</p> <pre><code>SELECT * FROM Users WHERE email='username@email.com';\n</code></pre> <p>It is possible to obtain the same query result using DynamoDB scan operation. However, scan operations access every item in a table which is slower than query operations that access items at specific indices. Imagine, you have to look for a book in a library by going through possibly all the books in the library versus you know which shelf the book is at.</p> <p>Thus, there is a need for another table or data structure that stores data with different primary key and maps a subset of attributes from this base table. This other table is called a secondary index and is managed by AWS DynamoDB. When items are added, modified, or deleted in the base table, associated secondary indexes will be updated to reflect the changes.</p>"},{"location":"database/dynamodb/secondary_index/#global-secondary-indexgsi","title":"Global Secondary Index(GSI)","text":"<p>Global secondary index is an index that have a partition key and an optional sort key that are different from base table's primary key. It is deemed global because queries on the index can access the data across different partitions of the base table. </p> <p>It can viewed as a different table that contains attributes based on the base table.</p> <pre><code>aws dynamodb create-table \\\n    --table-name MusicCollection \\\n    --attribute-definitions AttributeName=Artist,AttributeType=S AttributeName=Song,AttributeType=S AttributeName=Genre,AttributeType=S \\\n    --key-schema AttributeName=Artist,KeyType=HASH \\\n                 AttributeName=Song,KeyType=RANGE \\\n    --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 \\\n    --global-secondary-indexes \\\n    \"[\n          {\n              \\\"IndexName\\\" : \\\"GenreIndex\\\",\n              \\\"KeySchema\\\" : [\n                  {\\\"AttributeName\\\" : \\\"Genre\\\", \\\"KeyType\\\" : \\\"HASH\\\"},\n                  {\\\"AttributeName\\\" : \\\"Artist\\\", \\\"KeyType\\\" : \\\"RANGE\\\"}\n              ],\n              \\\"Projection\\\": {\n                  \\\"ProjectionType\\\" : \\\"INCLUDE\\\",\n                  \\\"NonKeyAttributes\\\" :[\\\"Song\\\"]\n              },\n              \\\"ProvisionedThroughput\\\": {\n                  \\\"ReadCapacityUnits\\\" : 10,\n                  \\\"WriteCapacityUnits\\\" : 6\n              }\n          }\n    ]\"\n</code></pre>"},{"location":"database/dynamodb/secondary_index/#local-secondary-indexlsi","title":"Local Secondary Index(LSI)","text":"<p>Local secondary index is an index that must have the same partition key but a different sort key from the base table. It is considered local because every partition of a local secondary index is bounded by the same partition key value of the base table. </p> <p>It enables data query with different sorting order of the specified sort key attribute.</p> <pre><code>aws dynamodb create-table \\\n    --table-name Music \\\n    --attribute-definitions AttributeName=Artist,AttributeType=S AttributeName=SongTitle,AttributeType=S \\\n        AttributeName=AlbumTitle,AttributeType=S  \\\n    --key-schema AttributeName=Artist,KeyType=HASH AttributeName=SongTitle,KeyType=RANGE \\\n    --provisioned-throughput \\\n        ReadCapacityUnits=10,WriteCapacityUnits=5 \\\n    --local-secondary-indexes \\\n        \"[{\\\"IndexName\\\": \\\"AlbumTitleIndex\\\",\n        \\\"KeySchema\\\":[{\\\"AttributeName\\\":\\\"Artist\\\",\\\"KeyType\\\":\\\"HASH\\\"},\n                      {\\\"AttributeName\\\":\\\"AlbumTitle\\\",\\\"KeyType\\\":\\\"RANGE\\\"}],\n        \\\"Projection\\\":{\\\"ProjectionType\\\":\\\"INCLUDE\\\",  \\\"NonKeyAttributes\\\":[\\\"Genre\\\", \\\"Year\\\"]}}]\"\n</code></pre> <p>Data projection</p> <p>When setting up an index, DynamoDB will allow three data projection options. A data projection is a definition of which item attributes are projected \u2013 or copied \u2013 into the index:</p> <ol> <li>Key-only projection: only the <code>primary-key</code> and <code>sort-key</code> item attributes will be available in index</li> <li>All projection: an exact copy of all item attributes will be available in the index, which doubles the storage space the item takes in total</li> <li>Custom projection: the developer chooses which items will be available in the index apart from the <code>primary-key</code> and <code>sort-key</code></li> </ol> Global secondary index Local secondary index Definition An index with a partition key and a sort key that can be different from those on the base table. An index that has the same partition key as the base table, but a different sort key. Span of query Queries on the index can span all of the data in the base table, across all partitions. Every partition of a local secondary index is scoped to a base table partition that has the same partition key value. Primary Key Schema The partition key and, optionally, the sort key. Must be both partition key and sort key. Partition Key Attributes Any base table attribute of type string, number, or binary. Must have the same attribute as the partition key of the base table. Sort Key Attributes Any base table attribute of type string, number, or binary. Any base table attribute of type string, number, or binary. Key Values Do not need to be unique. The sort key value does not need to be unique for a given partition key value. Size Restrictions Per Partition Key Value No restriction. For each partition key value, the total size of all indexed items must be 10 GB or less. Index Operations Can be created during creation of a table. Can be created on an existing table. Can be deleted from an existing table. Can be created during creation of a table. Read Consistency for Queries Supports eventual consistency only from asynchronous updates and deletes. You can choose either eventual consistency or strong consistency. Provisioned Throughput Consumption Every global secondary index has its own provisioned throughput settings for read and write activity that you need to specify. Queries or scans on a global secondary index consume capacity units from the index, not from the base table. This is also case for global secondary index updates due to table writes. Queries or scans on a local secondary index consume read capacity units from the base table. When you write to a table, and its local secondary indexes are updated, these updates consume write capacity units from the base table. Projected Attributes With global secondary queries or scans, you can only request attributes that are projected into this index. If you query or scan a local secondary index, you can request attributes that are not projected into the index. DynamoDB will automatically fetch those attributes from the table. Index limit (default) 20 indexes. 5 indexes per table."},{"location":"database/postgres/advisory_locks/","title":"Advisory Locks","text":"<p>Postgres offers a special type of lock that is completely driven by the client application. They are not tied to any particular table or row \u2014 instead, you define a lock key (a number), and Postgres manages concurrency around that key.</p> <ul> <li>Locks can be exclusive (only one session) or shared (many sessions at once, but block exclusive).</li> <li>Keys can be provided either as a single bigint or as a pair of two ints.</li> <li>Locks are application-defined: Postgres doesn\u2019t know what your keys mean; it only ensures consistency across sessions.</li> </ul> <p>There are two types of advisory locks:</p> <ul> <li>Session-level advisory locks - Held until explicitly released or the session ends</li> <li>Transaction-level advisory locks - Released automatically at the end of the transaction</li> </ul>"},{"location":"database/postgres/advisory_locks/#session-level-advisory-lock","title":"Session-Level Advisory Lock","text":"<pre><code>-- Acquire exclusive lock (blocks until available)\nSELECT pg_advisory_lock(key);\nSELECT pg_advisory_lock(key1 int, key2 int);\n\n-- Try to acquire exclusive lock (returns immediately)\nSELECT pg_try_advisory_lock(key);\nSELECT pg_try_advisory_lock(key1 int, key2 int);\n\n-- Acquire shared lock\nSELECT pg_advisory_lock_shared(key);\nSELECT pg_advisory_lock_shared(key1 int, key2 int);\n\n-- Release specific lock\nSELECT pg_advisory_unlock(key);\nSELECT pg_advisory_unlock(key1 int, key2 int);\nSELECT pg_advisory_unlock_shared(key bigint);\nSELECT pg_advisory_unlock_shared(key1 int, key2 int);\n\n-- Release all session locks\nSELECT pg_advisory_unlock_all();\n</code></pre>"},{"location":"database/postgres/advisory_locks/#transaction-level-advisory-lock","title":"Transaction-Level Advisory Lock","text":"<p>Transaction-level locks are automatically released at the end of the transaction (commit or rollback).</p> <pre><code>-- Acquire exclusive lock (blocks until available)\nSELECT pg_advisory_xact_lock(key bigint);\nSELECT pg_advisory_xact_lock(key1 int, key2 int);\n\n-- Try to acquire exclusive lock (non-blocking, returns true/false)\nSELECT pg_try_advisory_xact_lock(key bigint);\nSELECT pg_try_advisory_xact_lock(key1 int, key2 int);\n\n-- Acquire shared lock\nSELECT pg_advisory_xact_lock_shared(key bigint);\nSELECT pg_advisory_xact_lock_shared(key1 int, key2 int);\n\n-- Try to acquire shared lock (non-blocking, returns true/false)\nSELECT pg_try_advisory_xact_lock_shared(key bigint);\nSELECT pg_try_advisory_xact_lock_shared(key1 int, key2 int);\n</code></pre>"},{"location":"database/postgres/advisory_locks/#exclusive-vs-shared","title":"Exclusive vs Shared","text":"<ul> <li>Exclusive lock (pg_advisory_lock): only one session can hold it at a time.</li> <li>Shared lock (pg_advisory_lock_shared): multiple sessions can hold simultaneously, but will block if someone tries to acquire the exclusive version.</li> </ul>"},{"location":"database/postgres/bitmap_index_scan/","title":"Bitmap index scan","text":"<p>While <code>Index Scan</code> is effective at high correlation, it falls short when the correlation drops to the point where the scanning pattern is more random than sequential and the number of page fetches increases. One solution here is to collect all the tuple IDs beforehand, sort them by page number, and then use them to scan the table. This is how Bitmap Scan, the second basic index scan method, works.</p> <p>Bitmap scans are a multi-step process that consist of a Bitmap Heap Scan, one or more Bitmap Index Scans and optionally BitmapOr and BitmapAnd operations.</p>"},{"location":"database/postgres/bitmap_index_scan/#how-bitmap-scan-work","title":"How bitmap scan work?","text":"<p>In order to understand what how bitmap index scan work we first need to understand a little bit more about how Postgres stores its data. The columns of a table in Postgres are separated into groups called heap pages. We can picture our table being composed of several pages as shown in the following diagram:</p> <p></p> <p>When we create an index on a column, Postgres can, in general, very efficiently figure out the pages where the rows resulting from a query are located. It can also tell their position within the pages.</p> <p>Using an <code>Index Scan</code> with a b-tree, for each row in the result Postgres will ask the b-tree for the heap page and row location on the page, load the heap page and then retrieve the row.</p> <p>Heap pages are stored in disk and thus loading a page in memory is costly. Thus, when using <code>Index Scan</code>, if the query results in a lot of rows, the query might become very slow because an Index Scan will make a page load for each row in the result.</p> <p>The following diagram shows a scenario where the query results in four rows (show in color) located in three different pages. Each color represents a consecutive group of rows that belong to the result of the query. Using an <code>Index Scan</code> Postgres will load pages 2 and 7 once and page 5 twice.</p> <p></p> <p>Note: Random access is also the main problem causing slow performance in <code>Index Scan</code> when it returns multiple rows.</p> <p>Consider the following plan:</p> <pre><code>EXPLAIN\nSELECT * FROM bookings WHERE total_amount = 48500.00;\n\n   QUERY PLAN\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\n Bitmap Heap Scan on bookings  (cost=54.63..7040.42 rows=2865 wid...\n   Recheck Cond: (total_amount = 48500.00)\n   \u2212&gt; Bitmap Index Scan on bookings_total_amount_idx\n       (cost=0.00..53.92 rows=2865 width=0)\n       Index Cond: (total_amount = 48500.00)\n(5 rows)\n</code></pre> <p>The query plan that we obtained above shows an alternative strategy where first, Postgres uses the index to figure out the heap pages that contain results. Then it will read each of those pages entirely in order to fetch the results.</p> <p>The advantage here is that each page is loaded only once into memory and pages can be loaded sequentially. The disadvantage is that each of those pages will be fully read, meaning that Postgres will inspect some rows that do not belong to the result.</p> <p>We can thus summarize the scans as follows:</p> <ul> <li>Bitmap Index Scan: Identify all heap pages that contain results that match the query</li> <li>Bitmap Heap Scan: Scan each heap page fully and recheck conditions</li> </ul> <p>The whole bitmap is a single bit array, with as many bits as there are heap pages in the relation being scanned.</p> <p>Bitmap starting off with all entries 0 (false). Whenever an index entry that matches the search condition is found, the heap address pointed to by that index entry is looked up as an offset into the bitmap, and that bit is set to 1 (true). So rather than looking up the heap page directly, the bitmap index scan looks up the corresponding bit position in the bitmap.</p> <p>Then the bitmap contains information for heap pages we need to bother to load and examine.</p> <p>Since each heap page might contain multiple rows, we then have to examine each row to see if it matches all the conditions - that's what the <code>Recheck Cond</code> part is about.</p>"},{"location":"database/postgres/bitmap_index_scan/#bitmap-operations","title":"Bitmap operations","text":"<p>A query may include multiple fields in its filter conditions. These fields may each have a separate index. Bitmap Scan allows us to take advantage of multiple indexes at once. Each index gets a row version bitmap built for it, and the bitmaps are then ANDed and ORed together. Example:</p> <pre><code>Bitmap Heap Scan on customers  (cost=25.76..61.62 rows=10 width=13) (actual time=0.077..0.077 rows=2 loops=1)\n  Recheck Cond: (((username)::text &lt; 'user100'::text) AND (customerid &lt; 1000))\n  -&gt;  BitmapAnd  (cost=25.76..25.76 rows=10 width=0) (actual time=0.073..0.073 rows=0 loops=1)\n        -&gt;  Bitmap Index Scan on ix_cust_username  (cost=0.00..5.75 rows=200 width=0) (actual time=0.006..0.006 rows=2 loops=1)\n              Index Cond: ((username)::text &lt; 'user100'::text)\n        -&gt;  Bitmap Index Scan on customers_pkey  (cost=0.00..19.75 rows=1000 width=0) (actual time=0.065..0.065 rows=999 loops=1)\n              Index Cond: (customerid &lt; 1000)\n</code></pre> <p>Graphical example:</p> <pre><code>Heap, one square = one page:\n+---------------------------------------------+\n|c____u_____X___u___X_________u___cXcc______u_|\n+---------------------------------------------+\nRows marked c match customers pkey condition.\nRows marked u match username condition.\nRows marked X match both conditions.\n\n\nBitmap scan from customers_pkey:\n+---------------------------------------------+\n|100000000001000000010000000000000111100000000| bitmap 1\n+---------------------------------------------+\nOne bit per heap page, in the same order as the heap\nBits 1 when condition matches, 0 if not\n\nBitmap scan from ix_cust_username:\n+---------------------------------------------+\n|000001000001000100010000000001000010000000010| bitmap 2\n+---------------------------------------------+\n</code></pre> <p>Once the bitmaps are created a bitwise AND is performed on them:</p> <pre><code>+---------------------------------------------+\n|100000000001000000010000000000000111100000000| bitmap 1\n|000001000001000100010000000001000010000000010| bitmap 2\n &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;\n|000000000001000000010000000000000010000000000| Combined bitmap\n+-----------+-------+--------------+----------+\n            |       |              |\n            v       v              v\nUsed to scan the heap only for matching pages:\n+---------------------------------------------+\n|___________X_______X______________X__________|\n+---------------------------------------------+\n</code></pre> <p>The bitmap heap scan then seeks to the start of each page and reads the page:</p> <pre><code>+---------------------------------------------+\n|___________X_______X______________X__________|\n+---------------------------------------------+\nseek-------&gt;^seek--&gt;^seek---------&gt;^\n            |       |              |\n            ------------------------\n            only these pages read\n</code></pre> <p>Each read page is then re-checked against the condition since there can be &gt;1 row per page and not all necessarily match the condition.</p>"},{"location":"database/postgres/brin_index/","title":"BRIN Index","text":"<p>A BRIN is a Block Range Index. A block (page) is Postgres\u2019 base unit of storage and is by default 8kB of data. BRIN samples a range of blocks (default 128), storing the location of the first block in the range as well as the minimum and maximum values for all values in those blocks.</p> <p>It\u2019s very useful for ordered data sets, offering significant space savings for similar and sometimes better performance.</p> <p>BRIN is incredibly helpful in efficiently searching over large time-series data and has the benefit of taking up significantly less space on disk than a standard B-TREE index.</p>"},{"location":"database/postgres/brin_index/#where-it-can-be-used","title":"Where It can be used?","text":"<p>Many applications today record data from sensors, devices, tracking information, real-time bank transactions, and other things that share a common attribute: a timestamp that is always increasing. This timestamp is very valuable, as it serves as the basis for types of lookups, analytical queries, and more.</p> <p>when used appropriately, a BRIN index will not only outperform a B-tree but will also save over 99% of space on disk.</p> <p>BRIN index can be considered to be use if your table have:</p> <ul> <li>The physical row ordering </li> <li>The actual structure on disk and</li> <li>The logical ordering of column values</li> <li>Table update infrequently</li> </ul>"},{"location":"database/postgres/brin_index/#how-does-it-works","title":"How does it works?","text":"<p>The BRIN index is a small table that associates a range of values with a range of pages in the table order.</p> <p>Building the index just requires a single scan of the table, so compared to building a structure like a BTree, it is very fast (on write).</p> <p></p> <p>Because the BRIN has one entry for each range of pages, it's also very small. The number of pages in a range is configurable, but the default is <code>128</code> (16 pages, each page is 8KB).</p>"},{"location":"database/postgres/brin_index/#compare-to-btree","title":"Compare to Btree","text":"<p>Size</p> <p>B+Tree indexes tend to be larger in size compared to BRIN indexes because they store pointers to data rows and maintain a balanced tree structure.</p> <p>Query Performance:</p> <p>B+Tree indexes excel at point queries, where you need to locate a specific row quickly. They are also suitable for range queries but may become less efficient as the data size grows substantially.</p> <p>BRIN indexes are optimized for range queries. They are highly efficient when you need to scan large ranges of data. However, they are not well-suited for point queries, as they do not provide direct access to individual rows.</p> <p>Note: In some scenarios, BRIN may not offer a good performance compare to B+Tree index.</p> <p>Data Update Performance:</p> <p>B+Tree indexes can be relatively expensive to update</p> <p>BRIN indexes are designed with data blocks in mind, making them suitable for scenarios where the data is append-only or where updates are infrequent. However update data in BRIN index can make a good performance compare to BTree. (It still affect query performance if the table is updated)</p>"},{"location":"database/postgres/brin_index/#references","title":"References","text":"<p>BRIN Index for PostgreSQL: Don\u2019t Forget the Benefits</p>"},{"location":"database/postgres/range_types/","title":"Range Types","text":"<p>When working on applications such as a reservation app or calendar app, you need to store the <code>start time</code> and <code>end time</code> of an event. You may also need to query events occurring in a specific time frame or ensure that certain events do not overlap.</p>"},{"location":"database/postgres/range_types/#the-problem-with-traditional-date-columns","title":"The Problem with Traditional Date Columns","text":"<p>Traditionally, when dealing with events or periods, developers often use two separate columns to represent the start and end of a range. For example:</p> <pre><code>create table reservations (\n  id serial primary key,\n  title text,\n  start_at timestamptz,\n  end_at timestamptz\n);\n</code></pre> <p>While this approach works, it has a few drawbacks:</p> <ul> <li>Querying Complexity: Writing queries to find overlapping events or events within a specific period becomes complex and error-prone.</li> <li>Data Integrity: Ensuring that reservations do not overlap is difficult.</li> </ul>"},{"location":"database/postgres/range_types/#enter-range-types","title":"Enter range types","text":"<p>PostgreSQL has a better solution for these problems \u2014 range types. It comes with these additional built-in data types:</p> <ul> <li>int4range: Range of integer</li> <li>int8range: Range of bigint</li> <li>numrange: Range of numeric</li> <li>tsrange: Range of timestamp without time zone</li> <li>tstzrange: Range of timestamp with time zone</li> <li>daterange: Range of date</li> </ul> <p>You can use them as a column type in a table:</p> <pre><code>create table reservations (\n  id serial primary key,\n  title text,\n  table_id int4,\n  duration tstzrange\n);\n</code></pre>"},{"location":"database/postgres/range_types/#querying-range-columns","title":"Querying range columns","text":"<pre><code>select * from reservations where duration &amp;&amp; '[2024-07-04 16:00, 2024-07-04 19:00)';\n</code></pre> <p>Postgres provides more range-specific operators. The official Postgres documentation provides a complete list of range operators.</p>"},{"location":"database/postgres/range_types/#querying-range-columns_1","title":"Querying range columns","text":"<p>When working on a reservations app, you might want to ensure there are no overlapping reservations. Range columns make it easy to add such constraints. The following SQL statement adds an exclude constraint that prevents new inserts/ updates from overlapping on any of the existing reservations.</p> <pre><code>-- Enable the btree_gist index required for the constraint.\ncreate extension btree_gist\n\n-- Add a constraint to prevent overlaps with the same table_id\nalter table reservations\n  add constraint exclude_duration\n  exclude using gist (table_id WITH =, duration WITH &amp;&amp;);\n</code></pre> <pre><code>-- Add a first reservation\ninsert into reservations (title, table_id, duration)\nvalues ('Tyler Dinner', 1, '[2024-07-04 18:00, 2024-07-04 21:00)');\n\n-- Insert fails, because table 1 is taken from 18:00 - 21:00\ninsert into reservations (title, table_id, duration)\nvalues ('Thor Dinner', 1, '[2024-07-04 20:00, 2024-07-04 22:00)');\n\n-- Insert succeeds because table 2 is not taken by anyone\ninsert into reservations (title, table_id, duration)\nvalues ('Thor Dinner', 2, '[2024-07-04 20:00, 2024-07-04 22:00)');\n</code></pre>"},{"location":"database/postgres/range_types/#references","title":"References","text":"<ul> <li>Simplifying Time-Based Queries with Range Columns</li> </ul>"},{"location":"database/postgres/streaming_replication/","title":"Streaming Replication","text":"<p>Streaming replication, a standard feature of PostgreSQL, allows the updated information on the primary server to be transferred to the standby server in real time, so that the databases of the primary server and standby server can be kept in sync.</p>"},{"location":"database/postgres/streaming_replication/#streaming-replication-mechanism","title":"Streaming replication mechanism","text":""},{"location":"database/postgres/streaming_replication/#what-is-shipped","title":"What is shipped","text":"<p>PostgreSQL saves the updated information of the primary server as a transaction log known as write-ahead log, or WAL, in preparation for crash recovery or rollback. Streaming replication works by transferring, or shipping, the WAL to the standby server in real time, and applying it on the standby server.</p> <p></p>"},{"location":"database/postgres/streaming_replication/#how-the-wal-is-shipped-and-applied","title":"How the WAL is shipped and applied","text":"<p>WAL shipping between the primary server and the standby server is performed by the WAL sender process on the primary server to the WAL receiver process on the standby server. These processes are started by setting <code>postgresql.conf</code> and <code>pg_hba.conf</code> parameters</p> <p></p> <p>A walsender and a walreceiver communicate using a single TCP connection.</p> <p>The <code>pg_stat_replication</code> view shows the state of all running walsenders. An example is shown below:</p> <pre><code>testdb=# SELECT application_name,state FROM pg_stat_replication;\n application_name |   state\n------------------+-----------\n standby1         | streaming\n standby2         | streaming\n pg_basebackup    | backup\n(3 rows)\n</code></pre>"},{"location":"database/postgres/streaming_replication/#setup","title":"Setup","text":""},{"location":"database/postgres/streaming_replication/#multi-standby-setup-and-cascade-setup","title":"Multi-standby setup and cascade setup","text":"<p>Streaming replication can be built in a 1:N configuration, where only one primary server is configurable, but multiple standby servers can be set up.</p> <p>The configuration that connects (ships WAL from) a primary server to all standby servers is called a multi-standby setup.</p> <p></p> <p>You can also build a cascade setup where a standby server connects (ships WAL) to another standby server.</p> <p></p>"},{"location":"database/postgres/streaming_replication/#synchronous-replication-and-asynchronous-replication","title":"Synchronous replication and asynchronous replication","text":"<p>For streaming replication, you can select either synchronous or asynchronous replication for each standby server.</p> <p>Synchronous replication</p> <ul> <li>The primary server waits for a response from the standby server before completing a process.   Therefore, the overall response time includes the log shipping time.</li> <li>Since there is no delay in WAL shipping to the standby server, the data freshness (reliability) of the standby server is improved.</li> <li>Suitable for failover and read-only load balancing operations.</li> </ul> <p>Asynchronous replication</p> <ul> <li>The primary server completes a process without waiting for a response from the standby server. Therefore, the overall response time is about the same as when streaming replication is not used.</li> <li>Since WAL shipping and its application (data update) on the standby server are done asynchronously, the updated result on the primary server may not be immediately available on the standby server. Depending on the timing of failover, data may be lost.</li> <li>Suitable for replication to remote areas for disaster recovery.</li> </ul>"},{"location":"database/postgres/streaming_replication/#synchronousasynchronous-setup","title":"Synchronous/asynchronous setup","text":"<p>Synchronous setup is configured by synchronous_standby_names in <code>postgresql.conf</code> on the primary server. If there are multiple standby servers, you can specify the servers to be synchronized and the order of priority for COMMIT. Standby servers not specified in this parameter will be asynchronous.</p> <p>You can also specify the method by which to select synchronous standby servers from a list of servers, such as <code>FIRST n (list)</code> or <code>ANY n (list)</code></p> <p>The <code>FIRST</code> keyword specifies that the <code>n</code> first standby servers in list will be synchronous and that transaction commits will wait until their WAL records are replicated to the first n standby servers in list.</p> <p>The <code>ANY</code> keyword specifies a quorum-based synchronous replication and that transaction commits should wait until their WAL records are replicated to at least <code>n</code> standby servers in list.</p> <p>For example, in an environment using standby servers <code>s1</code>, <code>s2</code>, <code>s3</code>, and <code>s4</code>, setting <code>synchronous_standby_names</code> to <code>FIRST 2 (s1, s2, s3')</code> will configure <code>s1</code> and <code>s2</code> for synchronous replication, and <code>s3</code> and <code>s4</code> for asynchronous replication. The primary server will wait <code>s1</code> and <code>s2</code> to complete processing before it commits. If <code>s1</code> or <code>s2</code> fails, then <code>s3</code> changes to synchronous replication.</p> <p>Make sure that the name set in <code>synchronous_standby_names</code> matches the name set in <code>application_name</code> of <code>primary_conninfo</code> in <code>postgresql.conf</code> on the standby server.</p>"},{"location":"database/postgres/streaming_replication/#setting-the-synchronization-level","title":"Setting the synchronization level","text":"<p>To set the synchronization level of the standby server, set <code>synchronous_commit</code> in <code>postgresql.conf</code> on the primary server.</p> Sync Level Set Value Overview Guaranteed range Full synchronous remote_apply Commits wait until after the WAL is replicated to the standby servers and the updated data on the standby server is available to read-only queries. It is fully assured that the data is in sync, so it is well suited for load balancing of read-only workloads that require data freshness. 1 to 9 Synchronous on (default) Commits wait until after the WAL is replicated to the standby servers. This setting achieves the best balance between performance and reliability. 1 to 6 Semi-synchronous remote_write Commits wait until after WAL has been shipped to the standby servers. 1 to 5 Asynchronous local Commits wait until after the WAL write on the primary server is completed. 1 to 2 Asynchronous off Commits do not wait for the WAL write on the primary server to complete. This setting is not recommended. 1 to 1 <p> </p>"},{"location":"database/postgres/write_ahead_logging/","title":"Write Ahead Logging \u2014 WAL","text":"<p>Write-Ahead Log (WAL) is a very important term in transaction processing. In PostgreSQL, it is also known as a transaction log. A log is a record of all the events or changes and WAL data is just a description of changes made to the actual data</p> <p>The term <code>Write-Ahead Log</code> implies that any change that you make to the database must first be appended to the log file, and then the log file should be flushed to disk</p> <p>The basic purpose of Write-Ahead Logging is to ensure that when there is a crash in the operating system or PostgreSQL or the hardware, the database can be recovered.</p>"},{"location":"database/postgres/write_ahead_logging/#insertion-operations-without-wal","title":"Insertion Operations without WAL","text":"<ol> <li>Issuing the first INSERT statement, PostgreSQL loads the <code>TABLE_A</code>'s page from a database cluster into the in-memory shared buffer pool, and inserts a tuple into the page. This page is not written into the database cluster immediately. (a modified pages are generally called a dirty page.)</li> <li>Issuing the second INSERT statement, PostgreSQL inserts a new tuple into the page on the buffer pool. This page has not been written into the storage yet.</li> <li>If the operating system or PostgreSQL server should fail for any reasons such as a power failure, all of the inserted data would be lost.</li> </ol> <p>Note: Before WAL was introduced (version 7.0 or earlier), PostgreSQL did synchronous writes to the disk by issuing a sync system call whenever changing a page in memory in order to ensure durability. Therefore, the modification commands such as INSERT and UPDATE were very poor-performance.</p>"},{"location":"devops/github-auto-update-submodules/","title":"Automatically updating git submodules using GitHub Actions","text":""},{"location":"devops/github-auto-update-submodules/#implementation-strategies","title":"Implementation Strategies","text":""},{"location":"devops/github-auto-update-submodules/#1-repository-dispatch-method","title":"1. Repository Dispatch Method","text":"<p>For repositories you control, use <code>repository_dispatch</code> to trigger updates:</p> <p>Submodule Repository Workflow (triggers on push):</p> <pre><code>name: Dispatch Update\n\non:\n  push:\n    branches:\n      - master\n\njobs:\n  notify-infrastructure:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Notify parent-repo\n        uses: peter-evans/repository-dispatch@v3\n        with:\n          token: ${{ secrets.PAT }}\n          repository: owner/parent-repo\n          event-type: submodule-update\n          client-payload: |\n            {\n              \"repository\": \"${{ github.repository }}\"\n            }\n</code></pre> <p>Parent Repository Workflow (receives dispatch):</p> <pre><code>name: Update Submodules\non:\n  repository_dispatch:\n    types: [submodule-update]\n\njobs:\n  update-submodule:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout infrastructure\n        uses: actions/checkout@v4\n        with:\n          token: ${{ secrets.PAT }}\n          submodules: true\n\n      - name: Update submodule\n        run: |\n          REPO_NAME=$(echo \"${{ github.event.client_payload.repository }}\" | cut -d'/' -f2)\n\n          if [ \"$REPO_NAME\" = \"xxx\" ]; then\n            SUBMODULE_PATH=\"xxx\"\n          elif [ \"$REPO_NAME\" = \"yyy\" ]; then\n            SUBMODULE_PATH=\"yyy\"\n          else\n            echo \"Unknown repository: $REPO_NAME\"\n            exit 1\n          fi\n\n          echo \"Updating submodule: $SUBMODULE_PATH\"\n          cd $SUBMODULE_PATH\n          git fetch origin\n          git checkout origin/master\n          cd ../..\n\n      - name: Commit submodule update\n        run: |\n          REPO_NAME=$(echo \"${{ github.event.client_payload.repository }}\" | cut -d'/' -f2)\n\n          if [ \"$REPO_NAME\" = \"xxx\" ]; then\n            SUBMODULE_PATH=\"xxx\"\n          else\n            SUBMODULE_PATH=\"yyy\"\n          fi\n\n          git config --local user.email \"huy.duongdinh@gmail.com\"\n          git config --local user.name \"Huy Duong\"\n          git add $SUBMODULE_PATH\n          git commit -m \"chore: update $REPO_NAME submodule to latest\" || exit 0\n          git push\n</code></pre>"},{"location":"devops/aws/cloud_front/","title":"AWS CloudFront","text":"<p>AWS CloudFront is a globally-distributed network offered by Amazon Web Services, which securely transfers content such as software, SDKs, videos, etc., to the clients, with high transfer speed.</p> <p></p> <p>Benefits of AWS CloudFront</p> <ul> <li>It will cache your content in edge locations and decrease the workload, thus resulting in high availability of applications.</li> <li>Improve security with traffic encryption and access controls, and use AWS Shield Standard to defend against DDoS attacks at no additional charge.</li> </ul> <p>Amazon CloudFront Basics:</p> <p>There are three core concepts that you need to understand to start using CloudFront: distributions, origins, and cache control.</p> <p>Distributions</p> <p>To use Amazon CloudFront, you start by creating a distribution, which is identified by a DNS domain name. To serve files from Amazon CloudFront, you simply use the distribution domain name in place of your website\u2019s domain name; the rest of the file paths stay unchanged.</p> <p>Origins</p> <p>When you create a distribution, you must specify the DNS domain name of the origin \u2014 the Amazon S3 bucket or HTTP server \u2014 from which you want Amazon CloudFront to get the definitive version of your objects (web files).</p> <p>Cache-Control</p> <p>Once requested and served from an edge location, objects stay in the cache until they expire or are evicted to make room for more frequently requested content.</p>"},{"location":"devops/aws/cloud_front/#origins","title":"Origins","text":"<p>Origins serve as the source locations for the content distributed through CloudFront. These can be:</p> <ul> <li>S3 bucket (can be private, need to setup Origin Access Identity + S3 bucket policy)</li> <li>S3 website</li> <li>Application Load Balancer (ALB must be public, EC2 can be private)</li> <li>EC2 instance (must be public)</li> <li>Any HTTP backend</li> </ul>"},{"location":"devops/aws/cloud_front/#cloudfront-geo-restriction","title":"CloudFront Geo Restriction","text":"<p>You can restrict who can access your distribution:</p> <ul> <li>Whitelist: Allow your users to access your content only if they's are in one of the countries on a list of approved countries</li> <li>Blacklist: Prevent your users to access your content only if they's are in one of the countries on a blacklist of banned countries</li> </ul>"},{"location":"devops/aws/cloud_front/#cloudfront-vs-s3-region-replication","title":"CloudFront vs S3 Region Replication","text":"<p>CloudFront</p> <ul> <li>Global edge network</li> <li>Files are cached for a TTL</li> <li>Great for static content that must be available everywhere</li> </ul> <p>S3 Region Replication</p> <ul> <li>Must be setup each region you want replication to happen</li> <li>Files are update in nearly realtime</li> <li>Read only</li> <li>Great for dynamic content that needs to be available at low-latency in few regions</li> </ul>"},{"location":"devops/aws/cloud_front/#ttl-and-invalidations","title":"TTL and Invalidations","text":"<p>CloudFront\u2019s architecture is centered around caching content at edge locations to enhance content delivery performance and reduce latency for end users. However, this caching mechanism introduces the risk of serving stale data if updates occur at the origin.</p>"},{"location":"devops/aws/cloud_front/#time-to-live-ttl","title":"Time-to-Live (TTL)","text":"<p>Time-to-Live (TTL) refers to the duration for which an object is considered valid and cached at edge locations. When an object\u2019s TTL expires, CloudFront initiates an origin fetch to retrieve the latest version of the object from the origin server. If the object has been updated, CloudFront fetches the updated content from the origin server and refreshes its cache. If the object hasn\u2019t been updated, CloudFront continues serving the cached content. This ensures that users receive up-to-date content, promoting a seamless browsing experience.</p> <p>CloudFront sets a TTL of 24 hours for cached objects by default. However, users can customize TTL values to align with their specific caching requirements.</p> <p>In addition to this, CloudFront honors TTL directives specified by Cache-Control and Expires headers, allowing origin servers to exert control over caching behavior.</p> <ul> <li>Cache-Control header: The Cache-Control header includes directives instructing CloudFront on caching and serving content. Directives such as <code>max-age</code> specify the maximum amount of time (in seconds) that an object should be considered fresh and cached. CloudFront interprets this directive to set the TTL for cached objects accordingly.</li> <li>Expires header: The Expires header specifies an exact date and time when an object\u2019s cached version should be considered stale and expire. CloudFront uses this information to determine the TTL for the cached object, ensuring it remains cached until the specified expiration date and time.</li> </ul>"},{"location":"devops/aws/cloud_front/#how-cloudfront-works","title":"How CloudFront works","text":"<p>Consider a scenario where Bob and Alice, two users in different regions, aim to access a popular video hosted in an Amazon S3 bucket. Bob resides near an edge location in Italy, while Alice is closer to an edge location in Spain, but they have the same regional cache.</p> <p></p> <p>Video available at edge location: When Bob initiates a request to view the video, CloudFront checks if the content is cached in the edge location nearest to him. If the video is cached locally, Bob experiences a <code>cache hit</code>, resulting in immediate access to the content with minimal latency.</p> <p></p> <p>Video available at regional edge location: On the other hand, if the video is not cached in Bob\u2019s nearby edge location, CloudFront triggers a <code>cache miss</code>. In this case, CloudFront retrieves the video from the regional edge cache, a larger cache shared by multiple edge locations within the same geographic area. If the video is found in the regional edge cache, it is promptly delivered to Bob\u2019s edge location, ensuring efficient content delivery despite the cache miss.</p> <p></p> <p>Video not cached at edge or regional edge location: However, if the video is not available in the regional edge cache, CloudFront performs an origin fetch, retrieving the video directly from the S3 bucket, the content\u2019s origin. Once fetched, the video is stored in the regional edge cache for future requests, optimizing content delivery for subsequent users in the same geographic region.</p> <p></p> <p>Future fetch requests: When Alice initiates a request to fetch the same video, CloudFront can retrieve it from the regional edge cache.</p> <p></p>"},{"location":"devops/aws/cloud_front/#functions-at-edge","title":"Functions at Edge","text":"<p>Functions at Edge (FaE) are lightweight serverless functions that execute at the edge locations of a content delivery network (CDN) like AWS CloudFront. These functions enable developers to execute code in proximity to end-users, and we can use these functions for several purposes:</p> <ul> <li>Implementing advanced HTTP logic: CloudFront offers native features like redirecting from HTTP to HTTPS and routing to different origins based on request paths. Edge functions extend CloudFront\u2019s capabilities, enabling the implementation of advanced HTTP logic such as cache key normalization, URL rewriting, and HTTP CRUD operations.</li> <li>Reducing application latency: Offloading certain application logic from the origin to the edge leverages caching (e.g., A/B testing) and executes closer to users (e.g., HTTP redirections, URL shortening, HTML rendering). In microservices or micro-frontend architectures, edge functions centralize common logic (e.g., Authorization &amp; Authentication) at the application entry point, streamlining development and reducing redundancy</li> <li>Protecting the application perimeter: Edge functions enforce security controls like access control and advanced geoblocking at the edge, minimizing the attack surface of the origin and optimizing scalability.</li> <li>Request routing: Edge functions enable routing each HTTP request to specific origins based on application logic, facilitating scenarios such as advanced failover, origin load balancing, multi-region architectures, migrations, and application routing.</li> </ul> <p>CloudFront offers two types of edge functions: <code>CloudFront Functions</code> and <code>Lambda@Edge</code>. <code>CloudFront Functions</code> offer sub-millisecond startup times and immediate scalability to handle millions of requests per second, making them ideal for lightweight tasks such as cache normalization, URL rewriting, request manipulation, and authorization.</p> <p>On the other hand, <code>Lambda@Edge</code> extends AWS Lambda functionality, distributing execution across Regional Edge Caches. While <code>Lambda@Edge</code> provides more computing power and advanced features like external network calls, it has higher costs and latency overhead.</p>"},{"location":"devops/aws/cloud_front/#lambdaedge","title":"Lambda@Edge","text":"<p>Lambda@Edge is a powerful feature of CloudFront that enables the execution of lightweight Lambda functions at CloudFront edge locations.</p> <p>Limitations of Lambda@Edge</p> <ul> <li>Supported runtimes: Lambda@Edge supports only Node.js and Python runtimes, limiting the programming languages available for function development.</li> <li>No VPC access: Lambda@Edge functions execute within the AWS Public Zone, so they lack access to Virtual Private Cloud (VPC)-based resources, which might restrict certain use cases requiring VPC connectivity.</li> <li>No Lambda layers: Lambda@Edge does not support Lambda layers, limiting the ability to reuse common code across multiple functions.</li> <li>Execution limits: Lambda@Edge functions have distinct size and execution time limits compared to standard Lambda functions, imposing constraints on memory allocation and function timeout.</li> </ul> <p>Use cases for Lambda@Edge</p> <ul> <li>Dynamic content manipulation: Lambda@Edge allows us to execute serverless functions at AWS edge locations, enabling dynamic content manipulation closer to the end user.</li> <li>Content personalization: We can use Lambda@Edge to customize content based on user requests or device characteristics, such as geolocation or user-agent.</li> <li>Access control and authentication: Implement access control policies, authentication mechanisms, or authorization logic at the edge to restrict access to resources or content.</li> <li>A/B testing and experimentation: Conduct A/B testing or experimentation by routing requests to different origins or serving different versions of content based on predefined criteria.</li> </ul>"},{"location":"devops/aws/cloud_front/#cloudfront-functions","title":"CloudFront functions","text":"<p>CloudFront Functions are lightweight serverless functions that run at the edge of the AWS CloudFront global network. They enable developers to customize and manipulate content delivery at the edge locations, allowing for real-time processing of requests as they are received and responses as they are generated. This helps optimize content delivery, implement security measures, and personalize content without provisioning or managing additional infrastructure.</p> <p>Limitations of CloudFront Functions</p> <ul> <li>Limited language support: Currently, CloudFront Functions support only JavaScript (Node.js) as the programming language, limiting language choice for developers.</li> <li>Execution time limit: Functions have a maximum execution time limit of 5 seconds, which may restrict the complexity of operations that can be performed within a single function.</li> <li>Limited libraries and dependencies: CloudFront Functions have limited access to external libraries and dependencies compared to traditional serverless platforms like AWS Lambda, which may impact the ability to reuse existing code or libraries.</li> <li>Limited debugging tools: Debugging CloudFront Functions can be challenging, as limited debugging tools and logging capabilities are available compared to other serverless platforms.</li> <li>Statelessness: Functions are stateless by design, meaning they do not maintain state between invocations, which may require developers to implement additional mechanisms for state management if needed.</li> </ul> <p>Use cases for CloudFront functions</p> <ul> <li>Lightweight content transformation: CloudFront Functions are lightweight JavaScript functions that execute at the edge locations, primarily for simple content transformations or modifications.</li> <li>Header manipulation: Modify request or response headers, such as adding custom headers, removing sensitive headers, or altering caching directives.</li> <li>URL rewriting and redirection: Implement URL rewriting rules, URL redirections, or URL-based routing logic to customize the behavior of content delivery.</li> <li>Content security policies: Enforce security policies, such as cross-origin resource sharing (CORS), content security policies (CSP), or cookie attributes, to enhance security and mitigate common web vulnerabilities.</li> </ul>"},{"location":"devops/aws/cloud_watch_vs_cloud_trail/","title":"CloudWatch vs CloudTrail","text":"<ul> <li>CloudWatch focuses on monitoring AWS services and resources' health and performance.</li> <li>CloudTrail logs all actions performed within your AWS environment, tracking who did what.</li> </ul> <p>CloudWatch: <code>What is happening on AWS?</code> and logging all the events for a particular service or application. CloudTrail: <code>Who did what on AWS?</code> and the API calls to the service or resource.</p>"},{"location":"devops/aws/cloud_watch_vs_cloud_trail/#aws-cloudwatch","title":"AWS CloudWatch:","text":"<p>A monitoring service for AWS resources and applications.</p> <ul> <li>Collects and tracks metrics (e.g., CPU usage, memory, latency).</li> <li>Supports log collection from applications and AWS services.</li> <li>Allows setting alarms and triggering automated actions.</li> <li>Offers basic monitoring (5 min) and detailed monitoring (1 min).</li> </ul>"},{"location":"devops/aws/cloud_watch_vs_cloud_trail/#aws-cloudtrail","title":"AWS CloudTrail","text":"<p>A logging service for governance, compliance, and auditing.</p> <ul> <li>Records API activity across AWS accounts and services.</li> <li>Logs actions from AWS Console, SDKs, CLI, and services.</li> <li>Stores logs in S3 buckets or CloudWatch Logs for long-term retention.</li> <li>Helps track who made changes to AWS infrastructure.</li> <li>Free management event logging; data event logging incurs charges.</li> </ul>"},{"location":"devops/aws/cloud_watch_vs_cloud_trail/#summary","title":"Summary","text":"<ul> <li>Use CloudWatch for monitoring application and infrastructure performance.</li> <li>Use CloudTrail for tracking API activity, security audits, and compliance.</li> <li>Both services are enabled by default, but detailed monitoring and additional logging may incur extra costs.</li> </ul>"},{"location":"devops/aws/cloud_watch_vs_cloud_trail/#reference","title":"Reference","text":"<p>AWS \u2014 Difference between CloudWatch and CloudTrail</p>"},{"location":"devops/aws/ec2/","title":"EC2","text":"<p>Amazon Elastic Compute Cloud (EC2) is a resizable cloud computing capacity. It allows users to run virtual servers, known as instances, for various computing tasks. EC2 offers different benefits besides being a flexible computing capacity. It allows us to deploy instances in multiple Availability Zones (AZs) within a region in combination with different services such as elastic load balancer and auto scaling group to offer high availability within a region.</p> <p></p>"},{"location":"devops/aws/ec2/#instance","title":"Instance","text":"<p>An EC2 instance is a virtual server in the cloud. It can run different operating systems, including Linux, Windows, and CentOS. Instances are categories based on their computing power, memory, and networking capabilities. We can select any instance type based on our requirements.</p> <p>Each instance contains a root volume to boot the instance. After launching, an instance works similarly to a server and keeps running until it is stopped, hibernated, terminated, or failed.</p>"},{"location":"devops/aws/ec2/#amazon-machine-image-ami","title":"Amazon Machine Image (AMI)","text":"<p>An Amazon Machine Image (AMI) is a pre-configured virtual machine template that contains software configurations like operating systems and other packages used to launch an instance. AMIs serve as a blueprint for creating EC2 instances, allowing for easy replication and scaling of virtual servers. Multiple instances can be launched from a single AMI. AWS offers different AMIs to cater to user requirements, including the popular Amazon Linux, Ubuntu, and Windows.</p>"},{"location":"devops/aws/ec2/#instance-types","title":"Instance types","text":"<p>Instance type specifies the type of hardware for the virtual server in the cloud. AWS offers different types of instances based on their hardware capabilities:</p> <p>General Purpose</p> <p>Great for a diversity of workloads such as web servers or code repository. It balance between compute, memory and network.</p> <p>Compute Optimized</p> <p>Great for compute intensive tasks that require high performance processors:</p> <ul> <li>Batch processing workloads</li> <li>Media transcoding</li> <li>High performance web servers</li> <li>High performance computing</li> <li>Scientific modeling &amp; machine learning</li> <li>Dedicated gaming servers</li> </ul> <p>Memory Optimized</p> <p>Fast performance for workloads that process large data sets in memory</p> <ul> <li>High performance, relational/non-relational databases</li> <li>Distributed web scale cache stores</li> <li>In-memory databases optimized for BI</li> <li>Applications performing real-time processing of big unstructured data</li> </ul> <p>Storage Optimized</p> <p>Great for storage-intensive tasks that require high, sequential read and write access to large data sets on local storage.</p> <ul> <li>High frequency online transaction processing system</li> <li>Relational &amp; NOSQL databases</li> <li>Cache for in-memory databases</li> <li>Data warehousing applications</li> <li>Distributed file systems</li> </ul> <p></p>"},{"location":"devops/aws/ec2/#ec2-user-data","title":"EC2 User data","text":"<p>It's possible to bootstrap our instances using EC2 User Data script. It will run ONLY ONCE at the instance first start. EC2 User Data is used to automate boot tasks such as:</p> <ul> <li>Installing updates</li> <li>Installing software</li> <li>Download common files from the internet</li> <li>Anything you can think of</li> </ul> <p>The EC2 User data scripts run with the root user</p>"},{"location":"devops/aws/ec2/#security-group","title":"Security Group","text":"<p>Security groups are like firewalls to the associated resources; they control the inbound and outbound traffic for an associated resource.</p> <p>Security groups are used to secure EC2 instances from unwanted requests. We need to specify a security group to secure our EC2 instance whenever we launch an instance. If no security group is selected, EC2 uses the default security group of the VPC. The default security group allows all outbound traffic and only allows inbound requests from resources within the same security group.</p>"},{"location":"devops/aws/ec2/#inbound-rules","title":"Inbound rules","text":"<ul> <li>Inbound rules are used to define incoming traffic to the associated resources.</li> <li>By default, all inbound traffic is denied.</li> </ul>"},{"location":"devops/aws/ec2/#outbound-rules","title":"Outbound rules","text":"<ul> <li>Outbound rules define the outgoing traffic from the associated resource to the internet.</li> <li>All outgoing traffic from the associated resource is allowed by default.</li> </ul> <p>We can also allow inbound traffic to a resource from certain security groups; this helps us secure the resource in a more efficient manner.</p>"},{"location":"devops/aws/ec2/#elastic-network-interface","title":"Elastic Network Interface","text":"<p>An Elastic Network Interface (ENI) is a virtual network card that can be attached to the EC2 instances. It functions like a network interface card (NIC) and provides networking capabilities to the EC2 instances, allowing them to communicate with other resources in the same Virtual Private Cloud (VPC) or over the internet.</p> <p>Every instance launched has a default network interface, known as the primary network interface; by default, it offers a private IP address to the instance. However, it can be configured to offer the public as well as the elastic IP address. A primary network interface can not be detached from an instance. However, we can attach more network interfaces to an instance. The number of network interfaces that can be attached to an instance depends upon the instance type and size. For example, <code>m1.xlarge</code> can have up to 4 network interfaces; similarly, <code>t2.micro</code> can have 2 network interfaces maximum.</p>"},{"location":"devops/aws/ec2/#storage","title":"Storage","text":"<p>AWS offers flexible and easy-to-use data storage options for EC2 instances to meet all the requirements. Each option has its performance perks and cost. Some storage options offer persistent storage, while others provide fast temporary storage for the instance.</p> <p></p>"},{"location":"devops/aws/ec2/#elastic-block-store","title":"Elastic Block Store","text":"<p>Elastic Block Store (EBS) is a highly reliable, durable block-level storage volume that can be attached to the EC2 instances. Multiple EBS blocks can be attached to an instance. It's a network drive (not a physical drive) then it uses the network to communicate the instance, which means there might be a bit of latency. EBS is locked to an Availability Zone thus EBS volume in <code>us-east-1a</code> can not be attached to <code>us-east-1b</code>. EBS offers different types of volumes based on different characteristics, such as gp2, gp3, io2 Block Express3, and io1.</p> <ul> <li>gp2/ gp3 (SSD): General purpose SSD volumes</li> <li>io1/ io2 (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads</li> <li>st1 (HDD): Low cost HDD volume designed for frequency accessed, throughput-intensive workloads</li> <li>sc1 (HDD): Lowest cost HDD volume designed for less frequency accessed workloads</li> </ul> <p>[!NOTE] io1/io2 supports EBS multi-attach, that mean we can attach same EBS volume to multiple EC2 instances in the same AZ.</p>"},{"location":"devops/aws/ec2/#ebs-snapshots","title":"EBS snapshots","text":"<p>EBS snapshots make a backup of your EBS volume at a point in time. We can copy snapshots across AZ or region.</p>"},{"location":"devops/aws/ec2/#instance-store","title":"Instance store","text":"<p>An instance store is a temporary block storage for an instance physically attached to the host. Instance storage is also known as ephemeral storage. It is the fastest storage block available for EC2 since it is physically attached to the host, but not all the EC2 instance families support instance stores; for example C6 , and R6 EC2 families don\u2019t support instance stores, while M5 EC2 instance family supports instance stores.</p> <p>An instance store can not be attached or detached once the instance is launched and only exists during the lifetime of the instance. It is important to note that no two instances can be attached to a single ephemeral storage. The instance store is ideal for temporary memory and cache, offering high read-and-write IOPS and high-performance hardware.</p>"},{"location":"devops/aws/ec2/#elastic-file-system","title":"Elastic File System","text":"<p>Amazon Elastic File System (Amazon EFS) provides serverless, fully elastic file storage so that you can share file data without provisioning or managing storage capacity and performance. EFS allows simultaneous access by various EC2 instances across different AZs within the same region, facilitating a shared data source for applications operating on multiple servers.</p> <p></p>"},{"location":"devops/aws/ec2/#efs-vs-ebs","title":"EFS vs EBS","text":"<p>EBS</p> <ul> <li>Can be attached to only one instance at a time</li> <li>Are locked at the AZ</li> <li>Can not be directly accessed via Internet</li> <li>Not a managed service. Resources should be provisioned beforehand by the user</li> <li>Data remains in the same AZ and multiple replicas are created within the same AZ</li> </ul> <p>EFS</p> <ul> <li>Mounting 100s of instances across AZ</li> <li>Only for Linux instances</li> <li>Can be accessed via Internet</li> <li>A complete managed service, where the resources are provisioned by AWS</li> <li>Data remains in the same region and multiple replicas are created within the same region</li> <li>Pay as you go model</li> </ul>"},{"location":"devops/aws/ecs/","title":"AWS Elastic Container Service (ECS)","text":""},{"location":"devops/aws/ecs/#amazon-elastic-container-registry","title":"Amazon Elastic Container Registry","text":"<p>Amazon Elastic Container Registry (ECR) is an AWS managed Docker container registry service. ECR provides a secure and scalable repository to store, manage, and deploy Docker images. ECR offers both public and private registries that can allow multiple repositories in the registries.</p>"},{"location":"devops/aws/ecs/#amazon-elastic-container-service-ecs","title":"Amazon Elastic Container Service (ECS)","text":"<p>Amazon ECS is a fully managed service by AWS that helps you deploy, manage, and scale containerized applications. It uses Docker containers to run your apps and is divided into three key layers: Provisioning, Controller, and Capacity.</p>"},{"location":"devops/aws/ecs/#1-provisioning","title":"1. Provisioning","text":"<p>This layer is responsible for the tools that help you deploy and manage your containers. You can interact with ECS using:</p> <ul> <li>AWS SDKs (for integration into custom apps)</li> <li>Copilot (a command-line tool to simplify containerized app deployment)</li> <li>AWS CLI (Command Line Interface)</li> <li>...</li> </ul>"},{"location":"devops/aws/ecs/#2-controller","title":"2. Controller","text":"<p>The controller layer is responsible for managing containers and their configurations. Here\u2019s how it works:</p> <ul> <li>Task Definition: Think of this as a blueprint for your container. It defines the container's image, CPU, memory, IAM role and other settings.</li> <li>Task/ Service: A task is simply an instance of a task definition (i.e., a running container).</li> <li>Cluster: A cluster is a group of tasks or services that run on shared infrastructure. You can run many services and tasks in the same cluster.</li> </ul>"},{"location":"devops/aws/ecs/#3-capacity","title":"3. Capacity","text":"<p>This layer handles the infrastructure where containers are actually running. ECS supports three types of infrastructure:</p> <ul> <li>On-premises: Containers can run on your local infrastructure.</li> <li>EC2 Instances: Containers are deployed on AWS EC2 virtual machines.</li> <li>AWS Fargate: A serverless option where AWS handles all the infrastructure for you, so you only focus on your containers.</li> </ul> <p></p>"},{"location":"devops/aws/ecs/#ec2-launch-type","title":"EC2 Launch Type","text":""},{"location":"devops/aws/ecs/#ec2-launch-type_1","title":"EC2 Launch Type","text":""},{"location":"devops/aws/ecs/#fargate","title":"Fargate","text":""},{"location":"devops/aws/ecs/#amazon-eks","title":"Amazon EKS","text":"<p>Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. EKS offers different compute services, from serverless Fargate to EC2 to offer high availability, security, and seamless integration with other AWS services for enhanced development and operational efficiency.</p>"},{"location":"devops/aws/ecs/#how-eks-works","title":"How EKS works","text":"<p>An EKS cluster consists of two major components: Control plane and Nodes. When provisioning an EKS cluster to run an application, the control plane is hosted on the AWS-managed VPC, whereas the worker nodes and other infrastructure are hosted in the customer-managed VPC.</p> <p>The worker nodes may communicate with the control plane through the managed API endpoint in the control plane. The worker nodes are connected to the control plane through the API or an EKS-managed ENI, placed in the customer subnets.</p> <p></p>"},{"location":"devops/aws/elb_asg/","title":"Elastic Load Balancing and Auto Scaling groups","text":""},{"location":"devops/aws/elb_asg/#elastic-load-balancing","title":"Elastic Load Balancing","text":"<p>ELB acts as a single point of interaction for clients and distributes incoming traffic across multiple targets (e.g., EC2 instances or IP addresses).</p> <p>Why use a load balancer</p> <ul> <li>Distribute Traffic: Spread load evenly across multiple instances.</li> <li>Single Access Point: Expose a single endpoint for your app.</li> <li>Fault Tolerance: Automatically handle failures of backend instances.</li> <li>Health Checks: Monitor the health of instances and route traffic only to healthy targets.</li> <li>SSL Termination: Manage HTTPS traffic with centralized SSL/TLS certificates.</li> <li>Sticky Sessions: Keep sessions consistent using cookies.</li> <li>High Availability: Distribute traffic across multiple availability zones.</li> <li>Public/Private Traffic Separation: Manage internal and external traffic separately.</li> </ul> <p>How ELB Work</p> <ol> <li>The client sends a request to the load balancer.</li> <li>The listeners in your load balancer receive requests matching the protocol and port that you configure (e.g., HTTPS on port 443)..</li> <li>The receiving listener evaluates the incoming request against the rules you specify, and if applicable, routes the request to the appropriate target group. You can use an HTTPS listener to offload the work of TLS encryption and decryption to your load balancer.</li> <li>Healthy targets in one or more target groups receive traffic based on the load-balancing algorithm, and the routing rules you specify in the listener.</li> </ol>"},{"location":"devops/aws/elb_asg/#types-of-load-balancer-on-aws","title":"Types of load balancer on AWS","text":"<ol> <li>Application Load Balancers: Ideal for routing HTTP/HTTPS traffic and performing advanced traffic routing and content-based routing - Layer 7</li> <li>HTTP, HTTPS, gRPC, WebSocket</li> <li>Network Load Balancers: Designed for handling TCP/UDP traffic with high performance and low latency - Layer 4</li> <li>Gateway Load Balancers: Used for deploying third-party virtual appliances, such as firewalls, intrusion detection systems, and other network appliances - Layer 3</li> <li>Classic Load Balancers: An older type of load balancer that is still available for use, primarily for applications not yet migrated to the newer load balancer types.</li> </ol> <p>[!NOTE] When using ALB, the application servers don't see the IP of the client directly. The true IP of the client is inserted in the header <code>X-Forwarded-For</code></p>"},{"location":"devops/aws/elb_asg/#application-load-balancer","title":"Application Load Balancer","text":"<p>An Application Load Balancer operates on the application layer (the seventh layer of the OSI model) and handles HTTP/HTTPS/Web Socket requests.</p>"},{"location":"devops/aws/elb_asg/#target-groups-in-alb","title":"Target groups in ALB","text":"<p>Target groups are used to define where an ALB routes the incoming requests from a client. While creating a target group, we define the type of targets, the protocols, and the ports. ALBs support EC2 instances, IP addresses, and Lambda functions as its target types, HTTP and HTTPS as its protocols, and ports 1-65535.</p> <p></p>"},{"location":"devops/aws/elb_asg/#routing-algorithms-in-alb","title":"Routing algorithms in ALB","text":"<p>Application Load Balancers support various routing algorithms to distribute incoming requests across multiple target groups.</p> <ul> <li>Round robin: This is the default algorithm used by ALBs and distributes incoming requests evenly across multiple targets in a sequential order.</li> <li>Least outstanding requests: Here, incoming requests are sent to the target with the least number of requests whose status is in progress.</li> <li>Weighted random: In this algorithm, weights are assigned randomly to requests that evenly distribute requests across targets in a random order.</li> </ul>"},{"location":"devops/aws/elb_asg/#listeners-in-alb","title":"Listeners in ALB","text":"<p>Application Load Balancers use listeners, that check connection requests received from the client. We define rules in listeners that determine how our load balancer routes the requests it receives. A single listener can have multiple rules and are evaluated according to the priority we set up. Following are the conditions we can define in rules for our listeners:</p> <ul> <li>HTTP header conditions: These rules are created to handle client requests based on their HTTP headers.</li> <li>HTTP request method conditions: These rules are created to handle client requests based on their HTTP request methods.</li> <li>Host conditions: Rules can be created to route requests based on the name of the host.</li> <li>Path conditions: Path conditions in the URL of the request can be used to create rules to route requests to different targets.</li> <li>Query string conditions: Key/value pairs in the query string of the incoming request can be used to create rules to route requests.</li> <li>Source IP address conditions: Rules can be created that route incoming requests based on the source IP address. This IP address must be specified in the CIDR format.</li> </ul> <p></p>"},{"location":"devops/aws/elb_asg/#network-load-balancer","title":"Network Load Balancer","text":"<p>Network Load Balancer (NLB) operates on the transport layer, the fourth layer of the OSI model, and is used to distribute incoming TCP and UDP traffic across multiple targets. NLB uses a single static IP address and is optimized to handle sudden and volatile traffic patterns.</p>"},{"location":"devops/aws/elb_asg/#target-groups-in-nlb","title":"Target groups in NLB","text":"<p>Network Load Balancers support EC2 instances, private IP addresses, and Application Load Balancers as their targets. We can use on-premises servers as targets if the selected target type is IP by connecting them to the AWS cloud using AWS Direct Connect or AWS Site-to-Site VPN.</p> <p>Network Load Balancers can forward the requests it receives to an Application Load Balancer. Following are some use cases where ALBs are added as targets for NLBs:</p> <ul> <li>Multimedia services: A combination of NLB and ALB can be used in multimedia services where a single endpoint is required for multiple protocols, such as HTTP for signaling and RTP to stream content.</li> <li>AWS PrivateLink: An NLB can be used to create a route between clients and an ALB over AWS PrivateLink.</li> </ul>"},{"location":"devops/aws/elb_asg/#gateway-load-balancer","title":"Gateway Load Balancer","text":"<p>Gateway Load Balancers (GLB) operate on the network layer, the third layer of the OSI model, and allow us to maintain, scale, and deploy third-party virtual appliances, such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems.</p> <p></p> <p>In the diagram above, traffic coming from the internet uses the internet gateway to get to the GLB endpoint located in the consumer VPC. This endpoint routes this traffic to the Gateway Load Balancer, which distributes this traffic to the virtual appliance (the target of the GLB). Once the virtual appliance analyzes the traffic, it is sent back to the Gateway Load Balancer, which sends it to the application servers located in the consumer VPC.</p>"},{"location":"devops/aws/elb_asg/#sticky-session","title":"Sticky session","text":"<p>Sticky sessions are a way to ensure that incoming requests, which belong to the same client session, land on the same target behind the load balancer. This feature is invaluable for applications that depend on server-side session state persistence.</p> <p>Benefits of using sticky sessions</p> <ul> <li>User experience consistency: This means, across requests, the state of the sessions should be consistent, and therefore, users should always experience consistency in usage. This uniformity is key to building trust and reliability in the application, as it reassures users that their actions and data are persistently recognized across their journey.</li> <li>Improved performance: This reduces the overhead of re-establishing the session state with each request and improves application performance. By streamlining session management, applications can serve users more swiftly and efficiently, leading to better overall performance and a more satisfying user experience.</li> </ul> <p>Considerations for using sticky sessions</p> <p>While sticky sessions offer significant benefits, they also present challenges that need careful consideration.</p> <ul> <li>Drawback: Sticky sessions can significantly affect the scalability of an application and limit the load balancer\u2019s ability to distribute traffic equally across all targets. This can lead to some servers being overburdened while others remain underutilized, potentially causing uneven resource consumption and performance issues across the infrastructure.</li> <li>Fault tolerance: In cases where a session is sticky to a target, unless the application is designed to duplicate the session states, this could lead to lost sessions in the event of a server failure. Therefore, implementing mechanisms for session state redundancy is crucial to prevent data loss and ensure the continuity of user sessions.</li> <li>Session data management: The application must be designed to manage session data effectively, especially in distributed environments where there is a need to replicate session data or centrally store it. This requires a well-thought-out strategy for session data synchronization and storage to facilitate a seamless user experience and robust data management practices.</li> </ul> <p>[!NOTE] It's important to note that stickiness, or session affinity, is not supported by Network Load Balancer (NLB).</p>"},{"location":"devops/aws/elb_asg/#cross-zone-load-balancing","title":"Cross zone load balancing","text":"<p>In Elastic Load Balancers, incoming traffic is distributed evenly across load balancer nodes in each AZ, which then forwards these requests to the targets registered within that AZ. However, Elastic Load Balancers allow us to enable cross-zone load balancing, where all load balancer nodes can distribute traffic to all available healthy targets, irrespective of their Availability Zone.</p> <p></p> <p>Cross-zone balancing in different load balancers</p> <p>Application Load Balancer (ALB)</p> <ul> <li>Always enabled (cannot be disabled).</li> <li>No inter-AZ data transfer charges.</li> </ul> <p>Network Load Balancer (NLB)</p> <ul> <li>Disabled by default but can be enabled.</li> <li>Enabling incurs inter-AZ data transfer charges.</li> </ul> <p>Gateway Load Balancer (GLB)</p> <ul> <li>Optionally enabled to distribute traffic across all AZs.</li> </ul> <p>Classic Load Balancer (CLB)</p> <ul> <li>Disabled by default but can be enabled.</li> <li>No inter-AZ data transfer charges.</li> </ul>"},{"location":"devops/aws/elb_asg/#auto-scaling-groups","title":"Auto Scaling Groups","text":"<p>Amazon Auto Scaling Groups (ASG) is a service provided by AWS that helps your applications automatically adjust the number of EC2 instances they are using based on the load and demand. This ensures your application has enough capacity to handle traffic while minimizing unnecessary costs.</p> <p>The goal of the ASG is to:</p> <ul> <li>Scale out: Automatically add more EC2 instances when there is increased load.</li> <li>Scale in: Automatically remove EC2 instances when there is less load.</li> <li>Minimum and maximum instance limits: Ensure there are always a minimum number of instances (to maintain uptime) and never more than a specified maximum (to control costs).</li> <li>Automatic load balancing: Automatically register new instances with a load balancer to distribute traffic across all healthy instances.</li> <li>Instance replacement: If an instance is terminated (e.g., due to failure or scaling in), ASG can automatically create a new instance to replace it.</li> </ul> <p>Attributes of an Auto Scaling Group</p> <ol> <li> <p>Launch Template:</p> </li> <li> <p>A blueprint that defines the configuration for new EC2 instances.</p> </li> <li> <p>Includes settings such as the AMI (Amazon Machine Image), instance type, key pair, security groups, and any startup scripts.</p> </li> <li> <p>Minimum Size / Maximum Size / Initial Capacity</p> </li> <li> <p>The minimum number of instances ASG will maintain.</p> </li> <li>The maximum number of instances allowed in the group.</li> <li> <p>The starting number of instances when the ASG is created.</p> </li> <li> <p>Scaling Policies</p> </li> <li> <p>Define how the ASG should respond to changes in load.</p> </li> <li>Can be configured based on metrics like CPU usage, network activity, or custom metrics.</li> </ol> <p>ASG can also scale automatically based on CloudWatch alarms (e.g., if CPU utilization exceeds a threshold, ASG can trigger scaling).</p> <p>Recommended Metrics for Scaling</p> <ul> <li>CPUUtilization: Monitors the average CPU usage across all instances in the ASG. Scaling out can occur when CPU usage is too high.</li> <li>RequetsCountPerTarget: Ensures the number of requests per instance is stable. Useful for web applications or APIs.</li> <li>Average Network In/ Out: Useful for applications that are network-intensive.</li> <li>Custom metric: You can define custom metrics (e.g., queue length or memory usage) based on the unique needs of your application.</li> </ul> <p>Scaling Cooldown Period</p> <ul> <li>After a scaling activity (either scaling out or scaling in), the ASG enters a cooldown period.</li> <li>Default Cooldown: 300 seconds (5 minutes).</li> <li>During this period, no further scaling actions will be taken to allow the system to stabilize. This helps prevent over-scaling or unnecessary actions due to temporary spikes or drops in metrics.</li> </ul> <p>How It All Works Together</p> <ol> <li>If traffic to your application increases, CloudWatch monitors metrics (e.g., high CPU usage or increased requests).</li> <li>ASG triggers a scale-out action and launches new EC2 instances using the launch template.</li> <li>New instances are automatically registered to a load balancer.</li> <li>When traffic decreases, ASG triggers a scale-in action and terminates excess instances.</li> </ol>"},{"location":"devops/aws/iam/","title":"IAM","text":"<p>IAM is a service that controls access to AWS resources</p>"},{"location":"devops/aws/iam/#components-of-iam","title":"Components of IAM","text":"<p>IAM entities: These are the IAM resources to authenticate the requesting entity. These include the following:</p> <ul> <li>IAM users</li> <li>IAM roles</li> </ul> <p>IAM identities: The IAM resources that IAM uses to check the permissions scope of the requesting entity. These include the following:</p> <ul> <li>IAM users</li> <li>IAM roles</li> <li>IAM groups</li> </ul> <p>Principal: The user, service, or application that requests access to an IAM service or a resource. It can be both an external or an internal entity.</p> <p>Other IAM resources: These are the IAM resources that do not fall into any of the above categories. These are used for a wide range of operations that deal with identity and access management. These include the following:</p> <ul> <li>IAM policies</li> <li>Identity providers</li> <li>Access Analyzer</li> </ul>"},{"location":"devops/aws/iam/#how-iam-works","title":"How IAM works","text":"<p>When an entity requests access to any of the AWS services or resources, that request is first analyzed by IAM. IAM checks the credentials provided by the requesting entity to authenticate it. After the requesting entity has been authenticated, it analyzes the permissions granted to the entity and checks if the current request falls into that pool of permissions</p> <p></p>"},{"location":"devops/aws/iam/#iam-policy","title":"IAM policy","text":"<p>AM policy is a JSON document attached to the AWS resource that is used by the logged-in entity to authenticate itself or to the AWS resource to which secure access is required.</p>"},{"location":"devops/aws/iam/#identity-based-policies","title":"Identity-based policies","text":"<p>Identity-based policies are attached to an IAM user, group, or role. These policies let you specify what that identity can do (its permissions).</p> <p>The structure of Identity-based policy</p> Column1 Explanation Effect This specifies if the associated actions are allowed or denied. Its value can either be <code>Allow</code> or <code>Deny</code>. Action This contains a list of actions. Resource This contains resources on which the actions will be effective. Condition This contains resources on which the actions will be effective. <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:CreateBucket\"],\n      \"Resource\": [\"arn:aws:s3:::demo-bucket\"]\n    }\n  ]\n}\n</code></pre> <p>This policy allows the principal entity to create an S3 bucket by the name <code>demo-bucket</code>.</p>"},{"location":"devops/aws/iam/#resource-based-policies","title":"Resource-based policies","text":"<p>Resource-based policies are attached to a resource. For example, you can attach resource-based policies to Amazon S3 buckets, Amazon SQS queues... With resource-based policies, you can specify who has access to the resource and what actions they can perform on it.</p> <p>The structure of Resource-based policy</p> Column1 Explanation Effect This specifies if the associated actions are allowed or denied. Its value can either be <code>Allow</code> or <code>Deny</code>. Action This contains a list of actions. Resource This contains resources on which the actions will be effective. Principal This specifies the principal (such as an IAM user, roles, and others) who is allowed (or denied) access to the resource. Condition This contains resources on which the actions will be effective. <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::demo-bucket/demo-object\",\n      \"Principal\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>This policy allows anyone to read <code>demo-object</code> stored in an S3 bucket by the name <code>demo-bucket</code>.</p>"},{"location":"devops/aws/iam/#aws-iam-roles","title":"AWS IAM Roles","text":"<p>AWS IAM Roles are a feature of IAM that allows you to grant permissions to entities that you trust to access your AWS resources. An entity can be an AWS service, an IAM user, an IAM role, or an external identity provider. By using IAM roles, you can delegate access to your AWS resources without sharing your long-term credentials, such as your IAM username and password or access keys.</p>"},{"location":"devops/aws/iam/#how-iam-roles-work","title":"How IAM Roles Work","text":"<p>An IAM role is an IAM identity that has a set of permissions attached to it. Unlike an IAM user, an IAM role does not have its own credentials. Instead, an entity that assumes an IAM role is temporarily granted the permissions of the role. The entity can then use those permissions to access your AWS resources.</p> <p>To assume an IAM role, an entity needs to have permission to do so. This permission is granted by a trust policy that is attached to the role. The trust policy defines which entities are allowed to assume the role. For example, you can create a trust policy that allows an EC2 instance to assume a role, or a trust policy that allows a user from another AWS account to assume a role.</p> <p>When an entity assumes an IAM role, it receives a set of temporary security credentials that consist of an access key ID, a secret access key, and a session token. The entity can use these credentials to make API requests to AWS services. The credentials are valid for a limited time, typically from a few minutes to several hours. When the credentials expire, the entity can request new credentials or stop using the role.</p> <p></p> <p>Example of trust policy</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"sts:AssumeRole\"],\n      \"Principal\": {\n        \"Service\": [\"ec2.amazonaws.com\"]\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"devops/aws/iam/#restricting-policies","title":"Restricting Policies","text":""},{"location":"devops/aws/iam/#permission-boundary","title":"Permission boundary","text":"<p>AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.</p> <p></p>"},{"location":"devops/aws/iam/#session-policy","title":"Session policy","text":"<p>Similar to permission boundaries, a session policy is a restricting policy. Its application is however different from that of permission boundary. Session policies are used to set an upper bound on the permissions of a session which is created when a role is assumed. It can only be attached when a role is assumed programatically.</p> <p>When do we need a session policy?</p> <p>An IAM role can be used to create multiple sessions at once. By default, the permissions of each session are the ones allowed by the IAM policy attached with the role. Consider a scenario where we've created a role to provide temporary access to external individuals in different roles. We have an admin access policy attached with the role that allows the assuming entities to perform all the actions within that account. What we want next is a tool to limit the permissions of each session based on the role of the individuals assuming the role. This is where session policy comes in. We can attach a session policy with each session, based on the role of the individual allowing them to perform only the actions that should be allowed to that entity.</p> <p>So if we're creating a session for an entity to manage DynamoDB operations, we'll attach a session policy that'll restrict that entity from doing anything out of the scope of their role.</p> <p></p>"},{"location":"devops/aws/iam/#permission-intersections","title":"Permission intersections","text":""},{"location":"devops/aws/iam/#permissions-boundary-with-identity-based-policy","title":"Permissions boundary with identity-based policy","text":"<p>Permissions boundary as a superset of identity-based policy: Only the permissions that are common in both the identity-based policy and permissions boundary are effective. Any additional permissions in the permissions boundary are ineffective.</p> <p></p> <p>Permissions boundary as a subset of identity-based policy: Only the permissions that are common in both the identity-based policy and permissions boundary are effective. Any additional permissions in the identity-based policy are ineffective</p> <p></p> <p>Overlapping permissions boundary and identity-based policy: Only the permissions that are common in both the identity-based policy and permissions boundary are effective. Any additional permissions in the identity-based policy or permissions boundary are ineffective.</p> <p></p> <p>Denial effect in permissions boundary or identity-based policy: The deny permissions defined in the permissions boundary always take precedence. However, other than that, only the common permissions in the identity-based policy and permissions boundary are effective.</p> <p>NOTE: The denial effect in any policy always takes precedence, even if the action is common in all the relevant policies.</p> <p>Permissions boundary, identity-based policy and resource-based policy: The deny action also takes precedence in this case. Apart from that, the common permissions in both the identity-based policy and permissions boundary are allowed, AND all the permissions mentioned in the resource-based policy are allowed.</p> <p></p>"},{"location":"devops/aws/iam/#best-practices","title":"Best practices","text":"<ul> <li>One IAM User per person ONLY</li> <li>One IAM Role per Application</li> <li>Never use the ROOT account except for initial setup</li> <li>It's best to give users the minimal amount of permissions to perform their job</li> </ul>"},{"location":"devops/aws/lambda/","title":"Lambda","text":"<p>AWS Lambda is a serverless compute service that allows us to run code without provisioning or managing servers.</p>"},{"location":"devops/aws/lambda/#cold-starts-in-lambda-function","title":"Cold starts in Lambda function","text":"<p>One of the reasons AWS Lambda is cost-effective is that you only pay when your code runs, unlike EC2 servers that incur costs even when idle. However, the trade-off is that when a Lambda function is invoked after a period of inactivity, it takes some time to initialize the environment before running your code. This delay is known as a <code>cold start</code>.</p> <p>AWS keeps Lambda environments <code>warm</code> for a short period after execution to allow subsequent invocations without a <code>cold start</code>. <code>Cold starts</code> occur roughly 1% of the time under typical conditions, but spiky or unpredictable loads can lead to more frequent cold starts.</p> <p>Stages of Lambda Invocation</p> <ol> <li>Download Code: Code is loaded from S3 or ECR.</li> <li>Start Execution Environment: A new container is initialized.</li> <li>Execute Initialization Code: Global objects or connections are initialized.</li> <li>Execute Handler Code: The core function logic is run.</li> </ol> <p>The first two stages are responsible for cold starts and are not charged. However, the third step contributes to cold start latency from a user perspective. If the same Lambda environment is reused, only the handler code (step 4) is executed.</p> <p>Persistent Resources</p> <p>Global variables, such as database connections, may persist between Lambda invocations because the environment can be reused.</p> <p>Warmers and Scalability Considerations</p> <p>Using a <code>warmer</code> function to invoke the Lambda every minute can help reduce cold starts, but this strategy is only effective for single-threaded scenarios. When multiple simultaneous requests are received, new Lambda environments are spun up, each experiencing its own cold start.</p> <p>AWS Lambda may invoke new environments across different availability zones to balance load, so quick successive requests may not reuse the same environment.</p> <p>Cold Start Duration</p> <p>Cold starts generally last between 100 ms and 1 second. Although you are not charged for environment setup, the additional latency can impact the user experience.</p>"},{"location":"devops/aws/lambda/#lambda-layers","title":"Lambda Layers","text":"<p>Lambda layers allow us to centrally manage and share code, libraries, and dependencies across multiple Lambda functions. Instead of including all dependencies within the deployment package of each function, Lambda layers enable us to package common components separately in a .zip file, making it easier to maintain, update, and reuse code.</p> <p></p>"},{"location":"devops/aws/lambda/#lambda-limits","title":"Lambda limits","text":"<p>The Lambda function has certain limitations as compared to other computing services</p> <ul> <li>The Lambda function's maximum execution time can be increased to 900 seconds (15 minutes). We would have to switch to other computing options for workloads with execution time of more than 15 minutes.</li> <li>The maximum size of the compressed deployment package for the Lambda function is 50 MB. On the other hand size of uncompressed deployments, including code and dependencies, can exceed to 250 MB.</li> <li>The disk capacity in the function container ranges from 512 MB to 10 GB.</li> <li>By default, Lambda supports 1000 concurrent executions in an account. However, this number can be increased.</li> <li>The maximum size of environment variables of a function is 4 KB.</li> </ul>"},{"location":"devops/aws/lambda/#good-to-read","title":"Good to read","text":"<p>Lambda Cold Starts and Bootstrap Code</p>"},{"location":"devops/aws/nacl-and-security-groups/","title":"NACL and Security Groups","text":"<p>Network access control lists (NACLs) and security groups are types of firewalls that control the network traffic.</p> <p>Security groups are stateful firewalls that analyze everything in the data packets of the incoming traffic and maintain the state. We only need to configure rules for the incoming traffic, and the stateful firewall automatically configures the outgoing rules accordingly.</p> <p>The NACLs are stateless firewalls that check the source, destination, and other parameters/rules to allow or reject the traffic.</p>"},{"location":"devops/aws/nacl-and-security-groups/#security-groups","title":"Security groups","text":"<p>In the AWS environment, a security group is a VPC-based resource that works at the EC2 instance level. It validates the incoming traffic and allows only connection requests passed by the inbound rules. We specify a security group to secure our EC2 instance; if no security group is selected, EC2 uses the default security group of the VPC. The default security group has no inbound rules and allows all outbound traffic.</p>"},{"location":"devops/aws/nacl-and-security-groups/#nacls","title":"NACLs","text":"<p>A network access control list (NACL) is a VPC-based firewall that works on the subnet level and controls the ingress and egress traffic. Because of its stateless nature, we need to take care of the outbound and inbound rules. Every inbound rule must have an outbound rule if we want the traffic to leave our network. In NACLs, each rule is assigned a rule number that is processed in ascending order. This means that only one rule is processed at a time. We don\u2019t get charged for using NACLs.</p> <p></p>"},{"location":"devops/aws/nacl-and-security-groups/#nacls-rules","title":"NACLs rules","text":""},{"location":"devops/aws/nacl-and-security-groups/#nacl-inbound-rules","title":"NACL Inbound Rules","text":"Rule # Type Protocol Port Range Source IP Action 100 HTTP TCP 80 0.0.0.0/0 ALLOW 110 HTTPS TCP 443 0.0.0.0/0 ALLOW 120 SSH TCP 22 192.168.0.0/24 DENY 130 ICMP ICMP N/A 0.0.0.0/0 ALLOW 150 All traffic All All 0.0.0.0/0 DENY"},{"location":"devops/aws/nacl-and-security-groups/#default-inbound-rules","title":"Default inbound rules","text":"Rule # Type Protocol Port Range Source IP Action 100 All traffic All All 0.0.0.0/0 ALLOW * All traffic All All 0.0.0.0/0 DENY <p>Rule <code>100</code> is an <code>allow</code> rule that permits all types of traffic (regardless of protocol or port) from any source IP address to enter the subnet.</p> <p>The catch-all rule, denoted by <code>*</code> is a default rule that applies to any traffic that does not match the preceding rules. In this case, it denies all traffic. This is a common practice in security to ensure that any traffic not explicitly allowed is automatically denied, providing an additional layer of security.</p>"},{"location":"devops/aws/nacl-and-security-groups/#nacl-outbound-rules","title":"NACL Outbound Rules","text":"Rule # Type Protocol Port Range Destination IP Action 100 HTTP TCP 80 0.0.0.0/0 ALLOW 110 HTTPS TCP 443 0.0.0.0/0 ALLOW 120 SSH TCP 22 192.168.0.0/24 DENY 130 ICMP ICMP N/A 0.0.0.0/0 ALLOW 150 All traffic All All 0.0.0.0/0 DENY"},{"location":"devops/aws/nacl-and-security-groups/#default-outbound-rules","title":"Default outbound rules","text":"Rule # Type Protocol Port Range Source IP Action 100 All traffic All All 0.0.0.0/0 ALLOW * All traffic All All 0.0.0.0/0 DENY <p>Rule <code>100</code> is an <code>allow</code> rule that permits all outbound traffic to any destination.</p> <p>The catch-all rule, denoted by <code>*</code> is a default rule that applies to any traffic that does not match the preceding rules. This ensures that if any modifications to rule <code>100</code> or additional rules are added above <code>*</code> only the specified traffic is allowed, and all other traffic is denied by default.</p>"},{"location":"devops/aws/nacl-and-security-groups/#security-group-vs-nacl","title":"Security group vs NACL","text":"Security group Network ACL Operates at the instance level Operates at the subnet level Applies to an instance only if it is associated with the instance Applies to all instances deployed in the associated subnet (providing an additional layer of defense if security group rules are too permissive) Supports allow rules only Supports allow rules and deny rules Evaluates all rules before deciding whether to allow traffic Evaluates rules in order, starting with the lowest numbered rule, when deciding whether to allow traffic Stateful: Return traffic (outbound) is allowed, regardless of the rules Stateless: Return traffic (outbound) must be explicitly allowed by the rules"},{"location":"devops/aws/rds/","title":"RDS","text":"<p>RDS stands for Relational Database Service. It's allow you to create database in the cloud that are managed by AWS</p> <ul> <li>Postgres</li> <li>MySQL</li> <li>MariaDB</li> <li>Oracle</li> <li>Microsoft SQL Server</li> <li>Aurora (AWS proprietary database)</li> </ul>"},{"location":"devops/aws/rds/#rds-read-replicas","title":"RDS read replicas","text":"<ul> <li>Up to 5 Read Replicates</li> <li>Within AZ, Cross AZ, Cross Region</li> <li>Replication is ASYNC, so read are eventual consistency</li> <li>Replicas can be promoted to their own DB</li> <li>Application must update to the connection string to leverage read replicas</li> <li>RDS replicas within the same region, you don't pay that fee.</li> <li>RDS replicas across region, you must pay that fee.</li> </ul> <p>Benefit</p> <ul> <li>Improved reliability and disaster recovery</li> <li>Increased app performance</li> </ul>"},{"location":"devops/aws/rds/#rds-multi-az","title":"RDS multi AZ","text":"<ul> <li>SYNC replication</li> <li>One DNS name - automatic app failover to standby</li> <li>Increased the availability</li> <li>Not used for scaling</li> <li>Multi-AZ replication is free</li> </ul>"},{"location":"devops/aws/rds/#aurora","title":"Aurora","text":"<p>Amazon Aurora is a relational database service developed and offered by Amazon Web Service. Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Aurora was Postgres or MySQL database).</p> <p>Aurora claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS</p> <p>Aurora storage automatically grows in increments of 10Gb up to 128 Tb.</p> <p>Aurora can have up to 15 replicas (MySQL has 5) and replication process is faster</p> <p>Aurora costs more than RDS (20% more)</p>"},{"location":"devops/aws/rds/#aurora-high-availability-and-read-scaling","title":"Aurora High Availability and Read scaling","text":"<p>Aurora stores copies of the data in a DB cluster across multiple Availability Zones in a single AWS Region. When data is written to the primary DB instance, Aurora synchronously replicates the data across Availability Zones to six storage nodes associated with your cluster volume.</p> <p></p> <p></p>"},{"location":"devops/aws/rds/#best-practice-in-rds","title":"Best practice in RDS","text":"<ul> <li>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_BestPractices.html</li> <li>https://www.youtube.com/watch?v=9-7azhB27So</li> </ul>"},{"location":"devops/aws/route_53/","title":"Route 53","text":"<p>Amazon Route 53 is an AWS web service that provides Domain Name System (DNS) functionality. Route 53 offers two main services: domain registration and domain hosting. Route 53 is a globally resilient service as the name servers are distributed globally and have the same datasets across all the servers. So, even if a region is affected by outages, Route 53 will still function.</p>"},{"location":"devops/aws/route_53/#domain-name-system-dns","title":"Domain Name System (DNS)","text":"<p>Every device connected to the internet has its unique IP address, enabling communication with it. When we browse the internet, we access web applications hosted on servers, each identified by a distinct IP address.</p> <p>The Domain Name System (DNS) is a hierarchical decentralized naming system that translates human-readable domain names, such as <code>dinhhuy258.dev</code>, into IP addresses. The DNS organizes domain names into a hierarchical tree-like structure:</p> <ul> <li>At the top are root domain servers, the highest authority in DNS.</li> <li>Below are top-level domains (TLDs) like <code>.com</code> and country-code TLDs. Each TLD is managed by its registry.</li> <li>Second-level domains (SLDs) sit beneath TLDs and often represent organizations. Subdomains can extend from SLDs.</li> </ul> <p></p>"},{"location":"devops/aws/route_53/#dns-zone","title":"DNS Zone","text":"<p>A DNS zone is a distinct, logical segment within the domain namespace of the Domain Name System (DNS). It provides administrators, organizations, or other entities with fine-grained control over DNS records and configurations for specific parts of a domain.</p> <p>For instance, the domain <code>example.com</code> can be part of a larger DNS zone. Within this zone, subdomains like <code>blog.example.com</code> and <code>community.example.com</code> can exist. If <code>community.example.com</code> requires more specific or granular management\u2014perhaps due to a large number of devices or high traffic\u2014the administrator could decide to split it into its own DNS zone with a separate authoritative name server, thus giving independent control over its DNS records.</p>"},{"location":"devops/aws/route_53/#dns-records","title":"DNS Records","text":"Record Type Value SOA records Provides authoritative information about a DNS zone, including details such as the primary name server for the zone, the email address of the zone administrator, and other zone management parameters. NS records Specifies the authoritative name servers for a domain. These servers are responsible for providing DNS information about the domain. A records Maps a domain name to an IPv4 address. For example, we might have an A record that maps <code>example.com</code> to <code>192.0.2.1</code>\". AAAA records Maps a domain name to an IPv6 address. For example, we might have an A record that maps <code>example.com</code> to <code>2001:db8:3333:4444:5555:6666:7777:8888</code>. CNAME records Maps one domain name to another. For instance, we might use a CNAME record to point <code>www.example.com</code> to <code>example.com</code>. MX records Specifies mail servers responsible for receiving email on behalf of a domain. MX records often point to mail servers like <code>mail.example.com</code>. TXT records Stores arbitrary text information associated with a domain name. This can be used for various purposes, such as verifying domain ownership or providing SPF (Sender Policy Framework) records for email authentication."},{"location":"devops/aws/route_53/#amazon-route-53","title":"Amazon Route 53","text":""},{"location":"devops/aws/route_53/#domain-registration","title":"Domain Registration","text":"<p>AWS Route 53 enables you to register domain names, search for available domains, and manage DNS records all in one place. After registration, Route 53 integrates directly with its DNS service, simplifying domain management. It also offers optional WHOIS privacy protection, automatic domain renewal, and supports domain transfers from other registrars, making it easier to maintain control over your domains and associated resources.</p> <p>Additionally, you can transfer domain registration from another registrar to AWS Route 53 for supported top-level domains (TLDs). It\u2019s also possible to transfer domains between AWS accounts, streamlining management across different teams or organizations.</p>"},{"location":"devops/aws/route_53/#hosted-zones","title":"Hosted Zones","text":"<p>A hosted zone is a container for records, and records contain information about how you want to route traffic for a specific domain, such as <code>example.com</code>, and its subdomains (<code>acme.example.com</code>, <code>zenith.example.com</code>). A hosted zone and the corresponding domain have the same name. There are two types of hosted zones:</p> <ul> <li>Public hosted zones contain records that specify how you want to route traffic on the internet.</li> <li>Private hosted zones contain records that specify how you want to route traffic in an Amazon VPC</li> </ul> <p></p>"},{"location":"devops/aws/route_53/#health-checks","title":"Health checks","text":"<p>Route 53 health checks are a function that allow you to monitor the health of selected types of AWS resources or any endpoints that can respond to requests. Route 53 health checks can provide notifications of a change in the state of the health check and can help Route 53 to recognize when a record is pointing to an unhealthy resource, allowing Route 53 to failover to an alternate record.</p> <p></p> <p>Types of Route 53 health checks:</p> <ul> <li>Endpoint health checks: You can configure to monitor an endpoint that you specify by IP address or domain name. Within a fixed time interval that you specify. Route 53 submits automated requests over the Internet to your application, server, or other resources to verify that it is accessible, available, and functioning properly.</li> <li>Health checks that monitor other health checks: This type of health check monitors other Route 53 health checks. Basically, a <code>parent</code> health check will monitor one or more <code>child</code> health checks. If the provided number of child health checks report as healthy, then parent health checks will also be healthy. If the number of healthy <code>child</code> checks falls below a set threshold, the <code>parent</code> check will be unhealthy.</li> <li>Health checks for Amazon CloudWatch Alarms: You can also perform health checks associated with alarms created in the CloudWatch service. These types of Route 53 health checks monitor CloudWatch data streams sent to previously configured alarms. If the status of the CloudWatch alarm is OK, the health check will report as OK.</li> </ul>"},{"location":"devops/aws/route_53/#routing-policies","title":"Routing Policies","text":"<p>Routing policies are the rules and algorithms that route traffic to different endpoints like IP addresses, AWS resources, or other domain names based on various criteria. These routing policies provide flexibility and control over how traffic is distributed to different endpoints, allowing users to optimize the performance, availability, and cost-effectiveness of their applications and services hosted on AWS.</p>"},{"location":"devops/aws/route_53/#routing-strategies","title":"Routing strategies","text":"<p>Route 53 offers several routing policies to allow users to implement sophisticated traffic routing strategies tailored to their specific requirements. In this lesson, we will explore the common routing policies provided by Route 53. Let\u2019s explore the Routing Policies that are provided by Route 53.</p> <p>Simple routing policy</p> <p>The Simple Routing Policy is used for a single resource that performs a specific function for your domain. For example, you might use it to route traffic to an Amazon EC2 instance serving content for the <code>example.com</code> website.</p> <p>With this policy, you can configure multiple values (such as multiple IP addresses or endpoints). When multiple values are configured, Route 53 will randomly choose one to route traffic to.</p> <p>Note that the Simple Routing Policy cannot be used in conjunction with Health Checks.</p> <p></p> <p>Failover routing policy</p> <p>The Failover Routing Policy is designed to provide high availability by routing traffic to a backup resource when the primary resource becomes unavailable. It allows users to define a primary resource and a standby (failover) resource, along with health checks to monitor the availability of each resource.</p> <p>When Route 53 detects that the primary resource is unhealthy, it automatically directs traffic to the failover resource, ensuring seamless failover in case of resource failures or disruptions.</p> <p></p> <p>Weighted routing policy</p> <p>Weighted routing policy enables users to control the distribution of traffic among multiple resources by assigning weights to each resource. This allows you to assign weights to resource record sets. For instance, you can specify <code>25</code> for one resource and <code>75</code> for another, meaning that <code>25%</code> of requests will go to the first resource and <code>75%</code> will be routed to the second.</p> <p></p> <p>Latency routing policy</p> <p>Latency Routing Policy routes traffic to the resource with the lowest latency for the user, ensuring optimal performance and minimal response times.</p> <p></p> <p>This policy best suits latency-sensitive applications such as online gaming platforms, video streaming services, or real-time communication applications, where minimizing latency is essential for user satisfaction.</p> <p>Geolocation routing policy</p> <p>Geolocation routing policy directs traffic based on the user\u2019s geographic location, ensuring that users are routed to the nearest available resource or a resource optimized for their region.</p> <p></p> <p>This policy best suits content delivery networks (CDNs), regional applications, or services with data sovereignty requirements, where serving content from geographically closer servers is essential.</p> <p>Multivalue routing policy</p> <p>Multivalue Routing Policy combines elements of simple routing and failover routing to improve availability and fault tolerance. With multivalue routing, users can create multiple records with the same name, each associated with its health check. When a user queries a domain, up to eight healthy records are returned, while unhealthy ones are excluded. If more than eight healthy records exist, they are returned randomly.</p> <p></p> <p>IP-based routing policy</p> <p>IP-based routing policy directs traffic based on the IP address of the requesting device or DNS resolver. It allows users to define routing rules based on IP addresses or ranges, ensuring that requests from specific devices or networks are directed to designated resources.</p> <p></p>"},{"location":"devops/aws/s3/","title":"Amazon S3","text":"<p>Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services that provides object storage through a web service interface.</p>"},{"location":"devops/aws/s3/#buckets-and-objects","title":"Buckets and Objects","text":"<p>Amazon S3 allows people to store objects (files) in buckets (directories). Buckets must have a global unique name and are defined at the region level.</p> <p>An object consists of data, key (assigned name), and metadata. A bucket is used to store objects. When data is added to a bucket, Amazon S3 creates a unique version ID and allocates it to the object.</p> <p>Object values are the content of the body, maximum object size is 5TB, if uploading more than 5GB, must use <code>multi-part</code> upload feature.</p>"},{"location":"devops/aws/s3/#s3-versioning","title":"S3 Versioning","text":"<p>Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. You can use the S3 Versioning feature to preserve, retrieve, and restore every version of every object stored in your buckets.</p> <p>Versioning is enabled at the bucket level.</p> <p>Please refer this link to understand how versioning work.</p>"},{"location":"devops/aws/s3/#s3-encryption-for-objects","title":"S3 Encryption for Objects","text":"<p>There are 4 methods of encrypting object in S3</p> <ul> <li> <p>SSE-S3: encrypt S3 object using keys handled and managed by AWS S3</p> </li> <li> <p>Object is encrypted server side</p> </li> <li>AES-256 encryption type</li> <li> <p>Must set header: <code>\"x-amz-server-side-encryption\":\"AES256</code></p> </li> <li> <p>SSE-KMS: leverage AWS Key Management Service to manage encryption keys</p> </li> <li> <p>Object is encrypted server side</p> </li> <li> <p>Must set header: <code>\"x-amz-server-side-encryption\":\"aws:kms</code></p> </li> <li> <p>SSE-C: when you want to manage your own encryption key</p> </li> <li> <p>Server side encryption using data keys full managed by the customer outside AWS</p> </li> <li>AWS does not store the encryption key</li> <li>HTTPS must be used</li> <li> <p>Encryption key must provided in HTTP headers, for every HTTP request made</p> </li> <li> <p>Client side encryption</p> </li> <li> <p>Client library such as Amazon S3 Encryption Client</p> </li> <li>Client must encrypt data themselves before sending to S3</li> <li>Client must decrypt data themselves when retrieving to S3</li> <li>Customer fully manages they keys and encryption cycle</li> </ul>"},{"location":"devops/aws/s3/#s3-security","title":"S3 Security","text":"<ul> <li> <p>User based</p> </li> <li> <p>IMA policies - which API calls should be allowed for a specific user from IAM console.</p> </li> <li> <p>Resource based</p> </li> <li> <p>Bucket policies: bucket wide rules from S3 console - allow cross account</p> </li> <li>Object access Control List (ACL) - finer grain</li> <li>Bucket access Control List (ACL) - less common</li> </ul> <p>Note: an IAM principal can access an S3 object if the user IMA permissions allow it OR the resource policy ALLOW it; AND there's no DENY</p>"},{"location":"devops/aws/s3/#s3-cors","title":"S3 CORS","text":"<p>CORS means Cross-Origin Resource Sharing. You can find more details about CORS here</p> <p>If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers.</p> <p>How to enable CORS on S3</p> <ol> <li>Open the Amazon S3 console.</li> <li>Select the bucket that contains your resources.</li> <li>Select Permissions.</li> <li>Scroll down to Cross-origin resource sharing (CORS) and select Edit.</li> <li>Insert the CORS configuration in JSON format. You can find an example configuration here</li> <li>Select Save changes to save your configuration.</li> </ol>"},{"location":"devops/aws/s3/#s3-consistency-model","title":"S3 Consistency Model","text":"<p>Amazon S3 delivers strong read-after-write consistency automatically for all applications, for more information you can find here</p>"},{"location":"devops/aws/s3/#s3-replication","title":"S3 Replication","text":"<ul> <li>Must enable versioning in source and destination</li> <li>Cross Region Replication</li> <li>Same Region Replication</li> <li>Buckets can be in different account</li> <li>Copying is asynchronous</li> <li>Must give proper IAM permissions to S3</li> <li>There is no <code>chaining</code> of replication; eg: bucket1 has replication into bucket2, which has replication into bucket3; then if objects is created in bucket1 are not replicated to bucket3</li> </ul>"},{"location":"devops/aws/s3/#s3-pre-signed-urls","title":"S3 Pre-Signed URLs","text":"<p>A presigned URL is a URL that you can provide to your users to grant temporary access to a specific S3 object.</p> <p>Using the URL, a user can either READ the object or WRITE an Object (or update an existing object).</p> <p>The URL contains specific parameters which are set by your application. A pre-signed URL uses three parameters to limit the access to the user:</p> <ul> <li>Bucket: The bucket that the object is in (or will be in)</li> <li>Key: The name of the object</li> <li>Expires: The amount of time that the URL is valid</li> </ul> <p>The URL itself is constructed using various parameters, which are created automatically through the AWS SDK. These include;</p> <ul> <li>X-AMZ-Algorithm</li> <li>X-AMZ-Credential</li> <li>X-AMZ-Date</li> <li>X-AMZ-Expires</li> <li>X-AMZ-Signature</li> <li>X-AMZ-SignedHeaders</li> </ul> <pre><code>https://presignedurldemo.s3.eu-west-2.amazonaws.com/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJJWZ7B6WCRGMKFGQ%2F20180210%2Feu-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20180210T171315Z&amp;X-Amz-Expires=1800&amp;X-Amz-Signature=12b74b0788aa036bc7c3d03b3f20c61f1f91cc9ad8873e3314255dc479a25351&amp;X-Amz-SignedHeaders=host\n</code></pre>"},{"location":"devops/aws/s3/#s3-storage-classes","title":"S3 storage classes","text":"<ol> <li> <p>Amazon S3 Standard for frequent data access: Suitable for a use case where the latency should below. Example: Frequently accessed data will be the data of students\u2019 attendance, which should be retrieved quickly.</p> </li> <li> <p>Amazon S3 Standard for infrequent data access: Can be used where the data is long-lived and less frequently accessed. Example: Students\u2019 academic records will not be needed daily, but if they have any requirement, their details should be retrieved quickly.</p> </li> <li> <p>Amazon Glacier: Can be used where the data has to be archived, and high performance is not required. Example: Ex-student\u2019s old record (like admission fee) will not be needed daily, and even if it is necessary, low latency is not required.</p> </li> <li> <p>One Zone-IA Storage Class: It can be used where the data is infrequently accessed and stored in a single region. Example: Student\u2019s report card is not used daily and stored in a single availability region (i.e., school).</p> </li> <li> <p>Amazon S3 Standard Reduced Redundancy storage: Suitable for a use case where the data is non-critical and reproduced quickly. Example: Books in the library are non-critical data and can be replaced if lost.</p> </li> </ol> <p></p>"},{"location":"devops/aws/secret_manager_vs_parameter_store_vs_kms/","title":"AWS Secrets Manager vs Parameter Store vs KMS","text":"<p>AWS Secrets Manager: Secure storage, management, and automatic rotation of secrets like API keys and database credentials. AWS Parameter Store: Centralized storage for configuration values, including both sensitive and non-sensitive data. AWS KMS (Key Management Service): Manages encryption keys used for securing data but does not store secrets.</p>"},{"location":"devops/aws/secret_manager_vs_parameter_store_vs_kms/#similarities","title":"Similarities","text":"<p>Encryption \u2013 All three support encryption via AWS KMS for secure data storage. IAM Integration \u2013 Fine-grained access control through IAM policies. Auditing &amp; Logging \u2013 Integration with AWS CloudTrail for tracking access and usage.</p>"},{"location":"devops/aws/secret_manager_vs_parameter_store_vs_kms/#key-differences","title":"Key Differences","text":"Feature AWS Secrets Manager AWS Parameter Store AWS KMS Primary Use Case Storing and managing secrets (API keys, database credentials, passwords, etc.) Storing application configurations and secrets Managing encryption keys for securing data Storage Stores secrets with encryption Stores parameters (both encrypted and unencrypted) Does not store secrets, only encryption keys Secret Rotation Built-in support for automatic rotation Manual rotation using AWS Lambda Not applicable Cost $0.40/month per secret + API call costs Free for standard parameters; paid for advanced parameters $1/month per key + API call costs Cross-Account Access Supported Not supported Supported Multi-Region Support Supports replication across AWS regions Not supported Supports multi-region keys Versioning Supports multiple active versions Supports versioning but only one active version Not applicable Limits 500,000 secrets per region 10,000 standard parameters per region No strict limit on key storage"},{"location":"devops/aws/secret_manager_vs_parameter_store_vs_kms/#when-to-choose-what","title":"When to Choose What?","text":"<ul> <li>Use AWS Secrets Manager if you need secure storage, encryption, and automatic secret rotation (e.g., for database credentials, API keys).</li> <li>Use AWS Parameter Store if you need a cost-effective way to store both configuration settings and secrets.</li> <li>Use AWS KMS if you need encryption key management to secure data across AWS services.</li> </ul>"},{"location":"devops/aws/secret_manager_vs_parameter_store_vs_kms/#reference","title":"Reference","text":"<p>AWS \u2014 Difference between Secrets Manager and Parameter Store (Systems Manager)</p>"},{"location":"devops/aws/sqs_sns/","title":"SQS/SNS","text":""},{"location":"devops/aws/sqs_sns/#sqs","title":"SQS","text":"<p>Amazon Simple Queue Service (SQS) is a message queuing service that can help us decouple our application components. It uses queues to store the messages sent by producers and sends them to consumers using a polling model where consumers check the queues for messages sent by the producer whenever they are ready to process them.</p>"},{"location":"devops/aws/sqs_sns/#standard-queue","title":"Standard Queue","text":"<p>A Standard Queue in Amazon SQS provides a flexible messaging service with the following key characteristics:</p> <ul> <li>Unlimited Throughput: There is no limit on the number of messages or the rate at which messages can be sent or received.</li> <li>Message Retention: Default retention period: 4 days.</li> <li>Maximum retention period: 14 days.</li> <li>Low Latency: Standard queues offer low latency for message delivery, typically in the millisecond range.</li> <li>Message Size Limitation: Each message can be up to 256 KB in size.</li> <li>At-Least-Once Delivery: Messages are guaranteed to be delivered at least once, but duplicates may occur.</li> <li>Out-of-Order Delivery: Messages may not always be delivered in the order they were sent.</li> </ul> <p>This queue type is suitable for scenarios where high throughput and availability are critical, and occasional duplicate or out-of-order messages are acceptable.</p>"},{"location":"devops/aws/sqs_sns/#fifo-queue","title":"FIFO queue","text":"<p>FIFO (First-In-First-Out) queues provide the same core features as standard queues, with added benefits:</p> <ul> <li>Strict Message Ordering: Ensures that messages are sent and received in the exact order they are queued.</li> <li>Exactly-Once Processing: Guarantees that each message is processed only once, without duplicates.</li> </ul> <p>Key Characteristics:</p> <ul> <li>Limited Throughput:</li> <li>300 messages/second without batching.</li> <li>3,000 messages/second with batching.</li> <li>Ordering: Messages are processed in the same sequence in which they are sent.</li> <li>Message Uniqueness: FIFO queues ensure that duplicate messages are not delivered within a 5-minute deduplication window.</li> </ul> <p>Deduplication</p> <p>FIFO queues prevent duplicate message delivery within a 5-minute deduplication interval. There are two deduplication methods:</p> <ul> <li>Content-Based Deduplication: Uses a SHA-256 hash of the message body to automatically detect duplicates.</li> <li>Explicit Deduplication: You can explicitly provide a <code>MessageDeduplicationId</code> for each message.</li> </ul> <p>[!NOTE] When using JSON-encoded messages, deduplication may be affected if the ordering of fields in the JSON changes, even though the content remains the same. AWS treats such messages as different because the SHA-256 hash is computed on the raw message body.</p> <p>Message Group ID</p> <p>The Message Group ID tag indicates if a message belongs to a specific group:</p> <ul> <li>Messages within the same group are processed one-by-one, preserving strict order.</li> <li>Messages in different groups can be processed independently and in parallel, potentially out of order.</li> </ul> <p>Using Message Group IDs allows parallel processing of multiple message groups while still ensuring strict ordering within each group. This improves throughput and scalability by enabling more efficient use of resources, as different consumers can handle different message groups concurrently.</p> <p>If no Message Group ID is provided, all messages are handled in the order they are sent, with only one consumer processing them.</p> <p></p>"},{"location":"devops/aws/sqs_sns/#features-of-sqs","title":"Features of SQS","text":""},{"location":"devops/aws/sqs_sns/#polling","title":"Polling","text":"<p>Polling is the process through which messages are received from a SQS queue. In this process, the consumers query the queue, whenever they are available, in order to receive messages from the queue. Amazon SQS supports short and long polling. These models provide different trade-offs in terms of latency, throughput, and cost, allowing us to choose the model that best fits our application\u2019s requirements.</p> <p>Short polling</p> <p>In this method, only a subset of the servers (based on a weighted random distribution) are queried when the SQS queue receives a message retrieval request. SQS sends an empty response immediately, in case it doesn\u2019t find any messages in the queue. Due to this, there is a chance we might not receive messages using short polling. However, if our queue contains less than 1000 messages, we\u2019ll be able to receive these messages with short polling. Also, if we keep sending message retrieval requests to our queue, all available servers are queried to send back a response.</p> <p></p> <p>In the above diagram, only the <code>S1</code>, <code>S3</code>, and <code>S5</code> servers are queried during short polling. Due to this, the message <code>d</code> is not delivered to the consumer.</p> <p>Short polling is useful in applications that have low latency requirements and send frequent polling requests.</p> <p>Long pooling</p> <p>In long polling, all available servers are queried for messages, and a response is sent only after SQS finds at least one message to deliver to the consumer who made the fetch request, and an empty response is sent only in case our request times out.</p> <p>By using long polling, we can reduce the number of empty responses sent by SQS. We can also reduce the false empty responses by querying all servers rather than a subset of servers Since we are charged on every polling request made to the SQS queue, long polling is preferred to reduce cost by reducing the number of empty responses received. However, it is not beneficial to use long polling in case our application expects an immediate response from the SQS queue. In this case, short polling will be prefered.</p>"},{"location":"devops/aws/sqs_sns/#visibility-timeout","title":"Visibility timeout","text":"<p>When the producer sends a message to SQS, it is stored in the queue until consumed by a consumer. When the consumer is ready, it polls SQS for new messages and ultimately receives the message.</p> <p>Once a message is received by a consumer, SQS doesn\u2019t automatically delete the message. Because there\u2019s no way for SQS to guarantee that the message has been received by the consumer. The message might get lost in transit or the consumer can fail while processing the message.</p> <p>So the consumer must delete the message from the queue after receiving and processing it.</p> <p>While a consumer is processing a message in the queue, SQS temporary hides the message from other consumers. This is done by setting a visibility timeout on the message, a period of time during which SQS prevents other consumers from receiving and processing the message.</p> <p>The default visibility timeout for a message is <code>30</code> seconds.</p> <p></p> <p>If the message is not processed within the visibility timeout, it will be processed twice. If visibility timeout is high, and consumer crashes, re-processing will take time; if visibility timeout is low, we may get duplicates</p>"},{"location":"devops/aws/sqs_sns/#dead-letter-queue","title":"Dead Letter Queue","text":"<p>Dead letter queue is a very useful feature in message queuing systems. By using this feature, we can automatically transfer messages, that have exceeded the maximum number of receiving message, to the dead letter queue.</p> <p>We have a configuration <code>Maximum Receives</code> means the maximum number of retries effectively if the number of retries exceed <code>Maximum Receives</code> value then the message will be sent to <code>Dead Letter Queue</code></p> <p></p>"},{"location":"devops/aws/sqs_sns/#delay-queue","title":"Delay queue","text":"<p>Delay queues allow you to postpone the delivery of new messages to consumers for a specified number of seconds. This can be useful when your consumer application requires extra time to process messages or manage workflows more effectively.</p> <p>How Delay Queues Work</p> <p>When a delay queue is configured, any messages sent to the queue remain invisible to consumers for the duration of the specified delay period.</p> <ul> <li>Minimum delay: 0 seconds (no delay).</li> <li>Maximum delay: 15 minutes.</li> </ul> <p>Configuring Delay Queues</p> <p>The delay period for a queue can be set using the <code>Delivery delay</code> parameter, which applies to all messages in the queue.</p> <p>Individual Message Timers</p> <p>If you want to set a delay for individual messages (instead of applying a queue-wide delay), you can use message timers. With message timers, Amazon SQS uses the message\u2019s <code>DelaySeconds</code> value instead of the queue's default delay.</p>"},{"location":"devops/aws/sqs_sns/#sns","title":"SNS","text":"<p>Amazon SNS is a messaging service that delivers messages from a publisher to one or more subscribers using a publish-subscribe (pub/sub) model.</p> <ul> <li>The producer sends messages to an SNS topic</li> <li>Multiple subscribers can listen to messages from the same topic.</li> <li>Maximum subscribers per topic: 12,500,000</li> <li>Topic limit per account: 100,000</li> </ul>"},{"location":"devops/aws/sqs_sns/#standard-topics","title":"Standard topics","text":"<ul> <li>Messages are delivered at least once, but duplicates can occur.</li> <li>There is no guarantee of message order.</li> <li>Duplicate messages may happen if the same message is published multiple times.</li> </ul>"},{"location":"devops/aws/sqs_sns/#fifo-topics","title":"FIFO topics","text":"<ul> <li>Exactly-once delivery: Ensures each message is delivered only once to subscribers.</li> <li>Strict ordering: Messages are delivered in the same order they are received.</li> <li>Uses a deduplication ID to eliminate duplicates within a 5-minute window.</li> </ul>"},{"location":"devops/aws/sqs_sns/#subscriber-endpoints-in-amazon-sns","title":"Subscriber endpoints in Amazon SNS","text":"<p>Amazon SNS supports different types of endpoints for delivering messages, enabling both:</p> <ul> <li>Application-to-Application (A2A) messaging (e.g., AWS Lambda, SQS, HTTP/S endpoints).</li> <li>Application-to-Person (A2P) messaging (e.g., SMS, email, mobile push notifications).</li> </ul> <p>You can apply filter policies to control which messages are sent to each subscriber based on message content.</p> <p></p>"},{"location":"devops/aws/sqs_sns/#message-filtering","title":"Message filtering","text":"<p>By default, Amazon SNS forwards all messages published to a topic, to the topic\u2019s subscribers. However, we can provide filter policies to ensure only a subset of the messages being published are sent to a subscriber.</p> <p></p>"},{"location":"devops/aws/sqs_sns/#fanout-pattern","title":"Fanout Pattern","text":"<p>The fanout pattern sends a single message to multiple subscribers at the same time, enabling asynchronous processing.</p> <p></p>"},{"location":"devops/aws/terraform_api_gateway_to_sqs/","title":"Configuring API Gateway to SQS Queue Using Terraform","text":"<p>First we need to create SQS queue</p> <pre><code>resource \"aws_sqs_queue\" \"queue\" {\n  name                       = \"sqs-queue\"\n  visibility_timeout_seconds = 300 # 5 minutes\n}\n</code></pre> <p>Now we need to define an IAM role so API Gateway has the necessary permissions to SendMessage to our SQS queue.</p> <pre><code>data \"aws_iam_policy_document\" \"api_gateway_assume_role\" {\n  statement {\n    effect = \"Allow\"\n\n    principals {\n      identifiers = [\"apigateway.amazonaws.com\"]\n      type        = \"Service\"\n    }\n\n    actions = [\n      \"sts:AssumeRole\"\n    ]\n  }\n}\n\nresource \"aws_iam_role\" \"api_gateway\" {\n  name               = \"api-gateway-role\"\n  assume_role_policy = data.aws_iam_policy_document.api_gateway_assume_role.json\n}\n\ndata \"aws_iam_policy_document\" \"api_gateway_policy_statement\" {\n  statement {\n    sid    = \"SqsAll\"\n    effect = \"Allow\"\n    actions = [\n      \"sqs:*\",\n    ]\n    resources = [\n      \"arn:aws:sqs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:sqs-queue\",\n    ]\n  }\n}\n\nresource \"aws_iam_policy\" \"api_gateway\" {\n  name   = \"api-gateway-policy\"\n  policy = data.aws_iam_policy_document.api_gateway_policy_statement.json\n}\n\nresource \"aws_iam_role_policy_attachment\" \"api_gateway\" {\n  role       = aws_iam_role.api_gateway.name\n  policy_arn = aws_iam_policy.api_gateway.arn\n}\n</code></pre> <p>Now define an API Gateway REST API. We will host the POST method at the root <code>/</code> of the API.</p> <pre><code>resource \"aws_api_gateway_rest_api\" \"api\" {\n  name        = \"sqs-api\"\n  description = \"POST records to SQS queue\"\n}\n</code></pre> <p>Create an API Gateway POST method for the API</p> <pre><code>resource \"aws_api_gateway_method\" \"post_api\" {\n  rest_api_id      = aws_api_gateway_rest_api.api.id\n  resource_id      = aws_api_gateway_rest_api.api.root_resource_id\n  api_key_required = false\n  http_method      = \"POST\"\n  authorization    = \"NONE\"\n}\n</code></pre> <p>Now we can define the API gateway integration that will forward records into the SQS queue.</p> <pre><code>resource \"aws_api_gateway_integration\" \"api_integration\" {\n  rest_api_id             = aws_api_gateway_rest_api.api.id\n  resource_id             = aws_api_gateway_rest_api.api.root_resource_id\n  http_method             = aws_api_gateway_method.post_api.http_method\n  type                    = \"AWS\"\n  integration_http_method = \"POST\"\n  passthrough_behavior    = \"NEVER\"\n  credentials             = aws_iam_role.api_gateway.arn\n  uri                     = \"arn:aws:apigateway:${data.aws_region.current.name}:sqs:path/${aws_sqs_queue.queue.name}\"\n\n  request_parameters = {\n    \"integration.request.header.Content-Type\" = \"'application/x-www-form-urlencoded'\"\n  }\n\n  request_templates = {\n    \"application/json\" = \"Action=SendMessage&amp;MessageBody=$input.body\"\n  }\n}\n</code></pre> <p>For the FIFO queue, the request template should be</p> <pre><code>resource \"aws_api_gateway_integration\" \"api_integration\" {\n  ...\n  request_templates = {\n    \"application/json\" = &lt;&lt;EOF\n#set($deduplicationId = $input.json('$.email'))\nAction=SendMessage&amp;MessageGroupId=default&amp;MessageDeduplicationId=$deduplicationId&amp;MessageBody=$input.body\nEOF\n  }\n}\n</code></pre> <p>We should define a basic 200 handler for successful requests with a custom response message. Layer on more responses as needed.</p> <p>Notice the response_templates value below, which is what the service will return as a <code>200</code> status code and message. The selection_pattern is nothing more than a regex pattern to match any 2XX status codes that come back from SQS, which will then return a <code>200</code> and the json message <code>{\"message\": \"Success\"}</code> to the client from the API Gateway request.</p> <pre><code>resource \"aws_api_gateway_method_response\" \"post_api_success\" {\n  rest_api_id = aws_api_gateway_rest_api.api.id\n  resource_id = aws_api_gateway_rest_api.api.root_resource_id\n  http_method = aws_api_gateway_method.post_api.http_method\n  status_code = 200\n\n  response_models = {\n    \"application/json\" = \"Empty\"\n  }\n}\n\nresource \"aws_api_gateway_integration_response\" \"post_api_success_response\" {\n  rest_api_id       = aws_api_gateway_rest_api.api.id\n  resource_id       = aws_api_gateway_rest_api.api.root_resource_id\n  http_method       = aws_api_gateway_method.post_api.http_method\n  status_code       = aws_api_gateway_method_response.post_api_success.status_code\n  selection_pattern = \"^2[0-9][0-9]\"\n\n  response_templates = {\n    \"application/json\" = \"{\\\"message\\\": \\\"Success!\\\"}\"\n  }\n\n  depends_on = [\"aws_api_gateway_integration.api\"]\n}\n</code></pre> <p>And lastly, create the API Gateway deployment.</p> <pre><code>resource \"aws_api_gateway_deployment\" \"api\" {\n  rest_api_id = aws_api_gateway_rest_api.api.id\n  stage_name  = \"main\"\n\n  depends_on = [\n    aws_api_gateway_integration.api,\n  ]\n}\n</code></pre>"},{"location":"devops/aws/terraform_deploy_react_app/","title":"Deploying a React Application on AWS Using Terraform","text":""},{"location":"devops/aws/terraform_deploy_react_app/#1-create-an-s3-bucket-for-static-hosting","title":"1. Create an S3 Bucket for Static Hosting","text":"<p>Amazon S3 will be used to store the React build files.</p> <pre><code>resource \"aws_s3_bucket\" \"frontend\" {\n  bucket = 'frontend_bucket_name'\n}\n</code></pre> <p>We need to block public access to ensure security.</p> <pre><code>resource \"aws_s3_bucket_public_access_block\" \"frontend\" {\n  bucket = aws_s3_bucket.frontend.id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n</code></pre> <p>Versioning allows you to recover previous versions of your files in case of accidental deletions or modifications.</p> <pre><code>resource \"aws_s3_bucket_versioning\" \"frontend\" {\n  bucket = aws_s3_bucket.frontend.id\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n</code></pre> <p>Enable static website hosting</p> <pre><code>resource \"aws_s3_bucket_website_configuration\" \"frontend_static_site\" {\n  bucket = aws_s3_bucket.frontend.id\n\n  index_document {\n    suffix = \"index.html\"\n  }\n}\n</code></pre>"},{"location":"devops/aws/terraform_deploy_react_app/#2-create-an-ssl-certificate-with-acm","title":"2. Create an SSL Certificate with ACM","text":"<p>AWS ACM provides an SSL certificate to secure the application over HTTPS.</p> <pre><code>resource \"aws_acm_certificate\" \"frontend\" {\n  domain_name               = local.frontend_domain_name\n  subject_alternative_names = [\"*.${local.frontend_domain_name}\"]\n  validation_method         = \"DNS\"\n\n  lifecycle {\n    create_before_destroy = true\n  }\n\n  # The certificate must be created in the us-east-1 region to be used with CloudFront\n  provider = aws.virginia\n}\n</code></pre>"},{"location":"devops/aws/terraform_deploy_react_app/#3-configure-route-53-for-dns-and-ssl-validation","title":"3. Configure Route 53 for DNS and SSL Validation","text":"<p>Fetch the Hosted Zone</p> <pre><code>data \"aws_route53_zone\" \"base_zone_name\" {\n  name = var.base_zone_name\n}\n</code></pre> <p>Create DNS Records for ACM Validation</p> <pre><code>resource \"aws_route53_record\" \"acm_validation\" {\n  for_each = {\n    for dvo in aws_acm_certificate.frontend.domain_validation_options :\n    dvo.domain_name =&gt; {\n      name   = dvo.resource_record_name\n      record = dvo.resource_record_value\n      type   = dvo.resource_record_type\n    }\n  }\n\n  allow_overwrite = true\n  name            = each.value.name\n  records         = [each.value.record]\n  ttl             = 300\n  type            = each.value.type\n  zone_id         = data.aws_route53_zone.base_zone_name.zone_id\n}\n</code></pre> <p>Validate ACM Certificate</p> <pre><code>resource \"aws_acm_certificate_validation\" \"frontend\" {\n  certificate_arn         = aws_acm_certificate.frontend.arn\n  validation_record_fqdns = [for record in aws_route53_record.acm_validation : record.fqdn]\n\n  depends_on = [\n    aws_acm_certificate.frontend,\n    aws_route53_record.acm_validation,\n  ]\n\n  # The certificate must be created in the us-east-1 region to be used with CloudFront\n  provider = aws.virginia\n}\n</code></pre>"},{"location":"devops/aws/terraform_deploy_react_app/#4-deploy-cloudfront-as-a-cdn","title":"4. Deploy CloudFront as a CDN","text":"<p>Create an Origin Access Identity (OAI)</p> <pre><code>resource \"aws_cloudfront_origin_access_identity\" \"frontend\" {\n  comment = \"Origin access identity for frontend\"\n}\n</code></pre> <p>Fetch AWS-Managed Cache and Origin Request Policies</p> <pre><code># AWS Managed Caching Policy - Found in AWS Management Console at CloudFront &gt; Policies &gt; Cache\ndata \"aws_cloudfront_cache_policy\" \"caching_optimised\" {\n  name = \"Managed-CachingOptimized\"\n}\n\n# AWS Managed Caching Policy - Found in AWS Management Console at CloudFront &gt; Policies &gt; Origin request\ndata \"aws_cloudfront_origin_request_policy\" \"cors_s3_origin\" {\n  name = \"Managed-CORS-S3Origin\"\n}\n</code></pre> <p>Set Security Headers in CloudFront</p> <pre><code>resource \"aws_cloudfront_response_headers_policy\" \"frontend_distribution\" {\n  name = \"frontend-response-headers-policy\"\n\n  security_headers_config {\n    strict_transport_security {\n      access_control_max_age_sec = \"31536000\"\n      include_subdomains         = true\n      override                   = true\n      preload                    = true\n    }\n    content_type_options {\n      override = false\n    }\n    frame_options {\n      frame_option = \"SAMEORIGIN\"\n      override     = false\n    }\n  }\n}\n</code></pre> <p>Enable WAF</p> <pre><code>resource \"aws_wafv2_web_acl\" \"waf_cloudfront\" {\n  name        = \"frontend-enable-aws-managed-rules\"\n  description = \"Enable AWS managed ruleset and attached to Cloudfront distribution\"\n  scope       = \"CLOUDFRONT\"\n\n  default_action {\n    allow {}\n  }\n\n  rule {\n    name     = \"AWS-AWSManagedRulesCommonRuleSet\"\n    priority = 1\n\n    override_action {\n      count {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesCommonRuleSet\"\n        vendor_name = \"AWS\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"AWS-AWSManagedRulesCommonRuleSet\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n  rule {\n    name     = \"AWS-AWSManagedRulesAmazonIpReputationList\"\n    priority = 2\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesAmazonIpReputationList\"\n        vendor_name = \"AWS\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"AWS-AWSManagedRulesAmazonIpReputationList\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n  rule {\n    name     = \"AWS-AWSManagedRulesAnonymousIpList\"\n    priority = 3\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesAnonymousIpList\"\n        vendor_name = \"AWS\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"AWS-AWSManagedRulesAnonymousIpList\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n  rule {\n    name     = \"AWS-AWSManagedRulesKnownBadInputsRuleSet\"\n    priority = 4\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesKnownBadInputsRuleSet\"\n        vendor_name = \"AWS\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"AWS-AWSManagedRulesKnownBadInputsRuleSet\"\n      sampled_requests_enabled   = false\n    }\n  }\n\n  visibility_config {\n    cloudwatch_metrics_enabled = false\n    metric_name                = \"AWS-WAF\"\n    sampled_requests_enabled   = false\n  }\n\n  provider = aws.virginia\n}\n</code></pre> <p>Create CloudFront distribution</p> <pre><code>resource \"aws_cloudfront_distribution\" \"frontend_distribution\" {\n  aliases = [\n    local.frontend_domain_name,\n    \"www.${local.frontend_domain_name}\",\n  ]\n\n  origin {\n    domain_name = aws_s3_bucket.frontend.bucket_regional_domain_name\n    origin_id   = \"s3-frontend\"\n\n    s3_origin_config {\n      origin_access_identity = aws_cloudfront_origin_access_identity.frontend.cloudfront_access_identity_path\n    }\n  }\n\n  enabled             = true\n  is_ipv6_enabled     = true\n  comment             = \"CloudFront distribution for frontend\"\n  default_root_object = \"index.html\"\n\n  default_cache_behavior {\n    cache_policy_id            = data.aws_cloudfront_cache_policy.caching_optimised.id\n    origin_request_policy_id   = data.aws_cloudfront_origin_request_policy.cors_s3_origin.id\n    allowed_methods            = [\"GET\", \"HEAD\"]\n    cached_methods             = [\"GET\", \"HEAD\"]\n    target_origin_id           = \"s3-frontend\"\n    viewer_protocol_policy     = \"redirect-to-https\"\n    response_headers_policy_id = aws_cloudfront_response_headers_policy.frontend_distribution.id\n  }\n\n  price_class = \"PriceClass_100\"\n\n  restrictions {\n    geo_restriction {\n      restriction_type = \"none\"\n      locations        = []\n    }\n  }\n\n  web_acl_id = aws_wafv2_web_acl.waf_cloudfront.arn\n\n  viewer_certificate {\n    acm_certificate_arn            = aws_acm_certificate.frontend.arn\n    cloudfront_default_certificate = false\n    minimum_protocol_version       = \"TLSv1.2_2021\"\n    ssl_support_method             = \"sni-only\"\n  }\n}\n</code></pre>"},{"location":"devops/aws/terraform_deploy_react_app/#5-configure-route-53-for-cloudfront","title":"5. Configure Route 53 for CloudFront","text":"<pre><code>resource \"aws_route53_record\" \"frontend_record_ipv4\" {\n  zone_id = data.aws_route53_zone.base_zone_name.zone_id\n  name    = local.frontend_domain_name\n  type    = \"A\"\n\n  alias {\n    name                   = aws_cloudfront_distribution.frontend_distribution.domain_name\n    zone_id                = aws_cloudfront_distribution.frontend_distribution.hosted_zone_id\n    evaluate_target_health = false\n  }\n}\n\nresource \"aws_route53_record\" \"frontend_record_ipv6\" {\n  zone_id = data.aws_route53_zone.base_zone_name.zone_id\n  name    = local.frontend_domain_name\n  type    = \"AAAA\"\n\n  alias {\n    name                   = aws_cloudfront_distribution.frontend_distribution.domain_name\n    zone_id                = aws_cloudfront_distribution.frontend_distribution.hosted_zone_id\n    evaluate_target_health = false\n  }\n}\n\nresource \"aws_route53_record\" \"frontend_record_cname\" {\n  zone_id = data.aws_route53_zone.base_zone_name.zone_id\n  name    = \"www\"\n  type    = \"CNAME\"\n  ttl     = \"300\"\n  records = [local.frontend_domain_name]\n}\n</code></pre>"},{"location":"devops/aws/terraform_deploy_react_app/#6-update-s3-bucket-policy-to-allow-cloudfront-access","title":"6. Update S3 Bucket Policy to Allow CloudFront Access","text":"<pre><code>data \"aws_iam_policy_document\" \"frontend_policy_document\" {\n  statement {\n    effect = \"Allow\"\n    principals {\n      type        = \"AWS\"\n      identifiers = [aws_cloudfront_origin_access_identity.frontend.iam_arn]\n    }\n    actions = [\n      \"s3:GetObject\",\n    ]\n    resources = [\n      \"${aws_s3_bucket.frontend.arn}/*\"\n    ]\n  }\n}\n\nresource \"aws_s3_bucket_policy\" \"frontend_policy\" {\n  bucket = aws_s3_bucket.frontend.id\n  policy = data.aws_iam_policy_document.frontend_policy_document.json\n}\n</code></pre>"},{"location":"devops/aws/vpc/","title":"Virtual Private Cloud (VPC)","text":"<p>A VPC is a logically isolated section of a data center where you have complete control over your virtual networking environment, including the selection of IP address ranges, etc.</p>"},{"location":"devops/aws/vpc/#key-components-of-vpc","title":"Key components of VPC","text":"<ul> <li>Subnets: Within a VPC, subnets are like smaller neighborhoods, dividing the overall IP address range into manageable segments. Each subnet is associated with a specific Availability Zone (AZ), which ensures redundancy and fault tolerance. For example, you might have one subnet for web servers and another for databases, each located in different AZs, to mitigate the impact of failures.</li> <li>Route tables: Think of route tables as traffic directors within the VPC. They determine how network traffic flows between subnets and external networks like the internet. For instance, a route table might specify that traffic destined for the internet should be directed through the internet gateway, while internal traffic stays within the VPC.</li> <li>Internet gateways: Internet gateways are the connection points between the VPC and the wider internet. They allow instances within the VPC to communicate with resources outside the VPC, such as websites, APIs, and external databases.</li> <li>Elastic IP addresses (EIP): EIPs are like reserved parking spots for instances within the VPC. They provide a static IP address that can be associated with an instance, ensuring that the instance maintains the same public-facing IP address even if it\u2019s stopped and restarted.</li> </ul>"},{"location":"devops/aws/vpc/#default-vpc","title":"Default VPC","text":"<p>By default, when you create an AWS account, AWS creates a default VPC for you in each AWS Region. The default VPC comes pre-configured with a set of default subnets, route tables, internet gateways, and network access control lists (ACLs). This simplifies the process of launching instances and other resources without needing to create a custom VPC. The default VPC comes pre-configured with the following settings:</p> <ul> <li>VPC configuration: A VPC with a size <code>/16</code> IPv4 CIDR block (<code>172.31.0.0/16</code>), providing up to <code>65,536</code> private IPv4 addresses.</li> <li>Default subnets: A size <code>/20</code> default subnet in each Availability Zone, providing up to <code>4,096</code> addresses per subnet. Some addresses are reserved for AWS use.</li> <li>Internet gateway: An internet gateway connected to the default VPC, allowing instances to communicate with the internet.</li> <li>Route configuration: A route in the main route table that points all traffic (<code>0.0.0.0/0</code>) to the internet gateway.</li> <li>Security group and network ACL: A default security group and network access control list (ACL) associated with the default VPC.</li> <li>DHCP options set: The default DHCP options set for your AWS account is associated with your default VPC.</li> </ul>"},{"location":"devops/aws/vpc/#subnets","title":"Subnets","text":"<p>Subnets are vital components within VPC, allowing us to deploy our resources carefully and manage the network access effectively.</p>"},{"location":"devops/aws/vpc/#types-of-subnets","title":"Types of Subnets","text":"<ul> <li>Public subnet: This subnet is connected to an internet gateway, allowing resources within it to access the public internet.</li> <li>Private subnet: The private subnet doesn\u2019t have a direct route to the public internet. It requires a NAT device to allow its resources access to the public internet.</li> <li>VPN-only subnet: This subnet routes to a Site-to-Site VPN using a virtual private gateway and does not have a direct route to the public internet.</li> <li>Isolated subnet: Subnets of this type have no routes leading to destinations outside their VPC. Resources within an isolated subnet can solely communicate with other resources within the same VPC.</li> </ul>"},{"location":"devops/aws/vpc/#classless-inter-domain-routing-cidr","title":"Classless Inter-Domain Routing (CIDR)","text":"<p>An IP address has two parts:</p> <ul> <li>The network address is a series of numerical digits pointing to the network's unique identifier</li> <li>The host address is a series of numbers indicating the host or individual device identifier on the network</li> </ul> <p>Classful addresses</p> <ul> <li>Class A: 8 network prefix bits (supported 16,777,214 hosts)</li> <li>Class B: 16 network prefix bits (supported 65,534 hosts)</li> <li>Class C: 24 network prefix bits (supported 254 hosts)</li> </ul> <p>The classful arrangement was inefficient when allocating IP addresses and led to a waste of IP address spaces. For example, an organization with 300 devices couldn\u2019t have used a Class C IP address, which only permitted 254 devices. So, the organization would\u2019ve been forced to apply for a Class B IP address, which provided 65,534 unique host addresses. However, only 300 devices would\u2019ve been connected, which would\u2019ve left 65,234 unused IP address spaces.</p> <p>Classless Inter-Domain Routing (CIDR) is an IP address allocation method that improves data routing efficiency on the internet. CIDR addresses use variable length subnet masking (VLSM) to alter the ratio between the network and host address bits in an IP address.</p> <p>A VLSM sequence allows network administrators to break down an IP address space into subnets of various sizes. Each subnet can have a flexible host count and a limited number of IP addresses. A CIDR IP address appends a suffix value stating the number of network address prefix bits to a normal IP address.</p> <p>Let\u2019s look at a few examples to further cement our understanding:</p> <ul> <li>In <code>x.x.x.x/24</code>, <code>/24</code> represents the number of network bits, which means the given address block contains 2<sup>8</sup> or 256 hosts or IP addresses.</li> <li>Similarly, in <code>x.x.x.x/20</code>, <code>/20</code> represents the number of network bits, which means the given address block contains 2 <sup>12</sup> or 4096 hosts or IP addresses</li> <li>In the edge case of <code>/32</code> network bits, there would only be <code>1</code> host IP address. While <code>/0</code> represents all the 2<sup>32</sup> IP addresses in IPv4.</li> </ul>"},{"location":"devops/aws/vpc/#route-table","title":"Route Table","text":"<p>The Route Table, also referred to as the routing table, is responsible for providing routing instructions within a network and is associated with specific subnets. By default, every VPC is created with a main route table, and each subnet in the VPC is automatically associated with this main route table. The main route table cannot be deleted. However, we can modify its routes. We do have the option of creating customized route tables for our subnet.</p> <p>For instance, in the scenario where a Virtual Private Cloud (VPC) is established with the network layer <code>10.10.0.0/16</code>, along with two subnets, <code>10.10.1.0/24</code> and <code>10.10.2.0/24</code>, each default subnet will be allocated a default route table.</p> <p>Inside the route table, there will exist a route entry with the following details:</p> <ul> <li>Destination: 10.10.0.0/16</li> <li>Target: local</li> </ul> <p>This particular route entry signifies that resources created within the same VPC can communicate with each other.</p> <p></p>"},{"location":"devops/aws/vpc/#internet-gateways","title":"Internet gateways","text":"<p>An internet gateway is a component that facilitates communication between instances within a VPC and the internet. It essentially acts as a gateway for internet traffic to and from instances in the VPC. When attached to a VPC, the internet gateway enables instances to connect with resources outside the VPC, such as accessing the internet or communicating with other AWS services outside the VPC.</p> <p>An internet gateway scales horizontally, is highly available, and doesn\u2019t cause bandwidth constraints for our VPC\u2019s traffic. It facilitates outbound and inbound traffic flows for instances within your VPC.</p> <ul> <li>Outbound traffic: The internet gateway enables instances within our VPC to communicate with resources and services outside the VPC boundaries, such as accessing websites, downloading updates, or interacting with external APIs. It serves as the conduit through which traffic from our VPC is routed to the internet, allowing the instances to interact with the global network.</li> <li>Inbound traffic: With the appropriate security configurations, the internet gateway permits incoming traffic to reach your instances from the internet. This functionality is crucial for hosting services, websites, or applications that need to be publicly accessible. By properly configuring security groups and network access control lists, you can control and manage the types of inbound traffic allowed to reach your instances, enhancing security posture.</li> </ul> <p>[!NOTE] Internet gateways don\u2019t give access to the internet by themselves. We must create routes in our route table to allow the subnets in our VPCs to communicate with the internet.</p>"},{"location":"devops/aws/vpc/#nat-gateways","title":"NAT gateways","text":"<p>A private subnet is any subnet that doesn\u2019t have a route to an Internet Gateway (that is, any subnet that is not public). As a result, they have no means to communicate with the Internet. This is where NAT Gateways and Instances come in to play, they allow a private subnet access to the Internet.</p> <p>A NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances.</p> <p>To use the NAT Gateway, assign a route in the private subnet, in lieu of a route to an Internet Gateway. Traffic destined for the Internet will flow from the private subnet to the NAT Gateway in the public subnet, and then out to the Internet through the Internet Gateway.</p> <p>When you create a NAT gateway, you specify one of the following connectivity types:</p> <ul> <li>Public connectivity type: This is the default connectivity type used by NAT gateways, which enables instances in private subnets to access the internet through a public NAT gateway. However, these instances cannot receive unsolicited inbound connections from the internet. An elastic IP address must be associated with the NAT gateway during its creation in the public connectivity type.</li> <li>Private connectivity type: It allows instances in private subnets to connect to other VPCs or on-premises networks through a private NAT gateway. An elastic IP address cannot be associated with the private NAT gateway.</li> </ul>"},{"location":"devops/aws/vpc/#putting-it-together","title":"Putting it together","text":"<p>For a public subnet to have Internet access, inbound and outbound, an account needs:</p> <ul> <li>Internet Gateway attached to a VPC</li> <li>Route to the Internet Gateway in the attached route table</li> <li>Instances have public IP addresses (auto-assigned or attached Elastic IP address)</li> <li>Appropriate security group and NACL allowances</li> </ul> <p>For a private subnet to have Internet access, the following will provide outbound Internet access but not inbound:</p> <ul> <li>Internet Gateway attached to a VPC</li> <li>NAT Gateway or Instance in a public subnet in the same VPC</li> <li>Route to the NAT Gateway or Instance in the private subnet\u2019s attached route table</li> <li>Appropriate security group and NACL allowances</li> </ul> <p></p>"},{"location":"devops/aws/vpc/#vpc-peering","title":"VPC peering","text":"<p>A VPC peering connection is a networking connection between two Virtual Private Clouds (VPCs) that enables resources in these VPCs to communicate with each other as if they were part of the same VPC. This allows for private communication without exposing resources to the public internet.</p> <p>Key Considerations</p> <ul> <li>Non-overlapping CIDR Blocks: When setting up a VPC peering connection, the CIDR blocks of the two VPCs must not overlap. This ensures proper routing and communication between VPCs.</li> <li>No Transitive Peering: VPC peering is non-transitive, meaning that if VPC 1 is peered with VPC 2, and VPC 2 is peered with VPC 3, traffic cannot flow from VPC 1 to VPC 3 automatically. To enable communication between VPC 1 and VPC 3, you would need to set up a direct peering connection between them.</li> </ul> <p>Additionally, when creating a VPC peering connection, it's essential to update the route tables in each VPC to ensure that traffic can flow between the VPCs.</p> <p></p>"},{"location":"devops/aws/vpc/#vpc-peering-in-aws-regions","title":"VPC peering in AWS Regions","text":"<p>VPC peering allows to connect VPCs in the same and different AWS Regions.</p> <ul> <li>Intra-region connection: When configuring a VPC peering connection between VPCs within the same region, we can use security groups to allow traffic to and from the security group of the peering VPCs.</li> <li>Inter-region connection: If we connect the VPCs from different regions, we must use the CIDR address of the connecting VPC as the source or destination in the security group to control traffic.</li> </ul>"},{"location":"devops/aws/vpc/#advantages-of-vpc-peering","title":"Advantages of VPC peering","text":"<ul> <li>Low cost since you need to pay only for data transfer.</li> <li>No bandwidth limit.</li> </ul>"},{"location":"devops/aws/vpc/#disadvantages-of-vpc-peering","title":"Disadvantages of VPC peering","text":"<ul> <li>Complex at scale. Each new VPC increases the complexity of the network. Harder to maintain route tables compared to TGW.</li> <li>No transit routing.</li> <li>Maximum 125 peering connections per VPC.</li> </ul>"},{"location":"devops/aws/vpc/#aws-transit-gateway","title":"AWS Transit Gateway","text":"<p>The AWS Transit Gateway (TGW) is a regional network hub that simplifies the management of network traffic between thousands of VPCs and on-premises resources. It acts as a central hub to route traffic across VPCs and on-premises networks, offering a scalable and efficient solution for large networks.</p> <p></p>"},{"location":"devops/aws/vpc/#advantages-of-transit-gateway","title":"Advantages of Transit Gateway","text":"<ul> <li>Simplified VPC Management: With TGW, each spoke VPC only needs to connect to the Transit Gateway to gain access to other connected VPCs, simplifying the network architecture.</li> <li>Scalability: TGW supports a significantly higher number of VPCs than VPC peering, making it ideal for larger, more complex network environments.</li> <li>Fine-Grained Routing: TGW allows the use of Route Tables per attachment, enabling detailed and precise routing control between connected VPCs and on-premises networks.</li> </ul>"},{"location":"devops/aws/vpc/#disadvantages-of-transit-gateway","title":"Disadvantages of Transit Gateway","text":"<ul> <li>Additional hop introduces some latency.</li> <li>Extra cost of hourly charge per attachment in addition to data fees.</li> </ul>"},{"location":"devops/aws/vpc/#vpc-endpoint","title":"VPC Endpoint","text":"<p>A Virtual Private Cloud (VPC) is an isolated network segment. Without an internet gateway or NAT gateway, the VPC remains completely isolated. AWS services that are not deployed directly within a VPC, such as S3, Systems Manager, SQS, Secrets Manager, etc., are accessible via a public address. This means that a VPC with EC2 instances in a private subnet must communicate with these AWS services through a NAT gateway. If no NAT gateway is available, communication with AWS services from the private subnet is not possible.</p> <p></p> <p>To resolve this issue, VPC Endpoints can be used. They enable communication between a VPC and AWS services without requiring an internet or NAT gateway. Moreover, since the traffic stays within the AWS network, this approach enhances both security and efficiency.</p> <p>There are two types of VPC endpoints:</p>"},{"location":"devops/aws/vpc/#vpc-interface-endpoints","title":"VPC Interface Endpoints","text":"<p>A VPC Interface Endpoint is a type of VPC Endpoint that enables private, secure communication between your VPC and supported AWS services using private IPs. Interface Endpoints create elastic network interfaces (ENIs) within your VPC that are assigned private IP addresses. These ENIs establish a direct, secure connection to AWS services, eliminating the need for an internet gateway, NAT gateway, or VPN connection.</p> <p>A VPC interface endpoint enables you to connect privately to services over AWS PrivateLink. These services can include:</p> <ul> <li>AWS managed services (e.g., API Gateway, CloudWatch, SQS, SNS, etc.).</li> <li>Provider services hosted by another AWS account as a VPC endpoint service.</li> <li>Third-party services from AWS Marketplace partners.</li> </ul> <p></p>"},{"location":"devops/aws/vpc/#vpc-gateway-endpoint","title":"VPC Gateway Endpoint","text":"<p>A VPC gateway endpoint enables your instances in a VPC to connect to Amazon S3 and DynamoDB without requiring public IP addresses.</p> <p></p>"},{"location":"devops/docker/docker/","title":"Docker Overview","text":"<p>Docker provides the ability to package and run an application in a loosely isolated environment called a container. Containers are lightweight because they don\u2019t need the extra load of a hypervisor, but run directly within the host machine\u2019s kernel.</p>"},{"location":"devops/docker/docker/#dockercontainerization-vs-vmwarevirtualization","title":"Docker(Containerization) vs VMware(Virtualization)","text":""},{"location":"devops/docker/docker/#virtual-machine","title":"Virtual machine","text":"<p>Virtual machines are heavy software packages that provide complete emulation of low level hardware devices like CPU, Disk and Networking devices.</p> <p>Pros</p> <ul> <li>Full isolation security: Virtual machines run in isolation as a fully standalone system. This means that virtual machines are immune to any exploits or interference from other virtual machines on a shared host.</li> <li>Interactive development: Virtual machines are dynamic and can be interactively developed. Once the basic hardware definition is specified for a virtual machine the virtual machine can then be treated as a bare bones computer. Software can manually be installed to the virtual machine and the virtual machine can be snapshotted to capture the current configuration state.</li> </ul> <p>Cons</p> <ul> <li>Iteration speed: Virtual machines are time consuming to build and regenerate because they encompass a full stack system.</li> <li>Storage size cost: Virtual machines can take up a lot of storage space.</li> </ul> <p>Containerization is the packaging together of software code with all its necessary components like libraries, frameworks, and other dependencies so that they are isolated in their own \"container\"</p>"},{"location":"devops/docker/docker/#container","title":"Container","text":"<p>Containers are lightweight software packages that contain all the dependencies required to execute the contained software application.</p> <p>Pros</p> <ul> <li>Iteration speed: Because containers are lightweight and only include high level software, they are very fast to modify and iterate on.</li> <li>Robust ecosystem: We can download image from image public repository to run the container application, saving time for development teams</li> </ul> <p>Cons</p> <p>Shared host exploits: Containers all share the same underlying hardware system below the operating system layer, it is possible that an exploit in one container could break out of the container and affect the shared hardware.</p>"},{"location":"devops/docker/docker/#docker-architecture","title":"Docker architecture","text":"<p>Docker follows Client-Server architecture, which includes the three main components that are Docker Client, Docker Host, and Docker Registry.</p>"},{"location":"devops/docker/docker/#docker-registry","title":"Docker Registry","text":"<p>Docker Registry manages and stores the Docker images.</p> <p>There are two types of registries in the Docker:</p> <ul> <li>Pubic Registry - Public Registry is also called as Docker hub.</li> <li>Private Registry - It is used to share images within the enterprise.</li> </ul>"},{"location":"devops/docker/docker/#docker-client","title":"Docker client","text":"<p>The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as <code>docker run</code>, the client sends these commands to <code>dockerd</code>, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.</p>"},{"location":"devops/docker/docker/#docker-host","title":"Docker Host","text":"<p>Docker Host is used to provide an environment to execute and run applications. It contains the docker daemon, images, containers, networks, and storage.</p>"},{"location":"devops/docker/docker/#docker-daemon","title":"Docker daemon","text":"<p>The Docker daemon (<code>dockerd</code>) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.</p>"},{"location":"devops/docker/docker/#docker-objects","title":"Docker objects","text":"<p>Docker Images</p> <p>An image is a read-only template with instructions for creating a Docker container.</p> <p>Docker Containers</p> <p>A container is a runnable instance of an image.</p> <p>Docker Networking</p> <p>Docker Networking allows isolated packages communicate to each another.</p> <p>Docker Storage</p> <p>Docker Storage is used to store data on the container</p>"},{"location":"devops/docker/docker_cheat_sheet/","title":"Docker Cheat Sheet","text":""},{"location":"devops/docker/docker_cheat_sheet/#images","title":"Images","text":"<p>Build image</p> <pre><code>docker build .\ndocker build -t {tag_name} .\ndocker build -t {tag_name} Dockerfile\n</code></pre> <p>List all images</p> <pre><code>docker images\n</code></pre> <p>Remove image</p> <pre><code>docker rmi {image_name:tag_name}\n</code></pre> <p>Download image</p> <pre><code>docker pull {image_name}\n</code></pre>"},{"location":"devops/docker/docker_cheat_sheet/#containers","title":"Containers","text":"<p>List containers</p> <pre><code>docker ps # show running containers\ndocker ps -a # show all container\n</code></pre> <p>Get logs</p> <pre><code>docker logs [-f/--follow] {container}\n</code></pre> <p>Delete container</p> <pre><code>docker rm {container}\n</code></pre> <p>Run container</p> <pre><code>docker run {image}\ndocker run -ti ubuntu:latest # -ti = terminal keyboardInteractive\ndocker run -d -ti ubuntu bash # -d : (detach) run docker process in background\ndocker run -e \"DB_HOST=db\" -e \"DB_PORT=3306\" -e \"DB_USER=wikijs\" -e \"DB_PASS=wikijsrocks\" image_name\ndocker run --name {container_name} -p {host_port}:{container_port} -v {/host_path}:{/container_path} -it {image_name} /bin/bash\n</code></pre> <p>docker run command = docker create command + docker start command</p> <pre><code>docker create ubuntu:latest\ndocker start {container}\n</code></pre> <p>Inspect container</p> <pre><code>docker inspect {container}\n</code></pre> <p>Stop container</p> <pre><code>docker stop {container}\n</code></pre> <p>Convert detach mode to attach mode</p> <pre><code>docker attach {container}\n</code></pre> <p>Remove stopped contains</p> <pre><code>docker system prune # Remove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes.\n</code></pre> <p>Executing Commands</p> <pre><code>docker exec -it {container} sh\n</code></pre> <p>Copy file from host to container</p> <pre><code>docker cp foo.txt mycontainer:/foo.txt\n</code></pre> <p>Copy file from container to host</p> <pre><code>docker cp mycontainer:/foo.txt foo.txt\n</code></pre> <p>For more information, please refer here</p>"},{"location":"devops/docker/docker_images/","title":"Docker Images","text":""},{"location":"devops/docker/docker_images/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a text configuration file written using a special syntax. It describes step-by-step instructions of all the commands you need to run to assemble a Docker Image.</p> <p>It contains 3 main parts:</p> <ol> <li>Specify a base image</li> <li>Run some commands to install additional programs</li> <li>Specify a command to run on container startup</li> </ol> <p>Example</p> <pre><code>FROM ubuntu:18.04\nCOPY . /app\nRUN make /app\nCMD python /app/app.py\n</code></pre>"},{"location":"devops/docker/docker_images/#dockerfile-commands","title":"Dockerfile commands","text":"<p>FROM</p> <pre><code>FROM &lt;image&gt; [AS &lt;name&gt;]\n</code></pre> <p>FROM is used to define the base image to start the build process. Every Dockerfile must start with the FROM instruction.</p> <p>LABEL</p> <pre><code>LABEL &lt;key&gt;=\"&lt;value&gt;\"\n</code></pre> <p>Labels are used in Dockerfile to help organize your Docker Images. A label is a key-value pair, stored as a string. You can specify multiple labels for an object, but each key must be unique within an object.</p> <pre><code>LABEL version=\"1.0\"\n</code></pre> <p>Using command <code>docker inspect [OPTIONS] NAME|ID [NAME|ID...]</code> to inspect image's metadata.</p> <p>ENV</p> <pre><code>ENV &lt;key&gt;=\"&lt;value&gt;\"\n</code></pre> <p>This command used to set the environment variables that is required to run the project.</p> <pre><code>ENV HTTP_PORT=\"9000\"\n</code></pre> <p>WORKDIR</p> <pre><code>WORKDIR /path/to/workdir\n</code></pre> <p>WORKDIR tells Docker that the rest of the commands will be run in the context of the <code>/path/to/workdir</code> folder inside the image.</p> <p>RUN</p> <p>RUN has 2 forms:</p> <pre><code>RUN &lt;command&gt;\nRUN [\"executable\", \"param1\", \"param2\"]\n</code></pre> <p>The RUN instruction will execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.</p> <p>Example</p> <pre><code>RUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME'\n</code></pre> <p>COPY</p> <p>COPY has two forms:</p> <pre><code>COPY &lt;src&gt;... &lt;dest&gt;\nCOPY [\"&lt;src&gt;\",... \"&lt;dest&gt;\"] (this form is required for paths containing whitespace)\n</code></pre> <p>The COPY command is used to copy one or many local files or folders from source and adds them to the filesystem of the containers at the destination path.</p> <p>It builds up the image in layers, starting with the parent image, defined using FROM.The Docker instruction WORKDIR defines a working directory for the COPY instructions that follow it.</p> <p>The  is an absolute path, or a path relative to <code>WORKDIR</code>, into which the source will be copied inside the destination container. <pre><code>COPY test relativeDir/   # adds \"test\" to `WORKDIR`/relativeDir/\nCOPY test /absoluteDir/  # adds \"test\" to /absoluteDir/\n</code></pre> <p>ADD</p> <p>ADD has two forms:</p> <pre><code>ADD &lt;src&gt;... &lt;dest&gt;\nADD [\"&lt;src&gt;\",... \"&lt;dest&gt;\"]\n</code></pre> <p>The ADD command is used to add one or many local files or folders from the source and adds them to the filesystem of the containers at the destination path.</p> <p>It is Similar to COPY command but it has some additional features:</p> <ul> <li>If the source is a local tar archive in a recognized compression format, then it is automatically unpacked as a directory into the Docker image.</li> <li>If the source is a URL, then it will download and copy the file into the destination within the Docker image. However, Docker discourages using ADD for this purpose.</li> </ul> <pre><code>ADD rootfs.tar.xz /\nADD http://example.com/big.tar.xz /usr/src/things/\n</code></pre> <p>EXPOSE</p> <pre><code>EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...]\n</code></pre> <p>The EXPOSE command informs the Docker that the container listens on the specified network ports at runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if the protocol is not specified.</p> <p>EXPOSE will not allow communication via the defined ports to containers outside of the same network or to the host machine.</p> <p>VOLUME</p> <pre><code>VOLUME [\"/data\"]\n</code></pre> <p>The VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers.</p> <p>USER</p> <pre><code>USER user\n</code></pre> <p>By default, a Docker Container runs as a Root user. You can change or switch to a different user inside a Docker Container using the USER Instruction. For this, you first need to create a user and a group inside the Container:</p> <pre><code>RUN groupadd -r postgres &amp;&amp; useradd --no-log-init -r -g postgres postgres\n</code></pre> <p>ENTRYPOINT</p> <p>ENTRYPOINT has two forms:</p> <pre><code>ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec form, preferred)\nENTRYPOINT command param1 param2 (shell form)\n</code></pre> <p>An ENTRYPOINT allows you to configure a container that will run as an executable. ENTRYPOINT sets the command and parameters that will be executed first when a container is run. Any command-line arguments passed to <code>docker run &lt;image&gt;</code> will be appended to the ENTRYPOINT command.</p> <p>You can override ENTRYPOINT instructions using the <code>docker run --entrypoint</code></p> <pre><code>ENTRYPOINT [ \"sh\", \"-c\", \"echo $HOME\" ]\n</code></pre> <p>CMD</p> <p>The CMD instruction has three forms:</p> <pre><code>CMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form)\nCMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)\nCMD command param1 param2 (shell form)\n</code></pre> <p>The main purpose of a CMD is to provide defaults when executing a container. These will be executed after the entry point.</p> <p>Any command-line arguments passed to <code>docker run &lt;image&gt;</code> will override all elements specified using CMD.</p> <p>Example</p> <pre><code>CMD [\"executable\",\"param1\",\"param2\"]\n</code></pre> <p>If they omit the executable, you must specify an ENTRYPOINT instruction as well.</p> <pre><code>CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT)\n</code></pre> <p>NOTE: There can only be one CMD instruction in a Dockerfile. If you want to list more than one CMD, then only the last CMD will take effect.</p> <p>Understand how CMD and ENTRYPOINT interact</p> <ol> <li>Dockerfile should specify at least one of CMD or ENTRYPOINT commands.</li> <li>ENTRYPOINT should be defined when using the container as an executable.</li> <li>CMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container.</li> <li>CMD will be overridden when running the container with alternative arguments.</li> </ol> <p>Build image</p> <pre><code>docker build .\ndocker build -t {tag_name} .\ndocker build -t {tag_name} Dockerfile\n</code></pre> <p>List image</p> <pre><code>docker images\n</code></pre> <p>Remove image</p> <pre><code>docker rmi {image_name:tag_name}\n</code></pre>"},{"location":"devops/docker/docker_images/#docker-layer","title":"Docker Layer","text":"<p>A Docker image consists of several layers. Each layer corresponds to certain instructions in your Dockerfile. The following instructions create a layer: <code>RUN</code>, <code>COPY</code>, <code>ADD</code>. The other instructions will create intermediate layers and do not influence the size of your image.</p> <p></p>"},{"location":"devops/docker/docker_network/","title":"Docker Network","text":"<p>Docker networking enables a user to link a Docker container to as many networks as he/she requires. Docker Networks are used to provide complete isolation for Docker containers.</p> <p>At the highest level, Docker networking comprises three major components:</p> <ul> <li>The Container Network Model (CNM)</li> <li>libnetwork</li> <li>Drivers</li> </ul>"},{"location":"devops/docker/docker_network/#container-network-model","title":"Container Network Model","text":"<p>CNM has 3 main components: Sandbox, Endpoint, and Network.</p> <p>Network Sandbox is an isolated network stack. It includes; Ethernet interfaces, ports, routing tables, and DNS config.</p> <p>Endpoint is virtual network interfaces (E.g. veth). Like normal network interfaces, they\u2019re responsible for making connections. In the case of the CNM, it\u2019s the job of the endpoint to connect a sandbox to a network.</p> <p>Network is a software implementation of a switch (802.1d bridge). As such, they group together and isolate a collection of endpoints that need to communicate.</p>"},{"location":"devops/docker/docker_network/#libnetwork","title":"Libnetwork","text":"<p>The CNM is the design doc, and libnetwork is the canonical implementation. It\u2019s open-source, written in Go, cross-platform (Linux and Windows), and used by Docker.</p>"},{"location":"devops/docker/docker_network/#network-drivers","title":"Network drivers","text":""},{"location":"devops/docker/docker_network/#none-driver","title":"None Driver","text":"<p>The <code>none</code> driver simply disables networking for a container, making it isolated from other containers.</p> <p></p> <p>How to use none network</p> <pre><code>docker run -it --name app --network none alpine\n</code></pre> <p>Use case:</p> <ul> <li>To run network-isolation application that only perform file operations</li> <li>To run a one-off command which requires network-isolation</li> </ul>"},{"location":"devops/docker/docker_network/#host-driver","title":"Host Driver","text":"<p>When using the host driver, the container shares the network stack of the Docker host - appearing as if the container is the host itself, from a networking perspective.</p> <p></p> <p>How to use host network</p> <pre><code>docker run -d --name app --network host nginx:alpine\n</code></pre> <p>Use case:</p> <ul> <li>When the highest network performance is required</li> <li>When a single container needs to handle a large number of pods</li> <li>When network-isolation is not required</li> </ul> <p>Limitation</p> <ul> <li>Lack of network isolation</li> <li>Port conflict</li> <li>Only works on Linux machines</li> </ul>"},{"location":"devops/docker/docker_network/#bridge-driver","title":"Bridge driver","text":"<p>The bridge driver creates an internal network within a single Docker host. Containers placed within this network can communicate with each other but are isolated from containers, not on the internal network</p> <p></p> <p>Default bridge network</p> <p>How to use default bridge network</p> <pre><code>docker run -d --name app nginx:alpine\n</code></pre> <p>Note: The default network of the container is the <code>default bridge network</code></p> <p>Retrieve the ip address of the container</p> <pre><code>docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container_name_or_id\n</code></pre> <p>User-defined bridge network</p> <pre><code>docker network create my-net # You can specify the subnet, the IP address range, the gateway, and other options\ndocker network rm my-net\ndocker create --name my-nginx --network my-net --publish 8080:80 nginx:latest\ndocker network connect my-net my-nginx # connect a running container to an existing user-defined bridge\ndocker network disconnect my-net my-nginx # disconnect a running container from a user-defined bridge\n</code></pre> <p>User-defined bridges vs default bridges</p> <ul> <li>User-defined bridges provides automatic DNS resolution between containers: Containers on the default bridge network can only access each other by IP addresses. On a user-defined bridge network, containers can resolve by name or alias.</li> <li>User-defined bridges provide better isolation: All containers without a specified -network is attached to the default bridge network. This can be a risk, as unrelated stacks/services/containers can then communicate. Using a user-defined network provides a scoped network in which only containers attached to that network can communicate.</li> <li>Containers can be attached or detached from User-defined network on the fly: You can connect or disconnect them from user-defined networks on the fly during a container's lifetime. To remove a container from the default bridge network, you need to stop it and recreate it with different network options.</li> <li>Each User-defined network creates a configurable bridge: If your containers use the default bridge network, you can configure it, but all the containers use the same settings, such as MTU and iptables rules. In addition, configuring the default bridge network happens outside of Docker itself and requires a restart of Docker. User-defined bridge networks are created and configured using <code>docker network create</code>. If different groups of applications have different network requirements, you can configure each user-defined bridge separately as you create it.</li> <li>Containers connected to the same user-defined bridge network effectively expose all ports to each other: For a port to be accessible to containers or non-Docker hosts on different networks, that port must be published using the <code>-p</code> or <code>--publish</code> flag.</li> </ul> <p>Limitation</p> <ul> <li>Limited to a single host only</li> <li>Bridge driver is slower than Host driver</li> </ul>"},{"location":"devops/docker/docker_network/#overlay-driver","title":"Overlay driver","text":"<p>The overlay driver creates a distributed network that can span multiple Docker hosts, and therefore is the preferred driver for managing container communication within a multi-host cluster. overlay is the default driver for Docker swarm services.</p> <p></p>"},{"location":"devops/docker/docker_storage/","title":"Docker Storage","text":"<p>On a linux system, docker stores data pertaining to images, containers, volumes, etc under <code>/var/lib/docker</code>.</p> <p>When we run the <code>docker build</code> command, docker builds one layer for each instruction in the Dockerfile. These image layers are read-only layers. When we run the <code>docker run</code> command, docker builds container layer(s), which are read-write layers.</p> <p></p> <p>You can create new files on the container, for instance, temp.txt. You can also modify a file that belongs to the image layers on the container, for instance, app.py. When you do this, a local copy of that file is created on the container layer and the changes only live on the container \u2014 this is called the Copy-on-Write mechanism. This is important as several containers and child images use the same image layers. The life of the files on the container is as long as the container is alive. When the container is destroyed, the files/modifications on it are also destroyed.</p> <p>Copy on Write</p> <p>Copy-on-write or CoW is a technique to efficiently copy data resources in a computer system. If a resource is duplicated but not modified, it is not necessary to create a new resource; the resource can be shared between the copy and the original. Modifications must still create a copy, hence the technique: the copy operation is deferred to the first write. By sharing resources in this way, it is possible to significantly reduce the resource consumption of unmodified copies, while adding a small overhead to resource-modifying operations.</p> <p></p> <p>You can create a docker volume by using the <code>docker volume create</code> command. This command will create a volume in the <code>/var/lib/docker/volumes</code> directory.</p> <pre><code>docker volume create data_volume\n</code></pre> <p>Now when you run the docker run command, you can specify which volume to use using the <code>-v</code> flag. This is called Volume Mounting.</p> <pre><code>docker run -v data_volume:/var/lib/postgres postgres\n</code></pre> <p>If the volume does not exist, docker creates one for you. Now, even if the container is destroyed the data will persist in the volume.</p> <p>If you want to have your data on a specific location on the docker host or already have existing data on the disk, you can mount this location on the container as well. This is called Bind Mounting.</p> <pre><code>docker run -v /data/postgres:/var/lib/postgres postgres\n</code></pre> <p></p>"},{"location":"devops/kubernetes/cluster_architecture/","title":"Cluster Architecture","text":"<p>Kubernetes Architecture has the following main components:</p> <ul> <li>Master nodes</li> <li>Worker/Slave nodes</li> <li>Distributed key-value store (etcd)</li> </ul>"},{"location":"devops/kubernetes/cluster_architecture/#master-node","title":"Master Node","text":"<p>It is the entry point for all administrative tasks which is responsible for managing the Kubernetes cluster. There can be more than one master node in the cluster to check for fault tolerance. More than one master node puts the system in a High Availability mode, in which one of them will be the main node which we perform all the tasks.</p> <p>For managing the cluster state, it uses <code>etcd</code> in which all the master nodes connect to it.</p> <p>Master node consists of 4 components:</p> <p>API server</p> <p>API server is the central management entity that receives all REST requests for modifications (to pods, services, replication sets/controllers and others), serving as frontend to the cluster. Also, this is the only component that communicates with the etcd cluster, making sure data is stored in etcd and is in agreement with the service details of the deployed pods.</p> <p>Scheduler</p> <p>Scheduler helps schedule the pods (a co-located group of containers inside which our application processes are running) on the various nodes based on resource utilization. It reads the service\u2019s operational requirements and schedules it on the best fit node.</p> <p>Controller manager</p> <p>Controller manager runs a number of distinct controller processes in the background (for example, replication controller controls number of replicas in a pod, endpoints controller populates endpoint objects like services and pods, and others) to regulate the shared state of the cluster and perform routine tasks. When a change in a service configuration occurs (for example, replacing the image from which the pods are running, or changing parameters in the configuration yaml file), the controller spots the change and starts working towards the new desired state.</p> <p>etcd</p> <p>etcd is a simple, distributed key value storage which is used to store the Kubernetes cluster data (such as number of pods, their state, namespace, subnets, confimaps, secrets, etc), API objects and service discovery details. It is only accessible from the API server for security reasons.</p> <p>etcd can be part of the Kubernetes Master, or, it can be configured externally.</p>"},{"location":"devops/kubernetes/cluster_architecture/#nodes","title":"Nodes","text":"<p>Node is a worker machine and that is where containers will be launched by k8s.</p> <p>Container runtime</p> <p>Container runtime helps to run and manage the container life cycle.</p> <p>Some of the container runtime examples are:</p> <ul> <li>containerd</li> <li>CRI-O</li> <li>Docker</li> </ul> <p>Kubelet</p> <p>It's a agent which communicates with the master node and executes on nodes or worker nodes. It is the main service on a node, regularly taking in new or modified pod specifications (primarily through the kube-apiserver) and ensuring that pods and their containers are healthy and running in the desired state. This component also reports to the master on the health of the host where it is running.</p> <p>Kube-proxy</p> <p>K8s cluster can have multiple worker nodes and each node has multiple pods running, so if one has to access this pod, they can do so via Kube-proxy.</p> <p>In order to access the pod via k8s services, there are certain network policies, that allow network communication to your Pods from network sessions inside or outside of your cluster. These rules are handled via kube-proxy</p>"},{"location":"devops/kubernetes/config_maps_and_secrets/","title":"ConfigMaps and Secrets","text":""},{"location":"devops/kubernetes/config_maps_and_secrets/#configmap","title":"ConfigMap","text":"<p>A ConfigMap is an API object used to store non-credential data in key-value pairs.</p> <p>Kubernetes pods can use the created ConfigMaps as a:</p> <ul> <li>Configuration files</li> <li>Environment variable</li> <li>Command-line argument</li> </ul> <p>A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.</p> <p>Importantly, ConfigMaps are not suitalbe for storing a confidental data. They don't provide any kind of encryption, and all the data in them are visible to anyone who has access to the file.</p>"},{"location":"devops/kubernetes/config_maps_and_secrets/#define-a-configmap","title":"Define a ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: game-demo\ndata:\n  # property-like keys; each key maps to a simple value\n  player_initial_lives: \"3\"\n  ui_properties_file_name: \"user-interface.properties\"\n\n  # file-like keys\n  game.properties: |\n    enemy.types=aliens,monsters\n    player.maximum-lives=5\n\n  user-interface.properties: |\n    color.good=purple\n    color.bad=yellow\n    allow.textmode=true\n\n  prometheus.yaml: |\n    global:\n      scrape_interval: 15s\n    scrape_configs:\n      - job_name: prometheus\n        metrics_path: /prometheus/metrics\n        static_configs:\n          - targets:\n            - localhost:9090\n</code></pre> <p>In a ConfigMap, the required information can store in the <code>data</code> field. We can store values as two ways:</p> <ul> <li>As individual key-value pair properties</li> <li>In a granular format where they are fragments of a configuration format. (File Like Keys)</li> </ul>"},{"location":"devops/kubernetes/config_maps_and_secrets/#utilizing-configmaps-in-pod","title":"Utilizing ConfigMaps in Pod","text":"<p>There are four ways that you can use a ConfigMap to configure a container inside a Pod:</p> <ol> <li>Inside a container command and args</li> <li>Environment variables for a containers</li> <li>Add a file in read-only volumne, for application to read</li> <li>Write code inside the Pod that uses the K8s API to read a ConfigMap</li> </ol> <p>Here's an example Pod that that uses values from the above ConfigMap:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-demo-pod\nspec:\n  containers:\n    - name: demo\n      image: alpine\n      command: [\"sleep\", \"3600\"]\n      env:\n        # Define the environment variable\n        - name: PLAYER_INITIAL_LIVES # Notice that the case is different here\n          # from the key name in the ConfigMap.\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo # The ConfigMap this value comes from.\n              key: player_initial_lives # The key to fetch.\n        - name: UI_PROPERTIES_FILE_NAME\n          valueFrom:\n            configMapKeyRef:\n              name: game-demo\n              key: ui_properties_file_name\n      volumeMounts:\n        - name: config\n          mountPath: \"/config\"\n          readOnly: true\n  volumes:\n    # You set volumes at the Pod level, then mount them into containers inside that Pod\n    - name: config\n      configMap:\n        # Provide the name of the ConfigMap you want to mount.\n        name: game-demo\n        # An array of keys from the ConfigMap to create as files\n        items:\n          - key: \"game.properties\"\n            path: \"game.properties\"\n          - key: \"user-interface.properties\"\n            path: \"user-interface.properties\"\n          - key: \"prometheus.yaml\"\n            path: \"prometheus.yaml\"\n</code></pre> <p>For this example, defining a volume and mounting it inside the demo container as <code>/config</code> creates three files, <code>/config/game.properties</code>, <code>/config/user-interface.properties</code> and <code>prometheus.yaml</code>, even though there are four keys in the ConfigMap. This is because the Pod definition specifies an <code>items</code> array in the <code>volumes</code> section. If you omit the <code>items</code> array entirely, every key in the ConfigMap becomes a file with the same name as the key, and you get 5 files</p>"},{"location":"devops/kubernetes/config_maps_and_secrets/#pods-use-configmaps-through-environment-variables-and-configmap-volumes","title":"Pods use ConfigMaps through environment variables and configMap volumes","text":""},{"location":"devops/kubernetes/config_maps_and_secrets/#pass-configmap-entries-to-a-pod-as-files-in-a-volume","title":"Pass ConfigMap entries to a pod as files in a volume","text":""},{"location":"devops/kubernetes/config_maps_and_secrets/#pass-a-configmap-entry-as-a-command-line-argument","title":"Pass a ConfigMap entry as a command-line argument","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: fortune-args-from-configmap\nspec:\n  containers:\n    - image: luksa/fortune:args\n      env:\n        - name: INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: fortune-config\n              key: sleep-interval\n      args: [\"$(INTERVAL)\"]\n</code></pre>"},{"location":"devops/kubernetes/config_maps_and_secrets/#secrets","title":"Secrets","text":"<p>A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Secrets are similar to ConfigMaps but are specifically intended to hold confidential data.</p>"},{"location":"devops/kubernetes/config_maps_and_secrets/#define-a-secrets","title":"Define a Secrets","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secret\ndata:\n  DB_USER: YWRtaW4=\n  DB_PASS: MWYyZDFlMmU2N2Rm\n</code></pre> <p>Note:</p> <ul> <li>Secrets are not encrypted. Only encoded (base64).</li> <li>Secrets are not encrypted in etcd</li> <li>Anyone able to create pods/ deployments in the same namespace can access the secrets</li> </ul> <p>In order to safely use Secrets, take at least the following steps:</p> <ol> <li>Enable Encryption at Rest for Secrets. (encrypt the data in etcd)</li> <li>Enable or configure RBAC rules with least-privilege access to Secrets.</li> <li>Restrict Secret access to specific containers.</li> <li>Consider using external Secret store providers.</li> </ol>"},{"location":"devops/kubernetes/config_maps_and_secrets/#uses-for-secrets","title":"Uses for Secrets","text":"<p>There are three main ways for a Pod to use a Secret:</p> <ul> <li>As files in a volume mounted on one or more of its containers.</li> <li>As container environment variable.</li> <li>By the kubelet when pulling images for the Pod.</li> </ul> <pre><code>envFrom:\n  - secretRef:\n      name: app-config\n</code></pre> <pre><code>env:\n  - name: DB_PASS\n    valueFrom:\n      secretRef:\n        name: app-config\n        key: DB_PASS\n</code></pre> <pre><code>volumes:\n  - name: app-secret-volume\n    secret:\n      secretName: app-secret\n</code></pre>"},{"location":"devops/kubernetes/controllers/","title":"Controllers","text":"<p>Controller are using to monitor k8s objects and respond accordingly.</p>"},{"location":"devops/kubernetes/controllers/#replicaset-old-term-replicationcontroller","title":"ReplicaSet (old term ReplicationController)","text":"<ul> <li>High availability</li> <li>Load balancing + Scaling</li> </ul> <p>A ReplicaSet ensures that a specified number of pod replicas are running at any given time.</p> <p>Example:</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\nspec:\n  replicas: 2 # number of replicas\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    # same as pod definition - begin\n    metadata:\n      labels:\n        tier: frontend\n    spec:\n      containers:\n        - name: php-redis\n          image: gcr.io/google_samples/gb-frontend:v3\n    # same as pod definition - end\n</code></pre> <p>The selector in the ReplicaSet definition helps the ReplicaSet identify which pods belong to it. The selector enables the ReplicaSet to manage pods that were not created as part of the ReplicaSet creation.</p> <p>For example, if there were pods created before the creation of the ReplicaSet that matched the labels specified in the selector, the ReplicaSet would also consider those pods when creating replicas.</p> <p></p>"},{"location":"devops/kubernetes/controllers/#the-process-of-creating-replicaset","title":"The process of creating ReplicaSet","text":"<ol> <li>K8s client (kubectl) sent a request to the API server requesting the creation of ReplicaSet</li> <li>The controller is watching the API server for new events and it detected that there is a new ReplicaSet object</li> <li>The controller creates 2 new pod definitions because we have configured replica value as 2 in the above example</li> <li>The scheduler is watching the API server for new events and it detected that there are 2 unasigned pods</li> <li>The scheduler decided which node to assign the Pod and sent that information to the API server</li> <li>Kublet is also watching the API server. It detected that the 2 pods were assigned to the node it is running on</li> <li>Kublet sent request to Docker requesting the creation of the containers that form the Pod.</li> <li>Finally, Kublet sent a request to the API server notifying it that the pods were created successfully</li> </ol>"},{"location":"devops/kubernetes/controllers/#deployments","title":"Deployments","text":"<p>A ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features. Therefore, k8s recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don't require updates at all.</p> <p>Deployment manages the overall lifecycle of an application, including rolling updates and rollbacks, while a ReplicaSet ensures that a specified number of identical replicas of the application are running at any given time. Deployments use ReplicaSets to manage scaling and ensure the desired state is achieved.</p> <p>This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section.</p>"},{"location":"devops/kubernetes/controllers/#define-a-zero-downtime-deployment","title":"Define a zero-downtime deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: go-demo-2-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      type: api\n      service: go-demo-2\n  minReadySeconds: 1\n  progressDeadlineSeconds: 60\n  revisionHistoryLimit: 5\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        type: api\n        service: go-demo-2\n        language: go\n    spec:\n      containers:\n        - name: api\n          image: vfarcic/go-demo-2\n          env:\n            - name: DB\n              value: go-demo-2-db\n          readinessProbe:\n            httpGet:\n              path: /demo/hello\n              port: 8080\n            periodSeconds: 1\n          livenessProbe:\n            httpGet:\n              path: /demo/hello\n              port: 8080\n</code></pre> <ul> <li><code>spec.minReadySeconds</code>: defines the number of seconds before k8s starts considering the Pod healthy. The default value is <code>0</code>, meaning that the Pods will be considered available as soon as they are ready and, when specified <code>livenessProbe</code> returns OK.</li> <li><code>spec.revisionHistoryLimit</code>: defines the number of old <code>ReplicaSet</code> we can rollback (default value is <code>10</code>)</li> <li><code>spec.strategy.type</code>: can be either <code>RollingUpdate</code> or <code>Recreate</code> type.</li> </ul>"},{"location":"devops/kubernetes/controllers/#deployment-strategies","title":"Deployment strategies","text":""},{"location":"devops/kubernetes/controllers/#recreate","title":"Recreate","text":"<p>The recreate strategy is a dummy deployment which consists of shutting down version A then deploying version B after version A is turned off. This technique implies downtime of the service that depends on both shutdown and boot duration of the application.</p> <p></p> <p>Pros:</p> <ul> <li>Easy to setup</li> <li>Application state entirely renewed.</li> </ul> <p>Cons:</p> <ul> <li>High impact on the user, expect downtime that depends on both shutdown and boot duration of the application.</li> </ul>"},{"location":"devops/kubernetes/controllers/#rolling-update","title":"Rolling update","text":"<p>The rolling update deployment strategy consists of slowly rolling out a version of an application by replacing instances one after the other until all the instances are rolled out.</p> <p></p> <p>Pros:</p> <ul> <li>Easy to setup</li> <li>Version is slowly released across instances</li> <li>Convenient for stateful applications that can handle rebalancing of the data.</li> </ul> <p>Cons:</p> <ul> <li>Rollout/rollback can take time.</li> <li>Supporting multiple APIs is hard.</li> <li>No control over traffic.</li> </ul>"},{"location":"devops/kubernetes/ingress/","title":"Ingress","text":""},{"location":"devops/kubernetes/ingress/#what-is-a-ingress","title":"What is a ingress?","text":"<p>In Kubernetes, an Ingress is an object that allows access to Kubernetes services from outside the Kubernetes cluster. The external traffic could be via HTTP or HTTPS to a service running within your Kubernetes cluster.</p> <p>Ingress in Kubernetes offers more advanced features for managing and routing traffic to APIs compared to NodePort and LoadBalancer.</p> <ul> <li>It allows you to direct requests to different services based on domain names, paths, or headers</li> <li>Ingress also provides built-in support for SSL/TLS encryption and load balancing</li> <li>It simplifies the configuration and centralizes the management of traffic routing rules</li> </ul>"},{"location":"devops/kubernetes/ingress/#how-does-kubernetes-ingress-work","title":"How Does Kubernetes Ingress work","text":"<p>There are 2 concepts in k8s ingress:</p> <ol> <li>Kubernetes Ingress Resource: Kubernetes Ingress Resource is responsible for storing DNS routing rules in the cluster. It specifies how incoming requests should be handled, such as domain-based routing or SSL/TLS termination.</li> <li>Kubernetes Ingress Controller: Kubernetes ingress controllers (Nginx/HAProxy etc.) are responsible for implementing the actual traffic routing based on the rules defined in the Ingress Resource. It configures the underlying load balancers or reverse proxies to direct the traffic accordingly.</li> </ol>"},{"location":"devops/kubernetes/ingress/#kubernetes-ingress-resource","title":"Kubernetes Ingress Resource","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: test-ingress\n  namespace: dev\nspec:\n  rules:\n  - host: test.apps.example.com\n    http:\n      paths:\n      - backend:\n          serviceName: hello-service\n          servicePort: 80\n</code></pre> <p>The above declaration means, that all calls to <code>test.apps.example.com</code> should hit the service named <code>hello-service</code> residing in the <code>dev</code> namespace.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: minimal-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n    # - host: dinhhuy258.com\n    # http:\n    - http:\n        paths:\n          - path: /testpath\n            pathType: Prefix\n            backend:\n              service:\n                name: test\n                port:\n                  number: 80\n</code></pre> <p>Annotations</p> <p>Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the <code>rewrite-target</code> annotation. Different Ingress controllers support different annotations.</p> <p>Explain <code>rewrite-target</code> annotation. For example we have 2 services:</p> <ol> <li><code>watch</code> app at <code>http://&lt;watch-service&gt;:&lt;port&gt;/</code></li> <li><code>wear</code> app at <code>http://&lt;wear-service&gt;:&lt;port&gt;/</code></li> </ol> <p>We must configure Ingress to achieve the below:</p> <ul> <li>http://:/watch \u2013&gt; http://:/ <li>http://:/wear \u2013&gt; http://:/ <p>Without the <code>rewrite-target</code> option, this is what would happen:</p> <ul> <li>http://:/watch \u2013&gt; http://:/watch <li>http://:/wear \u2013&gt; http://:/wear <p>Notice <code>watch</code> and <code>wear</code> at the end of the target URLs. The target applications are not configured with <code>/watch</code> or <code>/wear</code> paths. They are different applications built specifically for their purpose, so they don\u2019t expect <code>/watch</code> or <code>/wear</code> in the URLs</p> <p>To fix that we want to ReWrite the URL when the request is passed on to the <code>watch</code> or <code>wear</code> applications. We don\u2019t want to pass in the same path that user typed in. So we specify the <code>rewrite-target</code> option. This rewrites the URL by replacing whatever is under <code>rules-&gt;http-&gt;paths-&gt;path</code> with the value in <code>rewrite-target</code>. This works just like a search and replace function.</p> <p>For example: replace(path, rewrite-target)</p> <p>In our case: replace(\"/testpath\",\"/\")</p> <p>Ingress rules</p> <p>Each HTTP each contains the following information:</p> <ul> <li>An optional host. In the above example, no host is specified, so the rule applies to all inbound HTTP traffic throught the IP adress specified. If a host is provided (eg: dinhhuy258.com) the rules apply to that host.</li> <li>A list of paths (Eg: <code>/testpath</code>), each of which has an associated backend defined with a <code>service.name</code> and <code>service.port.name</code> or <code>service.port.number</code>. Both the host and path must match the content of an incomming request before the load balancer directs traffic to the referenced Service.</li> </ul> <p>Ingress path types</p> <p>Each path in Ingress is required to have a corresponding path type. There are three supported path types:</p> <ul> <li><code>ImplementationSpecific</code>: With this path type, matching is up to the IngressClass. Implementation can treat this as a separate <code>pathType</code> or treat it identically to <code>Prefix</code> or <code>Exact</code> path types.</li> <li><code>Exact</code>: Match the URL path exactly and with case sensitive.</li> <li><code>Prefix</code>: Match based on a URL path prefix split by <code>/</code></li> </ul> <p>Note: If the last element of the path is a substring of the last element in the request path, it is not match (Eg: <code>/foo/bar</code> matches <code>/foo/bar/baz</code> but does not match <code>/foo/barbaz</code>)</p> <p>TLS</p> <p>You can secure an Ingress by specifying a Secret that contains a TLS private key an certificate.</p> <p>Eg:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: testsecret-tls\n  namespace: default\ndata:\n  tls.crt: base64 encoded cert\n  tls.key: base64 encoded key\ntype: kubernetes.io/tls\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tls-example-ingress\nspec:\n  tls:\n  - hosts:\n      - https-example.foo.com\n    secretName: testsecret-tls\n  rules:\n  - host: https-example.foo.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n   apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: tls-example-ingress\nspec:\n  tls:\n  - hosts:\n      - https-example.foo.com\n    secretName: testsecret-tls\n  rules:\n  - host: https-example.foo.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: service1\n            port:\n              number: 80\n         name: service1\n            port:\n              number: 80\n</code></pre>"},{"location":"devops/kubernetes/ingress/#kubernetes-ingress-controller","title":"Kubernetes Ingress Controller","text":"<p>An ingress controller is typically a reverse web proxy server implementation in the cluster. In kubernetes terms, it is a reverse proxy server deployed as kubernetes deployment exposed to a service type Loadbalancer. (Nginx is one of the widely used ingress controllers)</p> <p>Key things to understand about ingress objects.</p> <ol> <li>An ingress object requires an ingress controller for routing traffic.</li> <li>And most importantly, the external traffic does not hit the ingress API, instead, it will hit the ingress controller service endpoint configured directly with a load balancer.</li> </ol> <p>Follow this tutorial to learn how to setup an ingress controller in k8s.</p>"},{"location":"devops/kubernetes/namespaces/","title":"Namespaces","text":"<p>In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster.</p> <p></p>"},{"location":"devops/kubernetes/namespaces/#initial-namespaces","title":"Initial namespaces","text":"<p>Kubernetes starts with four initial namespaces:</p> <ul> <li> <p>default Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.</p> </li> <li> <p>kube-node-lease This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.</p> </li> <li> <p>kube-public This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster.</p> </li> <li> <p>kube-system The namespace for objects created by the Kubernetes system.</p> </li> </ul>"},{"location":"devops/kubernetes/namespaces/#working-with-namespaces","title":"Working with Namespaces","text":"<pre><code># list namespaces\nkubectl get ns\nkubectl get namespaces\n\n# create namespace\nkubectl create namespace &lt;namespace-name&gt;\n\n# list all objects from default, dev1 &amp; dev2 namespaces\nkubectl get all -n default\nkubectl get all -n dev1\nkubectl get all -n dev2\n\n# deploy all k8s objects\nkubectl apply -f kube-manifests/\nkubectl apply -f kube-manifests/ -n dev1\nkubectl apply -f kube-manifests/ -n dev2\n\n# switch namespace\nkubectl config set-context $(kubectl config current-context) --namespace=dev\n</code></pre>"},{"location":"devops/kubernetes/namespaces/#namespaces-and-dns","title":"Namespaces and DNS","text":"<p>When you create a Service, it creates a corresponding DNS entry. This entry is of the form <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>, which means that if a container only uses , it will resolve to the service which is local to a namespace. As a result, all namespace names must be valid RFC 1123 DNS labels."},{"location":"devops/kubernetes/pods/","title":"Pods","text":""},{"location":"devops/kubernetes/pods/#what-is-a-pod","title":"What is a pod?","text":"<p>Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.</p> <p>A Pod is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers.</p> <p>The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a Docker container.</p> <p>In terms of Docker concepts, a Pod is similar to a group of Docker containers with shared namespaces and shared filesystem volumes.</p> <p></p>"},{"location":"devops/kubernetes/pods/#pod-networking","title":"Pod networking","text":"<p>Each Pod is assigned a unique IP address for each address family. Every container in a Pod shares the network namespace, including the IP address and network ports. Inside a Pod (and only then), the containers that belong to the Pod can communicate with one another using <code>localhost</code>. The containers in a Pod can also communicate with each other using standard inter-process communications</p> <p>Containers in different Pods have distinct IP addresses and can not communicate by IPC without special configuration. Containers that want to interact with a container running in a different Pod can use IP networking to communicate.</p>"},{"location":"devops/kubernetes/pods/#pod-lifetime","title":"Pod lifetime","text":"<p>Pods are created, assigned a unique ID (UID), and scheduled to nodes where they remain until termination (according to restart policy) or deletion. If a Node dies, the Pods scheduled to that node are scheduled for deletion after a timeout period.</p> <p>Pods do not, by themselves, self-heal. If a Pod is scheduled to a node that then fails, the Pod is deleted; likewise, a Pod won't survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a higher-level abstraction, called a controller, that handles the work of managing the relatively disposable Pod instances.</p>"},{"location":"devops/kubernetes/pods/#how-can-we-create-pods-in-kubernetes","title":"How can we create Pods in Kubernetes?","text":"<p>Pod definition template</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata: ...\nspec: ...\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-2\n  labels:\n    name: nginx-2\n    env: production\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n</code></pre> <pre><code>kubectl apply -f mypod.yaml\n</code></pre>"},{"location":"devops/kubernetes/pods/#commands-and-arguments","title":"Commands and Arguments","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu-sleeper-pod\nspec:\n  containers:\n    - name: ubuntu-sleeper\n      image: ubuntu-sleeper # entry point: sleep\n      args: [\"10\"] # ~ cmd\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ubuntu-sleeper-pod\nspec:\n  containers:\n    - name: ubuntu-sleeper\n      image: ubuntu-sleeper\n      command: [\"sleep\"] # ~ entry point\n      args: [\"10\"] # ~ cmd\n</code></pre>"},{"location":"devops/kubernetes/pods/#env-variables","title":"ENV variables","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-2\n  labels:\n    name: nginx-2\n    env: production\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n\n      envFrom:\n        - configMapRef:\n            name: env-configmap\n        - secretRef:\n            name: env-secrets\n  env:\n    - name: DEBUG\n      value: false\n    - name:\n      valueFrom:\n        configMapKeyRef:\n    - name:\n      valueFrom:\n        secretKeyRef:\n</code></pre>"},{"location":"devops/kubernetes/pods/#init-containers","title":"Init containers","text":"<p>A Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started.</p> <p>Init containers are exactly like regular containers, except:</p> <ul> <li>Init containers always run to completion.</li> <li>Each init container must complete successfully before the next one starts.</li> </ul> <p>If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']\n</code></pre>"},{"location":"devops/kubernetes/pods/#security-context","title":"Security context","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-pod\nspec:\n  securityContext: # security in the pod level\n    runAsUser: 1000\n  containers:\n    - name: ubuntu\n      image: ubuntu\n      command: [\"sleep\", \"1000\"]\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: web-pod\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu\n      command: [\"sleep\", \"1000\"]\n      securityContext: # security in the container level\n        runAsUser: 1000\n        capabilities:\n          add: [\"MAC_ADMIN\"]\n</code></pre>"},{"location":"devops/kubernetes/pods/#resource-requirements","title":"Resource Requirements","text":"<p>Kubernetes defines Limits as the maximum amount of a resource to be used by a container. This means that the container can never consume more than the memory amount or CPU amount indicated. By default K8s sets a limit of one vCPU and 512 Mebibyte to containers.</p> <p>Requests, on the other hand, are the minimum guaranteed amount of a resource that is reserved for a container. By default k8s assumes that a pod requires: 0.5 CPU, 256 Mi</p> <p>If you know that your application need more than this. You can modify these values by specifying them in your pod definition.</p> <p></p> <p>What does <code>cpu: 1</code> mean?</p> <ul> <li>1 AWS vCPU</li> <li>1 GCP Core</li> <li>1 Azure Core</li> <li>1 Hyperthread</li> </ul> <p>1000 milicores = 1 core 1 core = 1024 shares 100 milicores = 102 shares</p> <p>Memory</p> <ul> <li>1G (Gigabyte) (1.000.000.000 bytes), 1M (Megabyte) (1.000.000 bytes), 1K (Kilobyte) (1.000 bytes)</li> <li>1Gi (Gibibyte) (1.073.741.824 bytes), 1Mi (Mebibyte) (1.048.576 bytes), 1Ki (Kibibyte) (1.024 bytes)</li> </ul>"},{"location":"devops/kubernetes/pods/#useful-commands","title":"Useful commands","text":"<pre><code>kubectl get pods\n</code></pre> <pre><code>kubectl port-forward {pod_name} 8080:80\n</code></pre> <pre><code>kubectl delete pod {pod_name}\n</code></pre> <pre><code>kubectl describe pod {pod_name}\n</code></pre> <pre><code>kubectl get pod &lt;pod-name&gt; -o yaml &gt; pod-definition.yaml\n</code></pre> <pre><code>kubectl edit pod &lt;pod-name&gt;\n</code></pre>"},{"location":"devops/kubernetes/service/","title":"Service","text":"<p>K8s services enable communication between various components within and outside of the application.</p> <p>Kubernetes pods are created and destroyed to match the state of your cluster. If you use Deployment to run your app, it can create and destroy pods dynamically.</p> <p>Each pods get it own IP address, however in Deployment, the set of pods running in one moment in time could be different from the set of pods running that application a moment later.</p> <p>It leads to a problem that if set of pods (backend) provides functionality for other pods (frontend) inside your cluster, how do the frontend can find out and keep track which ip address to connect to?</p> <p>Kubernetes Services provides addresses through which associated pods can be accessed.</p> <p></p>"},{"location":"devops/kubernetes/service/#1-define-a-service","title":"1. Define a service","text":"<p>For example, suppose you have a set of pods where each listens on TCP port <code>9376</code> and contains a label <code>app=MyApp</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre> <p>This specification creates a new Service object named <code>my-service</code> which targets TCP port <code>9376</code> on any Pod with the <code>app=MyApp</code> label.</p> <p>Kubernetes assigns this Service an IP address (sometimes called the \"cluster IP\"), which is used by the Service proxies.</p> <p>The controller for the Service selector continuously scans for Pods that match its selector, and then POSTs any updates to an Endpoint object also named <code>my-service</code>.</p>"},{"location":"devops/kubernetes/service/#2-services-without-selectors","title":"2. Services without selectors","text":"<p>Services most commonly abstract access to Kubernetes Pods, but they can also abstract other kinds of backends. For example:</p> <ul> <li>You want to have an external database cluster in production, but in your test environment you use your own databases.</li> <li>You want to point your Service to a Service in a different Namespace or on another cluster.</li> </ul> <p>Eg:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre> <p>Because this Service has no selector, the corresponding Endpoints object is not created automatically. You can manually map the Service to the network address and port where it's running, by adding an Endpoints object manually:</p> <pre><code>apiVersion: v1\nkind: Endpoints\nmetadata:\n  name: my-service\nsubsets:\n  - addresses:\n      - ip: 192.0.2.42\n    ports:\n      - port: 9376\n</code></pre>"},{"location":"devops/kubernetes/service/#3-type-of-service","title":"3. Type of service","text":"<p>Kubernetes Services allow you to specify what kind of Service you want. The default is <code>ClusterIP</code>.</p>"},{"location":"devops/kubernetes/service/#clusterip","title":"ClusterIP","text":"<p>Exposes the service on cluster internal IP. Choosing this makes the Service only reachable from within the cluster.</p> <p></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\nspec:\n  selector:\n    app: MyApp\n  type: ClusterIP\n  ports:\n    - targetPort: 80\n      port: 80\n</code></pre>"},{"location":"devops/kubernetes/service/#nodeport","title":"NodePort","text":"<p>Service helps us map a port on the node to a port on the pod. NodePort exposes the service on each Node'IP at a static port. A <code>ClusterIP</code> is automatically created. You will be able to reach the NodePort service from outside the cluster, by requesting : <p></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: MyApp\n  type: NodePort\n  ports:\n    - targetPort: 80\n      port: 80\n      nodePort: 30008\n</code></pre>"},{"location":"devops/kubernetes/service/#loadbalancer","title":"LoadBalancer","text":"<p>A LoadBalancer service is based on the NodePort service, and adds the ability to configure external load balancers in public and private clouds. It exposes services running within the cluster by forwarding network traffic to cluster nodes.</p> <p></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-loadbalancer-service\nspec:\n  type: LoadBalancer\n  clusterIP: 10.0.160.135\n  loadBalancerIP: 168.196.90.10\n  selector:\n    app: nginx\n  ports:\n    \u2014 name: http\n      protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre>"},{"location":"devops/kubernetes/service/#externalname","title":"ExternalName","text":"<p>An ExternalName service maps the service to a DNS name instead of a selector. You define the name using the spec:externalName parameter. It returns a CNAME record matching the contents of the externalName field (for example, my.service.domain.com), without using a proxy.</p> <p>This type of service can be used to create services in Kubernetes that represent external components such as databases running outside of Kubernetes. Another use case is allowing a pod in one namespace to communicate with a service in another namespace\u2014the pod can access the ExternalName as a local service.</p> <p></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-externalname-service\nspec:\n  type: ExternalName\n  externalName: my.database.domain.com\n</code></pre>"},{"location":"devops/kubernetes/service/#useful-commands","title":"Useful commands","text":"<pre><code>kubectl get services\n</code></pre>"},{"location":"devops/kubernetes/volumes/","title":"Volumes","text":"<p>A Volume in Kubernetes represents a directory with data that is accessible across multiple containers in a Pod. The container data in a Pod is deleted or lost when a container crashes or restarts, but when you use a volume, the new container can pick up the data at the state before the container crashes. The volume outlives the containers in a Pod and can be consumed by any number of containers within that Pod.</p>"},{"location":"devops/kubernetes/volumes/#types-of-volumes","title":"Types of Volumes","text":"<p>K8s supportes several types of volumes. We can categorize the Kubernetes Volumes based on their lifecycle.</p> <p>Considering the lifecycle of the volumes, we can have:</p> <ul> <li>Ephemeral Volumes, which are tightly coupled with the lifetime of the Node (for example emptyDir, or hostPath) and they are deleted if the Node goes down.</li> <li>Persistent Volumes, which are meant for long-term storage and are independent of the Pods or Nodes lifecycle. These can be cloud volumes (like gcePersistentDisk, awsElasticBlockStore, azureFile or azureDisk), NFS (Network File Systems) or Persistent Volume Claims (a series of abstraction to connect to the underlying cloud provided storage volumes).</li> </ul>"},{"location":"devops/kubernetes/volumes/#ephemeral-volumes","title":"Ephemeral Volumes","text":""},{"location":"devops/kubernetes/volumes/#emptydir","title":"emptyDir","text":"<p>The emptyDir volume is primarily designed for sharing files between containers within the same pod. When multiple containers are running within a pod and they need to exchange data or share files, an emptyDir volume can be used as a common location for storing and accessing that data.</p> <p>Here are some key characteristics of the emptyDir volume:</p> <ul> <li>Lifetime: An emptyDir volume is tied to the lifecycle of a pod. It is created when the pod is created and deleted when the pod is terminated.</li> <li>Accessibility: The emptyDir volume is accessible by all containers within the same pod. This means that multiple containers running in the same pod can read from and write to the same emptyDir volume.</li> <li>Storage Medium: The storage medium used for the emptyDir volume depends on the underlying infrastructure of the cluster. It could be a directory on the host's filesystem or a memory-based filesystem, depending on the configuration.</li> <li>Persistence: An emptyDir volume is not designed for persistent storage. If the pod is restarted or rescheduled to a different node, the contents of the emptyDir volume will be lost.</li> </ul> <p>Some use cases for an emptyDir are:</p> <ul> <li>Scratch space, for a sort algorithm for example</li> <li>Checkpointing a long computation for recovery from crashes</li> <li>As a cache</li> <li>Holding files that a content-manager container fetches while a webserver Container serves the data.</li> </ul> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-nginx\nspec:\n  containers:\n    - image: nginx\n      name: test-nginx\n      volumeMounts:\n        - mountPath: /cache\n          name: cache-volume\n  volumes:\n    - name: cache-volume\n      emptyDir: {}\n</code></pre> <p>Let\u2019s try applying the YAML file and get into the Pod.</p> <pre><code>kubectl apply -f emptydir.yml\npod/test-nginx created\n\nkubectl get pods\nNAME         READY   STATUS    RESTARTS   AGE\ntest-nginx   1/1     Running   0          7s\n\nkubectl exec -it test-nginx -- /bin/bash\nroot@test-nginx:/# mount | grep -i cache\n/dev/vda1 on /cache type ext4 (rw,relatime)\n</code></pre> <p>If we see the storage medium used for the <code>emptyDir</code> mounted on the container we just created, it shows up as <code>/dev/vda1</code> a paravirtualization disk driver.</p> <p>If you set the <code>emptyDir.medium</code> field to <code>Memory</code>. Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-nginx\nspec:\n  containers:\n    - image: nginx\n      name: test-nginx\n      volumeMounts:\n        - mountPath: /cache\n          name: cache-volume\n  volumes:\n    - name: cache-volume\n      emptyDir:\n        medium: Memory\n</code></pre>"},{"location":"devops/kubernetes/volumes/#hostpath","title":"hostPath","text":"<p>A hostPath volume mounts a file or directory from the host node's filesystem into your Pod.</p> <p>Here are some key characteristics of the hostPath volume:</p> <ul> <li>Accessibility: The hostPath volume allows containers within the pod to access files on the host node's filesystem. It enables sharing of files between the host and the containers.</li> <li>Storage Medium: The hostPath volume mounts a directory or file from the host's filesystem. The actual storage medium depends on the host node's configuration and can be a local disk, network-attached storage, or any other filesystem accessible to the node.</li> <li>Persistence: The hostPath volume is not tied to the lifecycle of the pod. If the pod is restarted or rescheduled to a different node, the hostPath volume will still be available as long as the file or directory exists on the host node.</li> <li>Security Considerations: The hostPath volume grants access to the host's filesystem, so it should be used with caution. Granting containers access to sensitive files or system directories on the host can pose security risks.</li> </ul> <p>Some use cases for an hostPath are:</p> <ul> <li>Running a container that needs access to Docker internals; use a hostPath of /var/lib/docker</li> <li>Persist database data, application data</li> </ul> <p>Example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pd\nspec:\n  containers:\n    - image: k8s.gcr.io/test-webserver\n      name: test-container\n      volumeMounts:\n        - mountPath: /test-pd\n          name: test-volume\n  volumes:\n    - name: test-volume\n      hostPath:\n        # directory location on host\n        path: /data\n        # this field is optional\n        type: Directory\n</code></pre> <p>The supported values for field <code>type</code> are:</p> <ul> <li><code>DirectoryOrCreate</code>: If nothing exists at the given path, an empty directory will be created there as needed with permission set to <code>0755</code>, having the same group and ownership with Kubelet.</li> <li><code>Directory</code>: A directory must exist at the given path</li> <li><code>FileOrCreate</code>: If nothing exists at the given path, an empty file will be created there as needed with permission set to <code>0644</code>, having the same group and ownership with Kubelet.</li> <li><code>File</code>: A file must exist at the given path</li> <li><code>Socket</code>: A UNIX socket must exist at the given path</li> <li><code>CharDevice</code>: A character device must exist at the given path</li> <li><code>BlockDevice</code>: A block device must exist at the given path</li> </ul>"},{"location":"devops/terraform/data_sources/","title":"Data Sources","text":"<p>Data sources allow Terraform to use information defined outside of Terraform, defined by another separate Terraform configuration, or modified by functions.</p>"},{"location":"devops/terraform/data_sources/#using-data-sources","title":"Using Data Sources","text":"<pre><code>data \"aws_ami\" \"example\" {\n  most_recent = true\n\n  owners = [\"self\"]\n  tags = {\n    Name   = \"app-server\"\n    Tested = \"true\"\n  }\n}\n</code></pre> <p>A data block requests that Terraform read from a given data source (<code>aws_ami</code>) and export the result under the given local name (<code>example</code>). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module.</p> <p>The data source and name together serve as an identifier for a given resource and so must be unique within a module.</p> <p>Within the block body (between <code>{</code> and <code>}</code>) are query constraints defined by the data source. Most arguments in this section depend on the data source, and indeed in this example <code>most_recent</code>, <code>owners</code> and <code>tags</code> are all arguments defined specifically for the <code>aws_ami</code> data source.</p>"},{"location":"devops/terraform/data_sources/#examples","title":"Examples","text":"<p>Using data sources to access external resource attributes</p> <pre><code>data \"aws_s3_bucket\" \"existing_bucket\" {\n  bucket = \"sumeet.life\"\n}\n</code></pre> <p>I have a bucket named <code>sumeet.life</code> in my AWS account.</p> <p>When this Terraform configuration with appropriate provider settings is initialized and enabled, Terraform reads the information from AWS and makes it available in the <code>data.aws_s3_bucket.existing_bucket</code> variable.</p> <p>Managing resource dependencies with data sources</p> <p>Data sources indirectly help manage resource dependencies. If the data being queried by data sources does not exist, then the resource that is dependent on the same will not be created.</p> <pre><code>data \"aws_subnets\" \"my_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [\"vpc-xxx\"]\n  }\n}\n\nresource \"aws_instance\" \"my_vm_2\" {\n  for_each      = toset(data.aws_subnets.my_subnets.ids)\n  ami           = var.ami //Ubuntu AMI\n  instance_type = var.instance_type\n\n  subnet_id = each.key\n\n  tags = {\n    Name = var.name_tag,\n  }\n\n  depends_on = [ data.aws_subnets.my_subnets ]\n}\n</code></pre> <p>Validate inputs with data sources</p> <pre><code>variable \"instance_ami\" {\n  description = \"Ubuntu AMI in Frankfurt region.\"\n  //default = \"ami-065deacbcaac64cf2\"\n  default = \"ami-xxx\" // This does not exist\n}\n\n\nresource \"aws_instance\" \"myec2\" {\n  ami = var.instance_ami\n  instance_type = \"t2.micro\"\n}\n</code></pre> <p>If we provide invalid <code>instance_ami</code> the terraform output does not throw any error</p> <pre><code>.\n.\n.\n+ user_data                            = (known after apply)\n      + user_data_base64                     = (known after apply)\n      + user_data_replace_on_change          = false\n      + vpc_security_group_ids               = (known after apply)\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n</code></pre> <p>However, it does throw an error when we proceed to apply this configuration since the given AMI does not exist.</p> <p>In certain scenarios, it might be desirable for this configuration to throw an error in the planning phase itself for various reasons. This is achieved using data sources.</p> <pre><code>data \"aws_ami\" \"selected\" {\n  most_recent = true\n  filter {\n    name = \"image-id\"\n    values = [var.instance_ami]\n  }\n}\n</code></pre> <p>If we provide invalid <code>instance_ami</code> the terraform plan output</p> <pre><code>.\n.\n.\n+ user_data_base64                     = (known after apply)\n      + user_data_replace_on_change          = false\n      + vpc_security_group_ids               = (known after apply)\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\u2577\n\u2502 Error: Your query returned no results. Please change your search criteria and try again.\n\u2502\n\u2502   with data.aws_ami.selected,\n\u2502   on main.tf line 9, in data \"aws_ami\" \"selected\":\n\u2502    9: data \"aws_ami\" \"selected\" {\n</code></pre> <p>Using data sources to access AWS secrets</p> <p>Secrets Manager is a service provided by AWS to manage sensitive data. It stores the sensitive variables securely using encryption, and makes them available to various services for automation purposes.</p> <p>In Terraform configurations, these secrets are accessed using data sources.</p> <pre><code>data \"aws_secretsmanager_secret\" \"mydb_secret\" {\n  arn = \"arn:aws:secretsmanager:eu-central-1:532199187081:secret:pg_db-dKuRrd\"\n}\n\ndata \"aws_secretsmanager_secret_version\" \"mydb_secret_version\" {\n  secret_id = data.aws_secretsmanager_secret.mydb_secret.id\n}\n\nresource \"aws_db_instance\" \"my_db\" {\n  allocated_storage = 20\n  storage_type = \"gp2\"\n  engine = \"postgres\"\n  engine_version = \"13.3\"\n  instance_class = \"db.t2.micro\"\n  username = jsondecode(data.aws_secretsmanager_secret_version.mydb_secret_version.secret_string).username\n  password = jsondecode(data.aws_secretsmanager_secret_version.mydb_secret_version.secret_string).password\n  skip_final_snapshot = true\n\n\n  tags = {\n    Name = \"ExampleDB\"\n  }\n\n  tags_all = {\n    Environment = \"Development\"\n  }\n}\n</code></pre>"},{"location":"devops/terraform/files/","title":"Files","text":""},{"location":"devops/terraform/files/#terraform-files","title":"Terraform files","text":"<p>policy.iam</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n            \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>main.tf</p> <pre><code>provider \"aws\" {\n    region = \"eu-west-1\"\n}\n\nresource \"aws_iam_policy\" \"my_bucket_policy\" {\n    name = \"list-buckets-policy\"\n    policy = file(\"./policy.iam\")\n}\n</code></pre>"},{"location":"devops/terraform/files/#templatefile","title":"templatefile","text":"<p>Sometimes we want to use a file but do not know all of the values before we run the project. Other times, the values are dynamic and generated as a result of a created resource. To use dynamic values in a file, we need to use the <code>templatefile</code> function.</p> <p>The <code>templatefile</code> function allows us to define placeholders in a template file and then pass their values at runtime.</p> <p>example.tpl</p> <pre><code>hello there ${name}\nthere are ${number} things to say\n</code></pre> <p>main.tf</p> <pre><code>locals {\n    rendered = templatefile(\"./example.tpl\", { name = \"kevin\", number = 7})\n}\n\noutput \"rendered_template\" {\n    value = local.rendered\n}\n</code></pre> <p>Project output:</p> <pre><code>Apply complete! Resources: 0 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nrendered_template =\nhello there kevin\nthere are 7 things to say\n</code></pre>"},{"location":"devops/terraform/files/#loops-in-a-template","title":"Loops in a Template","text":"<p>You can pass in an array of values into a template and loop through them.</p> <p>backends.tpl</p> <pre><code>%{ for addr in ip_addrs ~}\nbackend ${addr}:${port}\n%{ endfor ~}\n</code></pre> <p>main.tf</p> <pre><code>output \"rendered_template\" {\n    value = templatefile(\"./backends.tpl\", { port = 8080, ip_addrs = [\"10.0.0.1\", \"10.0.0.2\"] })\n}\n</code></pre> <p>Project output:</p> <pre><code>backend 10.0.0.1:8080\nbackend 10.0.0.2:8080\n</code></pre>"},{"location":"devops/terraform/input_variables/","title":"Input Variables","text":"<p>Terraform input variables are used as parameters to input values at run time to customize our deployments. Input terraform variables can be defined in the <code>main.tf</code> configuration file but it is a best practice to define them in a separate <code>variable.tf</code> file to provide better readability and organization.</p> <p>A variable is defined by using a variable block with a label. The label is the name of the variable and must be unique among all the variables in the same configuration.</p> <p>The variable declaration can optionally include three arguments:</p> <ul> <li>description: briefly explain the purpose of the variable and what kind of value is expected.</li> <li>type: specifies the type of value such as string, number, bool, map, list, etc.</li> <li>default: If present, the variable is considered to be optional and if no value is set, the default value is used.</li> </ul>"},{"location":"devops/terraform/input_variables/#input-variable-types","title":"Input Variable Types","text":"<p>The type argument in a variable block allows you to enforce type constraints on the variables a user passes in. Terraform supports a number of types, including string, number, bool, list, map, set, object, tuple, and any. If a type isn\u2019t specified, then Terraform assumes the type is any.</p> <pre><code>variable \"vpcname\" {\n    type = string\n    default = \"myvpc\"\n}\n\nvariable \"mylist\" {\n    type = list(string)\n    default = [\"value1\", \"value2\"]\n}\n\nvariable \"mymap\" {\n    type = map\n    default = {\n        key1 = \"value1\"\n        key2 = \"value2\"\n    }\n}\n\nvariable \"myobject\" {\n    type = object({\n        field1: string\n        field2: string\n    })\n}\n</code></pre>"},{"location":"devops/terraform/input_variables/#use-input-variables","title":"Use Input Variables","text":"<p>After declaration, we can use the variables in our <code>main.tf</code> file by calling that variable in the <code>var.&lt;name&gt;</code> format. The value to the variables is assigned during terraform apply.</p> <pre><code>provider \"azurerm\" {\n    version = \"1.38.0\"\n}\n\nresource \"azure_resource_group\" \"TFAzure-rg\" {\n    name = var.resourceGroupName\n    location = var.location\n    tag = {\n        Environment = \"Dev\"\n    }\n}\n</code></pre>"},{"location":"devops/terraform/input_variables/#assign-values-to-input-variables","title":"Assign Values To Input Variables","text":""},{"location":"devops/terraform/input_variables/#command-line-flags","title":"Command-line flags","text":"<pre><code>terraform apply -var=\"resourceGroupName=terraformdemo-rg\" -var=\"location=eastus\"\n</code></pre>"},{"location":"devops/terraform/input_variables/#variable-definition-tfvars-files","title":"Variable Definition (.tfvars) Files","text":"<pre><code>resourceGroupName = \"terraformdemo-rg\"\nlocation = \"eastus\"\n</code></pre> <p>If there are many variable values to input, we can define them in a variable definition file. Terraform also automatically loads a number of variable definitions files if they are present: - Files named exactly <code>terraform.tfvars</code> or <code>terraform.tfvars.json</code> - Any files with names ending in <code>.auto.tfvars</code> or <code>.auto.tfvars.json</code></p> <p>If the file is named something else, then use the <code>-var-file</code> flag directly to specify a file.</p> <pre><code>terraform apply -var-file=\"testing.tfvars\"\n</code></pre>"},{"location":"devops/terraform/locals/","title":"Terraform Locals","text":"<p>Terraform Locals are named values which can be assigned and used in your code. It mainly serves the purpose of reducing duplication within the Terraform code. </p> <pre><code>locals {\n    bucket_name = \"${var.text1}-${var.text2}\"\n}\n</code></pre>"},{"location":"devops/terraform/locals/#how-to-use-terraform-locals","title":"How to Use Terraform Locals?","text":"<ul> <li>First, declare the local along and assign a value</li> </ul> <pre><code>locals {\n    env         = \"dev\"\n    instance_ids = concat(aws_instance.ec1.*.id, aws_instance.ec3.*.id)\n    prefix_elements = [for elem in [\"a\", \"b\", \"c\"] : format(\"Hello %s\", elem)]\n    even_numbers = [for i in [1, 2, 3, 4, 5, 6] : i if i % 2 == 0]\n}\n</code></pre> <ul> <li>Then, use the local name anywhere in the code where that value is needed</li> </ul> <pre><code>resource \"aws_s3_bucket\" \"my_test_bucket\" {\n    bucket = \"${local.bucket_name}-newbucket\"\n    acl    = \"private\"\n\n    tags = {\n        Name        = local.bucket_name\n        Environment = local.env\n    }\n}\n</code></pre>"},{"location":"devops/terraform/locals/#terraform-locals-vs-variables","title":"Terraform Locals vs Variables","text":"<p>Variables - Terraform variable can be scoped globally - A variable value can be manipulated via expressions - A variable value can be set from an input or <code>.tfvars</code> file - Can not use dynamic expressions</p> <p>Locals - A Local is only accessible within the local module - A local in Terraform doesn\u2019t change its value once assigned - A local value can not be set from and input or <code>.tfvars</code> file - Can use dynamic expressions</p>"},{"location":"devops/terraform/locals/#examples","title":"Examples","text":"<p>Using locals to name resources</p> <pre><code>locals {\n    name_prefix = \"JacksDemo-${var.environment}\"\n}\n\nmodule \"appgateway\" {\n    application_gateway_name = \"${local.name_prefix}-${var.application_gateway_name}\"\n    ...\n}\n</code></pre> <p>Using locals to set resource tags</p> <pre><code>locals {\n    mandatory_tags = {\n        cost_center = var.cost_center,\n        environment = var.environment\n    }\n}\n\ntags = merge(var.resource_tags, local.mandatory_tags)\n</code></pre>"},{"location":"devops/terraform/modules/","title":"Modules","text":"<p>Modules are containers for multiple resources that are used together. A module consists of a collection of .tf and/or .tf.json files kept together in a directory.</p> <p>Modules are the main way to package and reuse resource configurations with Terraform.</p>"},{"location":"devops/terraform/modules/#root-module","title":"Root module","text":"<p>Every Terraform configuration has at least one module, known as its root module, which consists of the resources defined in the <code>.tf</code> files in the main working directory.</p>"},{"location":"devops/terraform/modules/#child-modules","title":"Child Modules","text":"<p>A Terraform module (usually the root module of a configuration) can call other modules to include their resources into the configuration. A module that has been called by another module is often referred to as a child module.</p> <p>Child modules can be called multiple times within the same configuration, and multiple configurations can use the same child module.</p>"},{"location":"devops/terraform/modules/#examples","title":"Examples","text":"<pre><code>provider \"aws\" {\n    region = \"eu-west-1\"\n}\n\nmodule \"work_queue\" {\n    source = \"./sqs-with-backoff\"\n    queue_name = \"work-queue\" # variable\n}\n\noutput \"work_queue_name\" {\n    value = module.work_queue.queue_name # access module output\n}\n\noutput \"work_queue_dead_letter_name\" {\n    value = module.work_queue.dead_letter_queue_name # access module output\n}\n</code></pre> <p>Remote module</p> <pre><code>provider \"aws\" {\n    region = \"us-east-2\"\n}\n\nmodule \"work_queue\" {\n    source = \"github.com/dinhhuy258/sqs-with-backoff\"\n    queue_name = \"work-queue\"\n}\n\noutput \"work_queue\" {\n    value = module.work_queue.queue # access module output\n}\n\noutput \"work_queue_dead_letter_queue\" {\n    value = module.work_queue.dead_letter_queue # access module output\n}\n</code></pre>"},{"location":"devops/terraform/resources/","title":"Terraform Resources","text":"<p>Terraform resources are components within a Terraform configuration that represent infrastructure objects or services that need to be managed.</p>"},{"location":"devops/terraform/resources/#terraform-resource-syntax","title":"Terraform resource syntax","text":"<pre><code>resource \"resource_type\" \"resource_name\" {\n  # Configuration settings for the resource\n  attribute1 = value1\n  attribute2 = value2\n  # ...\n}\n</code></pre> <p>For example:</p> <pre><code>resource \"aws_vpc\" \"my_vpc\" {\n    cidr_block = \"10.0.0.0/16\"\n}\n\nresource \"aws_security_group\" \"my_security_group\" {\n    vpc_id = aws_vpc.my_vpc.id\n    name = \"Example security group\"\n}\n\nresource \"aws_security_group_rule\" \"tls_in\" {\n    protocol = \"tcp\"\n    security_group_id = aws_security_group.my_security_group.id\n    from_port = 443\n    to_port = 443\n    type = \"ingress\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n}\n</code></pre>"},{"location":"devops/terraform/resources/#resource-types","title":"Resource Types","text":"<p>Each resource is associated with a single resource type, which determines the kind of infrastructure object it manages and what arguments and other attributes the resource supports.</p>"},{"location":"devops/terraform/resources/#references","title":"References","text":"<ul> <li>https://developer.hashicorp.com/terraform/language/resources</li> </ul>"},{"location":"devops/terraform/terraform/","title":"Terraform","text":""},{"location":"devops/terraform/terraform/#terraform-lifecycle","title":"Terraform Lifecycle","text":"<p>Terraform lifecycle consists of \u2013 init, plan, apply, and destroy.</p> <ol> <li>Terraform init initializes the (local) Terraform environment. Usually executed only once per session.</li> <li>Terraform plan compares the Terraform state with the as-is state in the cloud, build and display an execution plan. This does not change the deployment (read-only).</li> <li>Terraform apply executes the plan. This potentially changes the deployment.</li> <li>Terraform destroy deletes all resources that are governed by this specific terraform environment.</li> </ol>"},{"location":"devops/terraform/terraform/#terraform-configuration-files","title":"Terraform Configuration Files","text":"<ol> <li>Configuration file (*.tf files): Here we declare the provider and resources to be deployed along with the type of resource and all resources specific settings</li> <li>Variable declaration file (variables.tf or variables.tf.json): Here we declare the input variables required to provision resources</li> <li>Variable definition files (terraform.tfvars): Here we assign values to the input variables</li> <li>State file (terraform.tfstate): a state file is created once after Terraform is run. It stores state about our managed infrastructure.</li> </ol>"},{"location":"devops/terraform/terraform/#terraform-providers","title":"Terraform Providers","text":"<p>A provider is responsible for understanding API interactions and exposing resources. It is an executable plug-in that contains code necessary to interact with the API of the service. Terraform configurations must declare which providers they require so that Terraform can install and use them.</p> <p> </p> <pre><code># https://developer.hashicorp.com/terraform/language/providers/requirements\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.22\"\n    }\n  }\n}\n\n# provider configuration\n# https://developer.hashicorp.com/terraform/language/providers/configuration \nprovider \"aws\" {\n  region = \"ap-southeast-2\"\n}\n</code></pre> <p>In the Terraform block above, we have defined a <code>required_providers</code> block. This required provider block allows us to specify extra properties for each of the providers we are using in the project. Under required providers, we open an AWS block. There, we specify a version constraint for the AWS provider.</p> <p>List of supported providers can be found here</p>"},{"location":"devops/terraform/terraform/#backend-configuration","title":"Backend Configuration","text":"<p>A backend defines where Terraform stores its state data files. Terraform uses persisted state data to keep track of the resources it manages. </p> <p>By default, Terraform uses a backend called <code>local</code>, which stores state as a local file on disk. You can also configure one of the built-in backends supported by terraform.</p> <pre><code>terraform {\n  backend \"s3\" {\n    bucket = \"mybucket\"\n    key    = \"path/to/my/key\"\n    region = \"us-east-1\"\n  }\n}\n</code></pre>"},{"location":"devops/terraform/terragrunt/","title":"Terragrunt","text":""},{"location":"devops/terraform/terragrunt/#how-to-set-up-terragrunt-configurations","title":"How to set up Terragrunt configurations","text":"<p>Terragrunt configuration is defined in a <code>terragrunt.hcl</code> file.</p> <pre><code>include {\n  path = \"../../env/${get_aws_account_id()}-terragrunt.hcl\"\n}\n</code></pre>"},{"location":"devops/terraform/terragrunt/#terragrunt-use-cases","title":"Terragrunt use cases","text":""},{"location":"devops/terraform/terragrunt/#keep-your-terraform-code-dry","title":"Keep your Terraform code DRY","text":"<p>Consider the following file structure, which defines two environments (prod, stage) with the same infrastructure in each one (an app, a MySQL database):</p> <pre><code>\u2514\u2500\u2500 live\n    \u251c\u2500\u2500 prod\n    \u2502   \u251c\u2500\u2500 app\n    \u2502   \u2502   \u2514\u2500\u2500 main.tf\n    \u2502   \u2514\u2500\u2500 mysql\n    \u2502       \u2514\u2500\u2500 main.tf\n    \u2514\u2500\u2500 stage\n        \u251c\u2500\u2500 app\n        \u2502   \u2514\u2500\u2500 main.tf\n        \u2514\u2500\u2500 mysql\n            \u2514\u2500\u2500 main.tf\n</code></pre> <p>The contents of each environment will be more or less identical, except perhaps for a few settings (e.g. the prod environment may run bigger or more servers)</p> <p>Remote Terraform configurations</p> <p>Terragrunt has the ability to download remote Terraform configurations. The idea is that you define the Terraform code for your infrastructure just once, in a single repo, called, for example, modules:</p> <pre><code>\u2514\u2500\u2500 modules\n    \u251c\u2500\u2500 app\n    \u2502   \u2514\u2500\u2500 main.tf\n    \u2514\u2500\u2500 mysql\n        \u2514\u2500\u2500 main.tf\n</code></pre> <p>This repo contains typical Terraform code, with one difference: anything in your code that should be different between environments should be exposed as an input variable. For example, the app module might expose the following variables:</p> <pre><code>variable \"instance_count\" {\n  description = \"How many servers to run\"\n}\n\nvariable \"instance_type\" {\n  description = \"What kind of servers to run (e.g. t2.large)\"\n}\n</code></pre> <p>In a separate repo, called, for example, live, you define the code for all of your environments, which now consists of just one terragrunt.hcl file per component (e.g. app/terragrunt.hcl, mysql/terragrunt.hcl, etc). This gives you the following file layout:</p> <pre><code>\u2514\u2500\u2500 live\n    \u251c\u2500\u2500 prod\n    \u2502   \u251c\u2500\u2500 app\n    \u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl\n    \u2502   \u2514\u2500\u2500 mysql\n    \u2502       \u2514\u2500\u2500 terragrunt.hcl\n    \u2514\u2500\u2500 stage\n        \u251c\u2500\u2500 app\n        \u2502   \u2514\u2500\u2500 terragrunt.hcl\n        \u2514\u2500\u2500 mysql\n            \u2514\u2500\u2500 terragrunt.hcl\n</code></pre> <p>For example, stage/app/terragrunt.hcl may look like this:</p> <pre><code>terraform {\n  # Deploy version v0.0.3 in stage\n  source = \"git::git@github.com:foo/modules.git//app?ref=v0.0.3\"\n}\n\ninputs = {\n  instance_count = 3\n  instance_type  = \"t2.micro\"\n}\n</code></pre> <p>DRY common Terraform code with Terragrunt generate blocks</p> <p>Terragrunt has the ability to generate code in to the downloaded remote Terraform modules before calling out to terraform using the generate block. This can be used to inject common terraform configurations into all the modules that you use.</p> <pre><code>generate \"providers\" {\n  path      = \"providers.tf\"\n  if_exists = \"overwrite\"\n  contents  = &lt;&lt;EOF\nterraform {\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; ${local.aws_provider_version}\"\n    }\n  }\n\n  backend \"s3\" {}\n}\nEOF\n}\n</code></pre> <p>This instructs Terragrunt to create the file <code>provider.tf</code> in the working directory (where Terragrunt calls terraform) before it calls any of the Terraform commands (e.g <code>plan</code>, <code>apply</code>, <code>validate</code>, etc). This allows you to inject this provider configuration in all the modules that includes the root file.</p> <p>To include this in the child configurations (e.g mysql/terragrunt.hcl), you would update all the child modules to include this configuration using the include block:</p> <pre><code>include \"root\" {\n  path = find_in_parent_folders()\n}\n</code></pre> <p>Create remote state and locking resources automatically</p> <p>The generate block is useful for allowing you to setup the remote state backend configuration in a DRY manner, but this introduces a bootstrapping problem: how do you create and manage the underlying storage resources for the remote state? For example, when using the s3 backend, Terraform expects the S3 bucket to already exist for it to upload the state objects.</p> <p>Ideally you can manage the S3 bucket using Terraform, but what about the state object for the module managing the S3 bucket? How do you create the S3 bucket, before you run terraform, if you need to run terraform to create the bucket?</p> <p>To handle this, Terragrunt supports a different block for managing the backend configuration: the remote_state block.</p> <pre><code>remote_state {\n  backend = \"s3\"\n  generate = {\n    path      = \"backend.tf\"\n    if_exists = \"overwrite\"\n  }\n  config = {\n    bucket         = \"my-terraform-state\"\n    key            = \"${path_relative_to_include()}/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"my-lock-table\"\n  }\n}\n</code></pre> <p>Like the approach with <code>generate</code> blocks, this will generate a <code>backend.tf</code> file that contains the remote state configuration. However, in addition to that, terragrunt will also now manage the S3 bucket and DynamoDB table for you. This means that if the S3 bucket <code>my-terraform-state</code> and DynamoDB table <code>my-lock-table</code> does not exist in your account, Terragrunt will automatically create these resources before calling terraform and configure them based on the specified configuration parameters.</p>"},{"location":"distributed-system/api_gateway/","title":"API Gateway","text":"<p>An API gateway is a service which is the entry point into the application from the outside world and acts as a single point of entry for a defined group of microservices.</p> <p>In high level It\u2019s responsible for request routing, API composition, and other functions, such as authentication.</p> <p>Functionalities of API Gateway:</p> <ul> <li>Authentication and authorization</li> <li>Load balancing</li> <li>Service discovery integration</li> <li>Protocol translation</li> <li>Response caching</li> <li>Retry policies, circuit breaker, and QoS</li> <li>Rate limiting and throttling</li> <li>Logging, tracing, correlation</li> <li>Headers, query strings, and claims transformation</li> <li>IP whitelisting</li> <li>IAM</li> <li>Centralized Logging (transaction ID across the servers, error logging)</li> <li>Identity Provider, Authentication and Authorization</li> </ul> <p>List of notable API Gateways:</p> <ul> <li>Zuul</li> <li>Amazon API Gateway</li> <li>Kong Gateway</li> </ul>"},{"location":"distributed-system/api_gateway/#benefits-of-api-gateways","title":"Benefits of API gateways","text":"<ul> <li>Decoupling: If your clients which you have no control over communicated directly with many separate services, renaming or moving those services can be challenging as the client is coupled to the underlying architecture and organization. API gateways enables you to route based on path, hostname, headers, and other key information enabling you to decouple the publicly facing API endpoints from the underlying microservice architecture.</li> <li>Reduce Round Trips: Certain API endpoints may need to join data across multiple services. API gateways can perform this aggregation so that the client doesn\u2019t not need complicated call chaining and reduce number of round trips.</li> <li>Security: API gateways provide a centralized proxy server to manage rate limiting, bot detection, authentication, CORS, among other things. Many API gateways allow setting up a datastore such as Redis to store session information.</li> <li>Cross Cutting Concerns: Logging, Caching, and other cross cutting concerns can be handled in a centralized appliances rather than deployed to every microservice.</li> </ul>"},{"location":"distributed-system/api_gateway/#backend-for-frontend-bff-pattern","title":"Backend for Frontend (BFF) pattern","text":"<p>It is a variation of the API Gateway pattern. Rather than a single point of entry for the clients, it provides multiple gateways based upon the client. The purpose is to provide tailored APIs according to the needs of the client, removing a lot of bloats caused by making generic APIs for all the clients.</p> <p></p> <p>Why BFF:</p> <ul> <li>Decoupling of Backend and Frontend for sure gives us faster time to market as frontend teams can have dedicated backend teams serving their unique needs.   The release of new features of one frontend does not affect the other.</li> <li>We can much easier maintain and modify APIs and even provide API versioning dedicated for specific frontend, which is a big plus from a mobile app perspective as many users do not update the app immediately.</li> <li>The BFF can benefit from hiding unnecessary or sensitive data before transferring it to the frontend application interface, so keys and tokens for 3rd party services can be stored and used from BFF.</li> <li>Allows to send formatted data to frontend and because of that can minimalize logic on it.</li> <li>Additionally, give us possibilities for performance improvements and good optimization for mobile.</li> </ul>"},{"location":"distributed-system/causal_consistency/","title":"Causal Consistency (/\u02c8k\u0251\u02d0.z\u0259l/)","text":"<p>The easiest way to explain Causal Consistency is with an example:</p> <p>Suppose you just placed an order on an e-commerce platform. You get a <code>success</code> message.</p> <p>Then you go to the <code>all orders</code> page, and you don\u2019t see the one you just placed.</p> <p>That would be very irritating, right? Did the order actually go through? Did you get charged? Should you order again?</p> <p>Then, right when you\u2019re on the phone calling the support center, you reload the page, and the order suddenly appears.</p> <p>So what happened?</p> <p>Eventual Consistency Wasn\u2019t Enough</p> <p>Here\u2019s one possible scenario.</p> <p>The initial submission of the order went to a database server that accepts writes \u2013 a Primary server. However, when you requested to read all your orders, you were redirected to a Secondary replica that\u2019s still behind the Primary, e.g., your order is still not replicated. At some point, the order gets copied over to the Secondary, and you see it after a page refresh.</p> <p>You need stronger guarantees that cover causal (happens-before) relations.</p> <p>Transitioning to Causal Consistency</p> <p>The activities of submitting your order and then reviewing it are causally related. If your Write request (submit the order) successfully went through, your subsequent Read request (read the orders) should return it.</p> <p>Eventual Consistency doesn\u2019t deal with that.</p>"},{"location":"distributed-system/causal_consistency/#read-your-writes-consistency","title":"Read-your-writes Consistency","text":"<p>Read-your-writes consistency is a consistency model that ensures that once a write operation has been performed, any subsequent read operations by the same user or process will always return the updated value.</p>"},{"location":"distributed-system/causal_consistency/#monotonic-reads-consistency","title":"Monotonic Reads Consistency","text":"<p>Monotonic reads consistency is a consistency model that guarantees that if a user or process reads the latest version of a data item, all subsequent reads from that user or process will return at least the same version or a more recent version of the data. Without a Monotonic Reads guarantee, the Client might get a feeling he\u2019s moving back in time.</p>"},{"location":"distributed-system/causal_consistency/#monotonic-writes","title":"Monotonic Writes","text":"<p>On social media, a user may want to make his album private before uploading a new picture.</p> <p>In this case, there are two Write operations \u2013 one for updating the album access policy and one for the photo upload.</p> <p>Imagine what will happen if these two writes are somehow applied in the opposite order \u2013 first, the photo is uploaded, and then the album is made private.</p> <p>In that case, the uploaded photo will be public for some time which was never the intention.</p> <p>Even worse, what if the <code>privacy Write</code> gets lost forever?</p> <p>Let\u2019s review such scenarios.</p>"},{"location":"distributed-system/causal_consistency/#multiple-primary-servers","title":"Multiple Primary Servers","text":""},{"location":"distributed-system/causal_consistency/#rollback-during-a-failover","title":"Rollback During a Failover","text":""},{"location":"distributed-system/causal_consistency/#writes-follow-reads","title":"Writes Follow Reads","text":"<p>Imagine you read a blog post, and you go to the comments section.</p> <p>You read a comment by some Client A \u2013 let\u2019s say he\u2019s asking a question.</p> <p>You know the answer, so you write your own comment in response to the one by Client A.</p> <p>Now, imagine a scenario when some Client C goes to the comments and sees only yours but not the one by Client A.</p> <p>That would be pretty confusing. Your comment alone doesn\u2019t make any sense. It\u2019s supposed to answer a question that is now missing.</p>"},{"location":"distributed-system/causal_consistency/#references","title":"References","text":"<p>Causal Consistency Guarantees \u2013 Case Studies</p>"},{"location":"distributed-system/cohesion_and_coupling/","title":"Cohesion and coupling","text":""},{"location":"distributed-system/cohesion_and_coupling/#cohesion","title":"Cohesion","text":"<p>Cohesion is the degree to which the elements inside a microservice belong together.</p> <p>A microservice with high cohesion contains elements that are tightly related to each other and united in their purpose.</p>"},{"location":"distributed-system/cohesion_and_coupling/#coupling","title":"Coupling","text":"<p>Coupling is the degree of interdependence between microservices. When services are loosely coupled, a change to one service should not require a change to another.</p>"},{"location":"distributed-system/cohesion_and_coupling/#type-of-coupling","title":"Type of coupling","text":""},{"location":"distributed-system/cohesion_and_coupling/#domain-coupling","title":"Domain coupling","text":"<p>Domain coupling describes a situation in which one microservice needs to interact with another microservice, because the first microservice needs to make use of the functionality that the other microservice provides.</p> <p>In microservice, this type of interaction is largely unavoidable. A microservice-based system relies on multiple microservices collaborating in order for it to do its work. We still want to keep this to a minimum, though; whnever you see a single micorservice depending on multiple downstream services in this way, it can be a cause for concern.</p>"},{"location":"distributed-system/cohesion_and_coupling/#pass-through-coupling","title":"Pass through coupling","text":"<p>Pass through coupling describes a situation in which one micorservice passes data to another microservice purely because the data is needed by some other micorservice further downstream.</p> <p>The major issue with pass-through coupling is that a change to the required data downstream can cause a more significant upstream change.</p> <p></p>"},{"location":"distributed-system/cohesion_and_coupling/#common-coupling","title":"Common coupling","text":"<p>Common coupling occurs when 2 or more microservices make use of a common database.</p> <p></p>"},{"location":"distributed-system/cohesion_and_coupling/#content-coupling","title":"Content coupling","text":"<p>Content coupling describes a situation in which an upstream service reaches into the internals of a downstream service and changes its internal state.</p> <p>The most common manifestation of this is an external service accessing another microservice's database and changing it directly. The differences between content coupling and common coupling are subtle. In both cases, 2 or more microservices are reading and writing to the same database.</p> <ul> <li>With common coupling, you understand that you are making use of a shared database. You know it's not under your control.</li> <li>With content coupling, t he lines of wonership become less clear, and it becomes more difficult for developers to change a system.</li> </ul> <p></p>"},{"location":"distributed-system/distributed_system/","title":"Distributed System","text":""},{"location":"distributed-system/distributed_system/#1-cap-theorem","title":"1. CAP Theorem","text":"<p>In a distributed computer system, you can only support two of the following guarantees:</p> <ul> <li>Consistency: means that all clients see the same data at the same time, no matter which node they connect to. For this to happen, whenever data is written to one node, it must be instantly forwarded or replicated to all the other nodes in the system before the write is deemed <code>successful</code></li> <li>Availability: means that every request gets a (non-error) response regardless of the individual state of a node. This does not guarantee that the response contains the most recent write</li> <li>Partition Tolerance: means that the cluster must continue to work despite any number of communication breakdowns between nodes in the system</li> </ul> <p></p> <p>Networks aren't reliable, so you'll need to support partition tolerance (the P of CAP). That leaves a decision between the other two, C and A. When a network failure happens, one can choose to guarantee consistency or availability</p> <ul> <li>AP: When availability is chosen over consistency, the system is will always process the client request and try to return the most recent available version of the information even if it cannot guarantee it is up to date due to network partitioning. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors.</li> <li>CP: When consistency is chosen over availability, the system will return an error or time-out if particular information cannot be updated to other nodes due to network partition or failures. CP is a good choice if your business needs require atomic reads and writes.</li> </ul> <p>Database system designed with ACID guarantees (RDBMS) usually chooses consistency over availability whereas system Designed with BASE guarantees, chooses availability over consistency.</p>"},{"location":"distributed-system/distributed_system/#2-trade-offs","title":"2. Trade-offs","text":""},{"location":"distributed-system/distributed_system/#performance-vs-scalability","title":"Performance vs scalability","text":"<p>A service is scalable if it results in increased performance in a manner proportional to the resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.</p> <p>Another way to look at performance vs scalability:</p> <ul> <li>If you have a performance problem, your system is slow for a single user.</li> <li>If you have a scalability problem, your system is fast for a single user but slow under heavy load.</li> </ul> <p>A system has to sacrifice performance for scalability.</p> <p>For example you have a stock exchange engine running on an API. At first you run it in one location and the performance is fast. But then you start getting customers at the other end of the world and their latency numbers are bad. If you choose to spin up and api instance that is geographically closer to those customers (scale), the latency is shortened for them but now your apis must synchronize between instances. This synchronization will cost you time and extra computation steps, lowering your performance.</p> <p>http://highscalability.com/blog/2011/2/10/database-isolation-levels-and-their-effects-on-performance-a.html</p>"},{"location":"distributed-system/distributed_system/#latency-vs-throughput","title":"Latency vs throughput","text":"<ul> <li> <p>Latency: is the time to perform some action or produce some result</p> </li> <li> <p>Network latency</p> </li> <li> <p>Processing latency</p> </li> <li> <p>Throughput: is the number of such actions executed or results produced per unit of time (number of requests that servers can handle in the certain time)</p> </li> </ul> <p>Generally, you should aim for maximal throughput with acceptable latency.</p> <p>For example:</p> <p>An assembly line is manufacturing cars. It takes 8 hours to manufacture a car and that the factory produces 120 cars per day.</p> <ul> <li>The latency is: 8 hours.</li> <li>The throughput is: 120 cars / day or 5 cars / hour.</li> </ul> <p>Sometimes latency and throughput interfere with each other. Buses might deliver more people per hour than individually hailed cars (higher throughput), but it takes me personally longer to get downtown because I have to walk to a bus stop and wait for the bus (higher latency).</p> <p>Source(s) and further reading:</p> <p>https://medium.com/@kentbeck_7670/inefficient-efficiency-5b3ab5294791</p>"},{"location":"distributed-system/distributed_system/#availability-vs-consistency","title":"Availability vs consistency","text":"<p>Networks aren\u2019t reliable, so you\u2019ll need to support partition tolerance. According to CAP Theorem, you\u2019ll need to make a software tradeoff between consistency and availability.</p> <ul> <li>Consistency: Every read receives the most recent write or an error</li> <li>Availability: Every request receives a response, without guarantee that it contains the most recent version of the information</li> </ul>"},{"location":"distributed-system/distributed_system/#3-consistency-patterns","title":"3. Consistency patterns","text":"<p>With multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data. Recall the definition of consistency from the CAP theorem - Every read receives the most recent write or an error.</p>"},{"location":"distributed-system/distributed_system/#weak-consistency","title":"Weak consistency","text":"<p>After a write, reads may or may not see it. A best effort approach is taken. This approach is seen in systems such as memcached. Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games. For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.</p>"},{"location":"distributed-system/distributed_system/#eventual-consistency","title":"Eventual consistency","text":"<p>After a write, reads will eventually see it (typically within milliseconds). Data is replicated asynchronously. This approach is seen in systems such as DNS and email. Eventual consistency works well in highly available systems.</p>"},{"location":"distributed-system/distributed_system/#strong-consistency","title":"Strong consistency","text":"<p>After a write, reads will see it. Data is replicated synchronously. This approach is seen in file systems and RDBMSes. Strong consistency works well in systems that need transactions.</p>"},{"location":"distributed-system/http/","title":"HTTP","text":""},{"location":"distributed-system/http/#an-overview-of-http","title":"An overview of HTTP","text":"<p>Hypertext Transfer Protocol (HTTP) is an application-layer protocol for transmitting hypermedia documents, such as HTML. It was designed for communication between web browsers and web servers, but it can also be used for other purposes.</p> <p>Basic aspects of HTTP:</p> <ul> <li>HTTP is media independent: It means, any type of data can be sent by HTTP as long as both the client and the server know how to handle the data content</li> <li>HTTP is stateless: The server and client are aware of each other only during a current request. Afterwards, both of them forget about each other.</li> </ul>"},{"location":"distributed-system/http/#htpp-flow","title":"Htpp flow","text":"<p>When a client wants to communicate with a server, either the final server or an intermediate proxy, it performs the following steps:</p> <ol> <li>Open a TCP connection</li> <li>Send an HTTP message (HTTP/1 is human-readable but HTTP/2 is not)</li> </ol> <p>Eg:</p> <pre><code>GET / HTTP/1.1\nHost: developer.mozilla.org\nAccept-Language: fr\n</code></pre> <ol> <li>Read the response sent by the server, such as:</li> </ol> <pre><code>HTTP/1.1 200 OK\nDate: Sat, 09 Oct 2010 14:28:02 GMT\nServer: Apache\nLast-Modified: Tue, 01 Dec 2009 20:18:22 GMT\nETag: \"51142bc1-7449-479b075b2891b\"\nAccept-Ranges: bytes\nContent-Length: 29769\nContent-Type: text/html\n\n&lt;!DOCTYPE html... (here comes the 29769 bytes of the requested web page)\n</code></pre> <ol> <li>Close or reuse the connection for further requests.</li> </ol>"},{"location":"distributed-system/http/#http-messsage","title":"HTTP messsage","text":"<p>HTTP messages, as defined in HTTP/1.1 and earlier, are human-readable. In HTTP/2, these messages are embedded into a binary structure, a frame, allowing optimizations like compression of headers and multiplexing.</p>"},{"location":"distributed-system/http/#http-request","title":"HTTP Request","text":"<p>Requests consists of the following elements:</p> <ul> <li>An HTTP method (GET, POST, PUT, OPTIONS, HEAD...)</li> <li>A path of resource to fetch</li> <li>The version of HTTP protocol</li> <li>HTTP request headers that convey additional information for the servers</li> <li>Optionally, a body, for some methods like POST, which contain the resource sent</li> </ul>"},{"location":"distributed-system/http/#http-response","title":"HTTP Response","text":"<p>Responses consist of the following elements:</p> <ul> <li>The version of the HTTP protocol</li> <li>A status code</li> <li>A status message, a non-authoritative short description of the status code</li> <li>HTTP response headers</li> <li>Optionally, a body containing the fetched resource.</li> </ul>"},{"location":"distributed-system/http/#http-headers","title":"HTTP headers","text":"<p>HTTP headers allow the client and the server to pass additional information with the request or the response</p> <p>An HTTP header consists of its case-insensitive name followed by a colon (:), then by its value. Whitespace before the value is ignored.</p> <p>Headers can be grouped according to their contexts:</p> <ul> <li>General headers applying to both requests and responses but with no relation to the data eventually transmitted in the body</li> <li>Request headers contain more information about the resource to be fetched, or about the client requesting the resource</li> <li>Response headers hold additional information about the response, like its location or about the server providing it</li> <li>Representation headers contain information about the body of the resource, like its MIME type, or encoding/compression applied</li> </ul> <p>Example:</p> <ul> <li>Cookie: Contains stored HTTP cookies previously sent by the server with the Set-Cookie header.</li> <li>Set-Cookie: Send cookies from the server to the user-agent.</li> <li>Origin: Indicates where a request originates from</li> <li>Authorization: Contains the credentials to authenticate a user-agent with a server</li> <li>Access-Control-Allow-Origin: Indicates whether the response can be shared</li> <li>...</li> </ul>"},{"location":"distributed-system/http/#http-status-code","title":"HTTP status code","text":"<p>HTTP status code are like short notes from server to client. They are messsages from the server letting clients konw how things went when it received requests.</p> <p>HTTP status codes are grouped in five classed:</p> <ol> <li>Information responses (100- 199)</li> <li>Successful responses (200, 299)</li> <li>Redirect (300-399)</li> <li>Client errors (400-499)</li> <li>Server errors (500-599)</li> </ol>"},{"location":"distributed-system/http/#http-pull","title":"HTTP pull","text":"<p>In the HTTP pull method, the client sends a request to the server and the server responds to that request (and the connection is closed). The client pulls the data from the server whenever it requires (by creating a new connection). And it keeps doing it over and over to fetch the updated data.</p> <p>This is the default HTTP communication method and is extensively on the Internet for fetching HTTP pages from websites.</p> <p>The disadvantage of the HTTP pull method is that if clients keep on periodically makes the pull request for updated data, but there are no updates at the server hence, every time the client will get the same result, bandwidth will be wasted and the server will be busy too. Also, excessive pulls by the clients have the potential to bring down the server.</p> <p></p>"},{"location":"distributed-system/http/#http-push","title":"HTTP push","text":"<p>To overcome the problem with HTTP pull, an HTTP push was introduced. In the HTTP push method, the client opens a connection to the server by requesting a server only the first time and after that server keeps on pushing back updated content to the client, whenever there\u2019s any.</p> <p>There are multiple technologies involved in the HTTP push based mechanism such as:</p> <ul> <li>Ajax Long polling</li> <li>Web Sockets</li> <li>HTML5 Event Source</li> <li>Message Queues</li> <li>Streaming over HTTP</li> </ul> <p></p>"},{"location":"distributed-system/microservice_communication/","title":"Micro-service communication","text":""},{"location":"distributed-system/microservice_communication/#communication-types","title":"Communication types","text":""},{"location":"distributed-system/microservice_communication/#synchronous","title":"Synchronous","text":"<p>A microservice makes a call to another microservice and blocks operation waiting for the response.</p>"},{"location":"distributed-system/microservice_communication/#advantages","title":"Advantages","text":"<ul> <li>Simple</li> </ul>"},{"location":"distributed-system/microservice_communication/#disadvantages","title":"Disadvantages","text":"<ul> <li>Temporal coupling may occur</li> <li>Vulnerable to cascading issues caused by downstream outages</li> </ul>"},{"location":"distributed-system/microservice_communication/#asynchronous","title":"Asynchronous","text":"<p>The act of sending a call out over the network doesn\u2019t block the microservice issuing the call. It is able to carry on with any other processing without having to wait for a response.</p>"},{"location":"distributed-system/microservice_communication/#advantages_1","title":"Advantages","text":"<ul> <li>Good when dealing with time-consuming tasks</li> <li>Avoid temporal coupling</li> <li>Loosely coupling</li> </ul>"},{"location":"distributed-system/microservice_communication/#disadvantages_1","title":"Disadvantages","text":"<ul> <li>Complexity</li> </ul>"},{"location":"distributed-system/microservice_communication/#communication-styles","title":"Communication styles","text":""},{"location":"distributed-system/microservice_communication/#requestresponse-communication","title":"Request/response communication","text":"<p>A microservice sends a request to another microservice asking for something to be done. It expects to receive a response informing it of the result.</p>"},{"location":"distributed-system/microservice_communication/#event-driven","title":"Event driven","text":"<p>Microservices emit events, which other microservices consume and react to accordingly. The microservice emitting the event is unaware of which microservices, if any, consume the events it emits.</p>"},{"location":"distributed-system/microservice_communication/#common-data","title":"Common data","text":"<p>Not often seen as a communication style, microservices collaborate via some shared data source.</p>"},{"location":"distributed-system/microservice_communication/#technology-choices","title":"Technology choices","text":""},{"location":"distributed-system/microservice_communication/#rest","title":"REST","text":"<p>REST (Representational State Transfer) is an architectural style that provides guidelines for designing web APIs.</p> <p>REST itself doesn\u2019t really talk about underlying protocols, although it is most commonly used over HTTP. We can implement REST using different protocols, although this can require a lot of work. Some of the features that HTTP gives us as part of the specification, such as verbs, make implementing REST over HTTP easier, whereas with other protocols you\u2019ll have to handle these features yourself.</p> <p>Where to use:</p> <p>REST-over-HTTP-based API is an obvious choice for a synchronous request-response interface if you are looking to allow access from as wide a variety of clients as possible</p>"},{"location":"distributed-system/microservice_communication/#remote-procedure-call","title":"Remote procedure call","text":"<p>Remote procedure call (RPC) refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different RPC implementations in use. Most of the technology in this space requires an explicit schema, such as SOAP or gRPC. In this note, we only focus on gRPC</p> <p>gRPC stands for Google Remote Procedure Call and is a variant based on the RPC architecture. This technology follows an RPC API's implementation that uses HTTP 2.0 protocol</p> <p>gRPC uses Protocol Buffer by default to serialize payload data.</p>"},{"location":"distributed-system/microservice_communication/#rest-vs-grpc","title":"REST vs gRPC","text":"<p>Guidelines vs. Rules</p> <p>REST is a set of guidelines for designing web APIs without enforcing anything.</p> <p>gRPC enforces rules by defining a .proto file that must be adhered to by both client and server for data exchange.</p> <p>Underlying HTTP Protocol</p> <p>REST provides a request-response communication model built on the HTTP 1.1 protocol. Therefore, when multiple requests reach the server, it is bound to handle each of them, one at a time which consequently slows the entire system.</p> <p>gRPC follows a client-response model of communication for designing web APIs that rely on HTTP/2. Hence, gRPC allows streaming communication and serves multiple requests simultaneously. In addition to that, gRPC also supports unary communication similar to REST.</p> <p></p> <p>Data Exchange Format</p> <p>REST typically uses JSON and XML formats for data transfer</p> <p>gRPC relies on Protobuf for an exchange of data over the HTTP/2 protocol.</p> <p>The gRPC is based on binary protocol-buffer format which is very lightweight compared to text-based formats such as JSON. For example, for below payload JSON encoding takes 81 bytes and the same payload in protocol-buffers takes 33 bytes</p> <p>Serialization vs. Strong Typing</p> <p>REST, in most cases, uses JSON or XML that requires serialization and conversion into the target programming language for both client and server, thereby increasing response time and the possibility of errors while parsing the request/response.</p> <p>gRPC provides strongly typed messages automatically converted using the Protobuf exchange format to the chosen programming language.</p> <p>Latency</p> <p>REST utilizing HTTP 1.1 requires a TCP handshake for each request. Hence, REST APIs with HTTP 1.1 can suffer from latency issues.</p> <p>gRPC relies on HTTP/2 protocol, which uses multiplexed streams. Therefore, several clients can send multiple requests simultaneously without establishing a new TCP connection for each one. Also, the server can send push notifications to clients via the established connection.</p> <p>Browser Support</p> <p>REST APIs on HTTP 1.1 have universal browser support.</p> <p>gRPC has limited browser support because numerous browsers</p> <p>Code Generation Features</p> <p>REST provides no built-in code generation features. However, we can use third-party tools like Swagger or Postman to produce code for API requests.</p> <p>gRPC, using its protoc compiler, comes with native code generation features, compatible with several programming languages.</p> <p>Where to use:</p> <ul> <li>Great choice for inter-process communication in microservices applications (Not only the gRPC services are faster compared to RESTful services but also they are strongly typed)</li> <li>IoT systems that require light-weight message transmission</li> <li>Mobile applications with no browser support</li> <li>Applications that need multiplexed streams.</li> </ul>"},{"location":"distributed-system/microservice_communication/#graphql","title":"GraphQL","text":"<p>GraphQL is a query language and server-side runtime for application programming interfaces (APIs) that prioritizes giving clients exactly the data they request and no more.</p> <p>With a REST API, you would typically gather the data by accessing multiple endpoints.</p> <p>In the example, these could be <code>/users/&lt;id&gt;</code> endpoint to fetch the initial user data. Secondly, there\u2019s likely to be a <code>/users/&lt;id&gt;/posts</code> endpoint that returns all the posts for a user. The third endpoint will then be the <code>/users/&lt;id&gt;/followers</code> that returns a list of followers per user.</p> <p></p> <p>In GraphQL on the other hand, you\u2019d simply send a single query to the GraphQL server that includes the concrete data requirements. The server then responds with a JSON object where these requirements are fulfilled.</p> <p></p> <p>Where to use:</p> <ul> <li>Apps for devices such as mobile phones, smartwatches, and IoT devices, where bandwidth usage matters.</li> <li>Applications where nested data needs to be fetched in a single call. (For example, a blog or social networking platform where posts need to be fetched along with nested comments and commenters details.)</li> <li>Composite pattern, where application retrieves data from multiple, different storage APIs (For example, a dashboard that fetches data from multiple sources such as logging services, backends for consumption stats, third-party analytics tools to capture end-user interactions.)</li> </ul>"},{"location":"distributed-system/microservice_communication/#message-brokers","title":"Message brokers","text":"<p>Message brokers are programs that enables services to communicate with each other and exchange information.</p>"},{"location":"distributed-system/microservice_communication/#point-to-point-queues","title":"Point-to-Point (Queues)","text":""},{"location":"distributed-system/microservice_communication/#publish-and-subscribe-topics","title":"Publish and Subscribe (Topics)","text":"<p>Where to use:</p> <ul> <li>Long-running tasks</li> <li>Data post-processing</li> </ul>"},{"location":"distributed-system/service_discovery/","title":"Service Discovery","text":""},{"location":"distributed-system/service_discovery/#why-use-service-discovery","title":"Why Use Service Discovery?","text":"<p>Let\u2019s imagine that you are writing some code that invokes a service that has a REST API. In order to make a request, your code needs to know the network location (IP address and port) of a service instance.</p> <p>In a modern, cloud\u2011based microservices application, however, this is a much more difficult problem to solve as shown in the following diagram.</p> <p></p> <p>Service instances have dynamically assigned network locations. Moreover, the set of service instances changes dynamically because of autoscaling, failures, and upgrades. Consequently, your client code needs to use a more elaborate service discovery mechanism.</p>"},{"location":"distributed-system/service_discovery/#the-clientside-discovery-pattern","title":"The Client\u2011Side Discovery Pattern","text":"<p>When using client\u2011side discovery, the client is responsible for determining the network locations of available service instances and load balancing requests across them. The client queries a service registry, which is a database of available service instances. The client then uses a load\u2011balancing algorithm to select one of the available service instances and makes a request.</p> <p></p> <p>The network location of a service instance is registered with the service registry when it starts up. It is removed from the service registry when the instance terminates. The service instance\u2019s registration is typically refreshed periodically using a heartbeat mechanism.</p> <p>Netflix Eureka is a service registry. It provides a REST API for managing service\u2011instance registration and for querying available instances. Netflix Ribbon is an IPC client that works with Eureka to load balance requests across the available service instances</p> <p>Pros:</p> <ul> <li>Since the client knows about the available services instances, it can make intelligent, application\u2011specific load\u2011balancing decisions such as using hashing consistently.</li> </ul> <p>Cons:</p> <ul> <li>It couples the client with the service registry</li> <li>Must implement client\u2011side service discovery logic in each programming language and framework used by your service</li> </ul>"},{"location":"distributed-system/service_discovery/#the-serverside-discovery-pattern","title":"The Server\u2011Side Discovery Pattern","text":"<p>The client makes a request to a service via a load balancer. The load balancer queries the service registry and routes each request to an available service instance. As with client\u2011side discovery, service instances are registered and deregistered with the service registry.</p> <p></p> <p>The AWS Elastic Load Balancer  is an example of a server-side discovery router. An ELB is commonly used to load balance external traffic from the Internet. However, you can also use an ELB to load balance traffic that is internal to a virtual private cloud (VPC). A client makes requests (HTTP or TCP) via the ELB using its DNS name. The ELB load balances the traffic among a set of registered Elastic Compute Cloud (EC2) instances or EC2 Container Service (ECS) containers. There isn\u2019t a separate service registry. Instead, EC2 instances and ECS containers are registered with the ELB itself.</p> <p>Pros:</p> <ul> <li>Details of discovery are abstracted away from the client (Clients simply make requests to the load balancer.)</li> <li>Unless the load balancer is provided by the deployment environment, it is yet another highly available system component that you need to set up and manage.</li> </ul>"},{"location":"distributed-system/service_discovery/#the-service-registry","title":"The Service Registry","text":"<p>The service registry is a key part of service discovery. It is a database containing the network locations of service instances. A service registry needs to be highly available and up to date. Clients can cache network locations obtained from the service registry. However, that information eventually becomes out of date and clients become unable to discover service instances.</p> <p>Netflix Eureka is good example of a service registry. It provides a REST API for registering and querying service instances.</p> <ul> <li>A service instance registers its network location using a POST request.</li> <li>Every 30 seconds it must refresh its registration using a PUT request.</li> <li>A registration is removed by either using an HTTP DELETE request or by the instance registration timing out.</li> <li>A client can retrieve the registered service instances by using an HTTP GET request.</li> </ul> <p>Other examples of service registries include:</p> <ul> <li>etcd - A highly available, distributed, consistent, key\u2011value store that is used for shared configuration and service discovery. Kubernetes is using etcd</li> <li>Apache Zookeeper - A widely used, high\u2011performance coordination service for distributed applications.</li> <li>consul</li> </ul>"},{"location":"distributed-system/service_discovery/#service-registration-options","title":"Service Registration Options","text":""},{"location":"distributed-system/service_discovery/#the-selfregistration-pattern","title":"The Self\u2011Registration Pattern","text":"<p>When using the self\u2011registration pattern, a service instance is responsible for registering and deregistering itself with the service registry. Also, if required, a service instance sends heartbeat requests to prevent its registration from expiring. A good example of this approach is the Netflix OSS Eureka client</p> <p></p> <p>Pros:</p> <ul> <li>Simple and does not require any other system components</li> </ul> <p>Cons:</p> <ul> <li>It couples the service instances to the service registry</li> <li>Must implement the registration code in each programming language and framework used by your service</li> </ul>"},{"location":"distributed-system/service_discovery/#the-thirdparty-registration-pattern","title":"The Third\u2011Party Registration Pattern","text":"<p>When using the third-party registration pattern, service instances aren\u2019t responsible for registering themselves with the service registry. Instead, another system component known as the service registrar handles the registration. The service registrar tracks changes to the set of running instances by either polling the deployment environment or subscribing to events. When it notices a newly available service instance it registers the instance with the service registry. The service registrar also deregisters terminated service instances.</p> <p></p> <p>The service registrar is a built\u2011in component of deployment environments. The EC2 instances created by an Autoscaling Group can be automatically registered with an ELB. Kubernetes services are automatically registered and made available for discovery.</p> <p>Pros:</p> <ul> <li>Services are decoupled from service registry.</li> <li>No need to implement service\u2011registration logic for each programming language and framework used by your developers.</li> </ul> <p>Cons:</p> <ul> <li>Unless it\u2019s built into the deployment environment, it is yet another highly available system component that you need to set up and manage.</li> </ul>"},{"location":"distributed-system/service_mesh/","title":"Service Mesh","text":"<p>A service mesh is a dedicated infrastructure layer that adds features to a network between services: - Service to service communication (service discovery and encryption) - Observability (monitoring and tracking) - Resiliency (circuit breakers and retries)</p> <p> </p> <p>Without a service mesh: each microservice implements business logic and cross cutting concerns (CCC) by itself. With a service mesh: many CCCs like traffic metrics, routing, and encryption are moved out of the microservice and into a proxy.</p>"},{"location":"distributed-system/service_mesh/#service-mesh-implementations","title":"Service Mesh Implementations","text":"<ul> <li>Istio</li> <li>Linkerd</li> <li>Consul</li> </ul>"},{"location":"distributed-system/kafka/consumer/","title":"Kafka consumer","text":"<p>Kafka producer is a component of the Kafka ecosystem which is used to pull messages from a Kafka topic.</p> <p></p> <p>A consumer always reads data from a lower offset to a higher offset and cannot read data backwards.</p> <p>If the consumer consumes data from more than one partition, the message order is not guaranteed across multiple partitions because they are consumed simultaneously, but the message read order is still guaranteed within each individual partition.</p>"},{"location":"distributed-system/kafka/consumer/#kafka-consumer-groups","title":"Kafka Consumer Groups","text":"<p>Kafka consumers are typically part of a consumer group.</p> <p>When multiple consumers are subscribed to a topic and belong to the same consumer group, each consumer in the group will receive messages from a different subset of the partitions in the topic.</p> <p>For example: Let\u2019s take topic <code>T1</code> with four partitions. Now suppose we created a new consumer, <code>C1</code>, which is the only consumer in group <code>G1</code>, and use it to subscribe to topic <code>T1</code>. Consumer <code>C1</code> will get all messages from all four <code>T1</code> partitions.</p> <p></p> <p>If we add another consumer, <code>C2</code>, to group <code>G1</code>, each consumer will only get messages from two partitions. Perhaps messages from partition <code>0</code> and <code>2</code> go to <code>C1</code>, and messages from partitions <code>1</code> and <code>3</code> go to consumer <code>C2</code></p> <p></p> <p>Note: If we add more consumers to a single group with a single topic than we have partitions, some of the consumers will be idle and get no messages at all</p>"},{"location":"distributed-system/kafka/consumer/#kafka-consumer-offsets","title":"Kafka Consumer Offsets","text":"<p>Kafka brokers use an internal topic named <code>__consumer_offsets</code> that keeps track of what messages a given consumer group last successfully processed.</p> <p>As we know, each message in a Kafka topic has a partition ID and an offset ID attached to it.</p> <p>Therefore, in order to checkpoint how far a consumer has been reading into a topic partition, the consumer will regularly commit the latest processed message, also known as consumer offset.</p>"},{"location":"distributed-system/kafka/consumer/#automatic-commit","title":"Automatic Commit","text":"<p>The easiest way to commit offsets is to allow the consumer to do it for you. If you configure <code>enable.auto.commit=true</code>, then every <code>auto.commit.interval.ms</code> the consumer will commit the latest offset that your client received from <code>poll()</code></p> <p>This method may lead to duplicate message processing.</p> <p>Use <code>auto.offset.reset</code> to define the behavior of the consumer when there is no committed position. You can choose either to reset the position to the <code>earliest</code> offset or the <code>latest</code> offset (the default). </p>"},{"location":"distributed-system/kafka/consumer/#commit-current-offset","title":"Commit Current Offset","text":"<p>By setting <code>enable.auto.commit=false</code>, offsets will only be committed when the application explicitly chooses to do so.</p> <p>There are 2 ways to commit the current offset:</p> <ul> <li>Synchronous: <code>commitSync()</code></li> <li>Asynchronous: <code>commitAsync</code></li> </ul> <p>It is important to remember that <code>commitSync()</code> and <code>commitAsync</code> will commit the latest offset returned by <code>poll()</code>, so if you call <code>commitSync()</code> or <code>commitAsync()</code> before you are done processing all the records in the collection, you risk missing the messages that were committed but not processed, in case the application crashes.</p> <p>Note: You can commit a specific offset in Kafka</p>"},{"location":"distributed-system/kafka/exactly_once_semantics/","title":"Exactly-Once Semantics","text":"<p>Exactly-once semantics in Kafka is a combination of two key features:</p> <ul> <li>Idempotent producers: which help avoid duplicates caused by producer retries</li> <li>Transactional semantics: which guarantee exactly-once processing in stream processing application.</li> </ul>"},{"location":"distributed-system/kafka/exactly_once_semantics/#idempotent-producer","title":"Idempotent Producer","text":"<p>A service is called idempotent if performing the same operation multiple times has the same result as performing it a single time.</p> <p>How messages can be duplicated in Kafka system?</p> <p>For example, when a partition leader received a record from the producer and replicated it successfully to the followers, and then the broker on which the leader resides crashed before it could send a response to the producer. The producer, after a certain time without a response, will resend the message. The message will arrive at the new leader, who already has a copy of the message from the previous attempt\u2014resulting in a duplicate.</p>"},{"location":"distributed-system/kafka/exactly_once_semantics/#how-does-the-idempotent-producer-work","title":"How Does the Idempotent Producer Work?","text":"<p>When we enable the idempotent producer, each message will include a unique identified producer ID (PID) and a sequence number. These, together with the target topic and partition, uniquely identify each message.</p> <p>On the broker side, on a per partition basis, it keeps track of the largest PID-Sequence Number combination it has successfully written. The broker will reject a producer request if its sequence number is not exactly one greater than the last committed message from that PID/TopicPartition pair</p> <p>Producer ID is a Unique Identifier assigned to the producer by the broker that is not exposed to users but is passed on every request to the broker.</p> <p>If <code>transactional.id</code> is not specified, a fresh PID is generated every-time on producer initialization.</p> <p>If <code>transactional.id</code> is specified,the broker stores mapping of Transactional ID to PID so that it can return the same PID on producer restart.</p>"},{"location":"distributed-system/kafka/exactly_once_semantics/#how-do-i-use-the-kafka-idempotent-producer","title":"How Do I Use the Kafka Idempotent Producer?:","text":"<p>This is the easy part. Add <code>enable.idempotence=true</code> to the producer configuration.</p>"},{"location":"distributed-system/kafka/exactly_once_semantics/#transactions","title":"Transactions","text":"<p>Transactions give us the ability to atomically update data in multiple topic partitions. All the records included in a transaction will be successfully saved, or none of them will be.</p> <p>Prerequisite:</p> <p>Topic must have <code>Replication Factor &gt;= 3</code> and <code>min.insync.replicas &gt;= 2</code></p>"},{"location":"distributed-system/kafka/exactly_once_semantics/#what-problems-do-transactions-solve","title":"What Problems Do Transactions Solve?","text":"<p>Consider a simple stream processing application: it reads events from a source topic, maybe processes them, and writes results to another topic. We want to be sure that for each message we process, the results are written exactly once. What can possibly go wrong?</p> <p>Reprocessing caused by application crashes</p> <p>After consuming a message from the source cluster and processing it, the application has to do two things:</p> <ul> <li>Produce the result to the output topic</li> <li>Commit the offset of the message that we consumed</li> </ul> <p>What happens if the application crashes after the output was produced but before the offset of the input was committed? After several heartbeats are missed, the application will be assumed dead and its partitions reassigned to another consumer in the consumer group. That consumer will begin consuming records from those partitions, starting at the last committed offset. This means that all the records that were processed by the application between the last committed offset and the crash will be processed again, and the results will be written to the output topic again\u2014resulting in duplicates.</p> <p>Reprocessing caused by zombie applications</p> <p>What happens if our application just consumed a batch of records from Kafka and then froze or lost connectivity to Kafka before doing anything else with this batch of records?</p> <p>After several heartbeats are missed, the application will be assumed dead and its partitions reassigned to another consumer in the consumer group. That consumer will reread that batch of records, process it. Meanwhile, the first instance of the application\u2014the one that froze\u2014may resume its activity: process the batch of records it recently consumed, and produce the results to the output topic.</p>"},{"location":"distributed-system/kafka/exactly_once_semantics/#what-problems-arent-solved-by-transaction","title":"What problems aren't solved by transaction","text":"<p>Side effects while streaming processing</p> <p>Let's say that the record processing step in our stream processing app includes sending email to users. Enabling exactly-once semantics will not guarantee that the email will only be sent once. The guarantee only applies to records written to Kafka.</p> <p>Reading from Kafka topic, and writing to database</p> <p>In this case, the application is writing to an external database rather than to Kafka. We have 2 steps here:</p> <ul> <li>Writing record to database</li> <li>Update an offset to Kafka within the consumer</li> </ul> <p>There is no mechanism that allows writing results to an external database and committing offset to Kafka within a single transaction. Instead we could manage offsets in the database and commit both data and offset to the database in a single transaction, this would rely on the database transaction guarantees rather than Kafka. Reading data from database, writing to Kafka, and from there writing to another database</p> <p>Copy data from one Kafka cluster to another</p> <p>Publish/subscribe pattern</p>"},{"location":"distributed-system/kafka/installation/","title":"Kafka installation","text":""},{"location":"distributed-system/kafka/installation/#installing-zookeeper","title":"Installing ZooKeeper","text":"<p>Apache Kafka uses Apache ZooKeeper to store metadata about the Kafka cluster, as well as consumer client details.</p> <p></p> <p>We can download and install ZooKeeper from this site. We can also using the docker image from here</p>"},{"location":"distributed-system/kafka/installation/#standalone-zookeeper-configuration","title":"Standalone ZooKeeper configuration","text":"<pre><code>tickTime=2000\ndataDir=/var/zookeeper\nclientPort=2181\n</code></pre> <ul> <li>tickTime: the basic time unit in milliseconds used by ZooKeeper</li> <li>dataDir: the location to store the in-memory database snapshots</li> <li>clientPort: the port to listen for client connections</li> </ul>"},{"location":"distributed-system/kafka/installation/#zookeeper-ensemble-configuration","title":"ZooKeeper ensemble configuration","text":"<p>ZooKeeper is designed to work as a cluster, called an ensemble, to ensure high availability.</p> <p>To configure ZooKeeper servers in an ensemble, they must have a common configuration that lists all servers, and each server needs a myid file in the data directory that specifies the ID number of the server</p> <p>If the hostnames of the servers in the ensemble are <code>zoo1.example.com</code>, <code>zoo2.example.com</code>, and <code>zoo3.example.com</code>, the configuration file might look like this:</p> <pre><code>tickTime=2000\ndataDir=/var/lib/zookeeper\nclientPort=2181\ninitLimit=20\nsyncLimit=5\nserver.1=zoo1.example.com:2888:3888\nserver.2=zoo2.example.com:2888:3888\nserver.3=zoo3.example.com:2888:3888\n</code></pre> <ul> <li>initLimit: is the amount of time to allow followers to connect with a leader</li> <li>syncLimit: limits how long out-of-sync followers can be with the leader.</li> </ul> <p>Both values are a number of <code>tickTime</code> units, which makes the <code>initLimit</code> 20 \u00d7 2,000 ms, or 40 seconds.</p> <p>The configuration also lists each server in the ensemble. The servers are specified in the format <code>server.X=hostname:peerPort:leaderPort</code>, with the following parameters:</p> <ul> <li>X: The ID number of the server. This must be an integer.</li> <li>hostname: The hostname or IP address of the server</li> <li>peerPort: The TCP port over which servers in the ensemble communicate with one another.</li> <li>leaderPort: The TCP port over which leader election is performed.</li> </ul> <p>Clients only need to be able to connect to the ensemble over the <code>clientPort</code>, but the members of the ensemble must be able to communicate with one another over all three ports</p>"},{"location":"distributed-system/kafka/installation/#installing-a-kafka-broker","title":"Installing a Kafka Broker","text":"<p>We can download and install Kafka from this site. We can also using the docker image from here</p>"},{"location":"distributed-system/kafka/installation/#broker-configurations","title":"Broker Configurations","text":"<p>broker.id</p> <p>Every Kafka broker must have an integer identifier, which is set using the <code>broker.id</code> configuration.</p> <p>zookeeper.connect</p> <p>The location of the ZooKeeper used for storing the broker metadata is set using the <code>zookeeper.connect</code> configuration parameter</p> <p>log.dirs</p> <p>Kafka persists all messages to disk, and these log segments are stored in the directory specified in the <code>log.dir</code> configuration For multiple directories, the config <code>log.dirs</code> is preferable, <code>log.dirs</code> is a comma-separated list of paths on the local system. If more than one path is specified, the broker will store partitions on them in a least-used fashion, with one partition\u2019s log segments stored within the same path. Note that the broker will place a new partition in the path that has the least number of partitions currently stored in it, not the least amount of disk space used, so an even distribution of data across multiple directories is not guaranteed.</p> <p>num.recovery.threads.per.data.dir</p> <p>The number of threads per data directory to be used for log recovery at startup and flushing at shutdown</p> <p>By default, only one thread per log directory is used. As these threads are only used during startup and shutdown, it is reasonable to set a larger number of threads in order to parallelize operations.</p> <p>Default: 1</p> <p>Please refer this site for more broker config</p>"},{"location":"distributed-system/kafka/installation/#topic-configurations","title":"Topic Configurations","text":"<p>num.partitions</p> <p>Determines how many partitions a new topic is created with. Please keep in mind that the number of partitions for a topic can only be increased, never decreased.</p> <p>Default: 1</p> <p>default.replication.factor</p> <p>This configuration sets what the replication factor should be for new topics. It's highly recommended to set the replication factor to at least 1 above the <code>min.isync.replicas</code> setting.</p> <p>Default: 1</p> <p>log.retention.ms</p> <p>This configuration controls the maximum time we will retain a log before we will discard old log segments to free up space.</p> <p>Default: 604800000 (7 days)</p> <p>log.retention.bytes</p> <p>This configuration controls the maximum size a partition (which consists of log segments) can grow to before we will discard old log segments to free up space.</p> <p>Default: -1 (unlimited)</p> <p>log.segment.bytes</p> <p>As messages are produced to the Kafka broker, they are appended to the current log segment for the partition. Once the log segment has reached the size specified by the <code>log.segment.bytes</code>, which defaults to 1GB, the log segment is closed and a new one is opened. Once the log segment is closed, it can be considered for expiration.</p> <p>Adjusting the size of the log segments can be important if topics have a low produce rate. For example, if a topic receives only 100Mb per day of messages, and <code>log.segment.bytes</code> is set to the default, it will take 10 days to fill one segment. As messages can not be expired until the log segment is closed, if <code>log.retention.ms</code> is set to 1 week, there will actually be up to 17 days of messages retained until the closed log segment expired.</p> <p>Default: 1GB</p> <p>log.roll.ms</p> <p>This configuration controls the period of time after which Kafka will force the log to roll even if the segment file isn't full to ensure that retention can delete or compact old data.</p> <p>Default: 7 days</p> <p>min.insync.replicas</p> <p>This configuration specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend)</p> <p>Default: 1</p> <p>message.max.bytes</p> <p>The largest record batch size allowed by Kafka (after compression if compression is enabled)</p> <p>Default: 1MB</p> <p>Please refer this site for more topic config</p>"},{"location":"distributed-system/kafka/kafka_architecture/","title":"Kafka architecture","text":""},{"location":"distributed-system/kafka/kafka_architecture/#kafka-storage-architecture","title":"Kafka storage architecture","text":""},{"location":"distributed-system/kafka/kafka_architecture/#topic","title":"Topic","text":"<p>Kafka topics are the categories used to organize messages. Kafka Topics stores millions message of data</p>"},{"location":"distributed-system/kafka/kafka_architecture/#partitions","title":"Partitions","text":"<p>Kafka organizes data into topics, which are further divided into partitions. Each partition is an ordered and immutable sequence of records. Partitioning allows Kafka to parallelize the data ingestion and processing across multiple brokers and enables horizontal scalability.</p> <p>Kafka creates a separate directory for each topic partition. If you determined n partition count, Kafka going to create n directories for that topic (Eg: topic-0, topic-1, ..., topic-n).</p>"},{"location":"distributed-system/kafka/kafka_architecture/#replication-factor","title":"Replication Factor","text":"<p>Kafka provides built-in replication for fault tolerance. Each partition can have multiple replicas distributed across different brokers. Replication ensures that data is replicated across multiple nodes, allowing for high availability and fault tolerance in case of broker failures.</p> <p>Like partition, you can define the number of it when creating a new topic.</p> <pre><code>Number of Replicas(15) = Partitions(5) x Replication(3)\n</code></pre>"},{"location":"distributed-system/kafka/kafka_architecture/#segment","title":"Segment","text":"<p>Kafka brokers splits each partition into segments. Each segment is stored in a single data file on the disk attached to the broker. By default, each segment contains either 1 GB of data or a week of data, whichever limit is attained first. When the Kafka broker receives data for a partition, as the segment limit is reached, it will close the file and start a new one:</p> <p></p> <p>Configuration:</p> <ul> <li>log.segment.bytes: the max size of a single segment in bytes (default 1 GB)</li> <li>log.segment.ms: the time Kafka will wait before committing the segment if not full (default 1 week)</li> </ul>"},{"location":"distributed-system/kafka/kafka_architecture/#kafka-topic-segments-and-indexes","title":"Kafka Topic Segments and Indexes","text":"<p>Kafka allows consumers to start fetching messages from any available offset. In order to help brokers quickly locate the message for a given offset, Kafka maintains two indexes for each segment:</p> <ul> <li>An offset to position index - It helps Kafka know what part of a segment to read to find a message</li> <li>A timestamp to offset index - It allows Kafka to find messages with a specific timestamp</li> </ul>"},{"location":"distributed-system/kafka/kafka_architecture/#inspecting-the-kafka-directory-structure","title":"Inspecting the Kafka Directory Structure","text":"<p>Kafka stores all of its data in a directory on the broker disk. This directory is specified using the property <code>log.dirs</code> in the broker's configuration file.</p> <p>Explore the directory and notice that there is a folder for each topic partition. All the segments of the partition are located inside the partition directory. Here, the topic named <code>configured-topic</code> has three partitions, each having one directory - <code>configured-topic-0</code>, <code>configured-topic-1</code> and <code>configured-topic-2</code>. Descend into a directory for a topic partition. Notice the indexes - time and offset for the segment and the segment file itself where the messages are stored.</p> <p></p>"},{"location":"distributed-system/kafka/kafka_architecture/#kafka-cluster-architecture","title":"Kafka cluster architecture","text":""},{"location":"distributed-system/kafka/kafka_architecture/#cluster-membership","title":"Cluster membership","text":"<p>A Kafka cluster consists of a group of brokers that are identified by a unique numeric ID. When the brokers connect to their configured ZooKeeper instances, a group znode is created with each broker creating an <code>ephemeral znode</code> under this group znode. If a node fails, the ephemeral nature of this znode means it gets removed from the group znode.</p>"},{"location":"distributed-system/kafka/kafka_architecture/#controller-broker","title":"Controller broker","text":"<p>A controller broker is a normal broker that simply has additional responsibility.</p> <p>The most important part of that additional responsibility is keeping track of nodes in the cluster and appropriately handling nodes that leave, join or fail. This includes rebalancing partitions and assigning new partition leaders.</p> <ul> <li>Cluster Management: The Controller broker is responsible for maintaining and managing the metadata of the Kafka cluster. It keeps track of information such as the list of topics, their partitions, replicas, and their current leaders.</li> <li>Leader Election: When a leader for a partition is unavailable or needs to be reassigned, the Controller broker is responsible for coordinating the leader election process.</li> <li>Topic and Partition Management: The Controller broker handles operations related to topic creation, deletion, and configuration changes. It ensures that partitions are evenly distributed across the brokers and handles partition reassignment when necessary, such as when brokers are added or removed from the cluster.</li> <li>Broker Failure Detection: The Controller continuously monitors the health of all brokers in the cluster. It detects and handles broker failures by initiating leader elections and replica reassignments for affected partitions.</li> <li>Controller Failover: In case the current Controller broker fails, another broker automatically takes over the role of the Controller to ensure uninterrupted cluster management. The Controller uses Apache ZooKeeper for leader election and coordination during failover scenarios. <code>/controller</code></li> </ul>"},{"location":"distributed-system/kafka/kafka_architecture/#kafka-work-distribution-architecture","title":"Kafka work distribution architecture","text":""},{"location":"distributed-system/kafka/kafka_architecture/#partition-assignment","title":"Partition assignment","text":"<p>To distribute partitions, Kafka applies the following steps:</p> <ol> <li>Ordered List of Brokers</li> </ol> <p>Starting from a randomly chosen broker, the next broker must be one in a different rack and so on.</p> <ol> <li>Leader and Follower Assignment</li> </ol> <p></p> <p>Using the round robin method, Kafka assigns the leader partitions first.</p> <p></p> <p>Then it assigns the first follower, starting from the second broker in the list and following the round robin method.</p> <p></p> <p>The second follower of the partition is assigned starting from the third broker in the list and similarly following the round robin method to distribute partitions. </p>"},{"location":"distributed-system/kafka/kafka_architecture/#leader-partition","title":"Leader Partition","text":"<ul> <li>The leader partition is responsible for handling all read and write requests for that partition.</li> <li>Producers send messages to the leader partition, and consumers fetch messages from the leader partition.</li> <li>The leader partition maintains the in-sync replicas (ISRs) list, which includes the followers that are up-to-date with the leader's data.</li> </ul>"},{"location":"distributed-system/kafka/kafka_architecture/#follower-partition","title":"Follower Partition","text":"<ul> <li>Follower partitions are replicas of a topic partition that replicate the data stored in the leader partition.</li> <li>Follower partitions are not directly involved in handling read and write requests from clients.</li> <li>The primary role of follower partitions is to replicate the leader partition's data and stay in sync with it.</li> </ul> <p>How producer and consumer can know which broker is leader partition</p> <p>Producers and consumer can retrieve metadata information about the Kafka cluster. The metadata includes the list of brokers, topics, partitions, and the current leader for each partition.</p>"},{"location":"distributed-system/kafka/kafka_architecture/#in-sync-replicas","title":"In Sync Replicas","text":"<p>In sync Replicas (or ISR\u2019s) are replicas which are in sync with the leader.</p> <p>Follower can not be in synced with the leader because of:</p> <ul> <li>Network congestion</li> <li>Follower broker cash/ restart</li> </ul> <p>The leader partition maintains the in-sync replicas (ISRs) list and persisted in the ZooKeeper. All of the follower in ISRs list are known to be in synced with the leader, and they are an excellent candidate to be elected as the new leader when something wrong with the current leader.</p> <p>How leader know if follower is in synced or lagging behind?</p> <p>Followers stay in sync by sending fetch requests to the leader, which returns messages to the follower in order. The follower is considered to be in sync if it has caught up with the most recently committed message on the leader. The leader checks this by looking at the last offset requested by the follower.</p> <p></p> <p>If the replica has requested the most recent message in the last 10 seconds they deserve to be in the ISR list. If not, the leader removes the replica from the IRS list.</p>"},{"location":"distributed-system/kafka/kafka_architecture/#how-kafka-deals-with-messages-in-consistency","title":"How Kafka deals with messages in-consistency","text":"<p>Assuming that all the followers in the ISR list are 11 seconds behind the leader, the ISR list would be empty. If the leader crashed and we had to elect a new leader among the followers, then we would lose messages.</p> <p>The solution is simple which is implemented using two concepts:</p> <p>Committed and Uncommitted messages</p> <p></p> <p>The first idea is to introduce the notion of committed and uncommited messages.</p> <p>We can configure the leader to not consider a message commited until the message is copied at all the followers in the ISR list =&gt; the leader may have some committed as well as some uncommitted messages.</p> <p>If the leader crashed the uncommitted messages can be resent by the producer. (Producers can choose to receive acknowledgments of sent messages only after the message is fully committed)</p> <p>Minimum In-Sync replicas</p> <p></p> <p>Let's Assume that we start with three replicas and all of them are healthy enough to be in the ISR. However, after some time two of them failed and as a result of that, the leader will remove them from the ISR, we are left with a single in-sync replica that is the leader itself.</p> <p>Now the data is considered committed when it is written to all in-sync replicas, even when all means just one replica (the leader itself).</p> <p>It is a risky scenario for data consistency because data could be lost if we lose the leader.</p> <p>The second idea is to set a minimum in-sync replicas configuration.</p> <p></p> <p>If you would like to be sure that committed data is written to at least two replicas, you need to set the minimum number of in-sync replicas as two.</p>"},{"location":"distributed-system/kafka/kafka_connect/","title":"Kafka Connect","text":"<p>Kafka Connect is an opensource component of Apache Kafka and provides scalable and reliable way to transfer data from Kafka to other data systems like databases, filesystems, key-value stores and search indexes. It uses Connectors that moves large data sets into and out of Kafka.</p> <p></p>"},{"location":"distributed-system/kafka/kafka_connect/#kafka-connect-concepts","title":"Kafka Connect Concepts","text":"<p>Connectors: the high level abstraction that coordinates data streaming by managing tasks</p> <p>Tasks \u2013 the implementation of how data is copied to or from Kafka</p> <p>Workers \u2013 the running processes that execute connectors and tasks</p> <p>Converters \u2013 the code used to translate data between Connect and the system sending or receiving data</p> <p>Transforms \u2013 simple logic to alter each message produced by or sent to a connector</p> <p>Dead Letter Queue \u2013 how Connect handles connector errors</p>"},{"location":"distributed-system/kafka/kafka_connect/#connectors","title":"Connectors","text":"<p>Connectors in Kafka Connect define where data should be copied to and from. A connector instance is a logical job that is responsible for managing the copying of data between Kafka and another system.</p> <p>Connectors can be one of the following type:</p> <ul> <li>Source connectors that push data into Kafka</li> <li>Sink connectors that extract data out of Kafka</li> </ul> <p>Plugins provide the implementation for Kafka Connect to run connector instances. Connector instances create the tasks required to transfer data in and out of Kafka. The Kafka Connect runtime orchestrates the tasks to split the work required between the worker pods.</p> <p>We can find list of available connectors here. If we want to buld our own connector we can find more information in developer guide</p>"},{"location":"distributed-system/kafka/kafka_connect/#tasks","title":"Tasks","text":"<p>Data transfer orchestrated by the Kafka Connect runtime is split into tasks that run in parallel.</p> <ul> <li>A source connector task polls the external data system and returns a list of records that a worker sends to the Kafka brokers.</li> <li>A sink connector task receives Kafka records from a worker for writing to the external data system.</li> </ul> <p>For sink connectors, the number of tasks created relates to the number of partitions being consumed.</p> <p>For source connectors, how the source data is partitioned is defined by the connector. You can control the maximum number of tasks that can run in parallel by setting tasksMax in the connector configuration. The connector might create fewer tasks than the maximum setting</p>"},{"location":"distributed-system/kafka/kafka_connect/#workers","title":"Workers","text":"<p>Connectors and tasks are logical units of work and must be scheduled to execute in a process. Kafka Connect calls these processes workers and has two types of workers: standalone and distributed.</p>"},{"location":"distributed-system/kafka/kafka_connect/#converters","title":"Converters","text":"<p>When a worker receives data, it converts the data into an appropriate format using a converter. You specify converters for workers in the worker config in the KafkaConnect resource.</p> <p>Kafka Connect can convert data to and from formats supported by Kafka, such as JSON or Avro...</p>"},{"location":"distributed-system/kafka/kafka_connect/#transforms","title":"Transforms","text":"<p>Kafka Connect translates and transforms external data. Single-message transforms change messages into a format suitable for the target destination. For example, a transform might insert or rename a field. Transforms can also filter and route data.</p> <ul> <li>Source connectors apply transforms before converting data into a format supported by Kafka.</li> <li>Sink connectors apply transforms after converting data into a format suitable for an external data system.</li> </ul> <p>This is list of Kafka Connect Transformations </p>"},{"location":"distributed-system/kafka/kafka_connect/#dead-letter-queue","title":"Dead Letter Queue","text":"<p>An invalid record may occur for a number of reasons. One example is when a record arrives at the sink connector serialized in JSON format, but the sink connector configuration is expecting Avro format. When an invalid record cannot be processed by a sink connector, the error is handled based on the connector configuration property <code>errors.tolerance</code>.</p> <p>Dead letter queues are only applicable for sink connectors.</p> <p>There are two valid values for this configuration property: <code>none</code> (default) or <code>all</code>.</p> <p>When <code>errors.tolerance</code> is set to <code>none</code> an error or invalid record causes the connector task to immediately fail and the connector goes into a failed state. To resolve this issue, you would need to review the Kafka Connect Worker log to find out what caused the failure, correct it, and restart the connector.</p> <p>When <code>errors.tolerance</code> is set to <code>all</code>, all errors or invalid records are ignored and processing continues. No errors are written to the Connect Worker log. To determine if records are failing you must use internal metrics or count the number of records at the source and compare that with the number of records processed.</p> <p>An error-handling feature is available that will route all invalid records to a special topic and report the error. This topic contains a dead letter queue of records that could not be processed by the sink connector.</p>"},{"location":"distributed-system/kafka/kafka_connect/#how-kafka-connect-work","title":"How Kafka Connect work","text":"<ol> <li>A plugin provides the implementation artifacts for the source connector</li> <li>A single worker initiates the source connector instance</li> <li>The source connector creates the tasks to stream data</li> <li>Tasks run in parallel to poll the external data system and return records</li> <li>Transforms adjust the records, such as filtering or relabelling them</li> <li>Converters put the records into a format suitable for Kafka</li> <li>The source connector is managed using KafkaConnectors or the Kafka Connect API</li> </ol> <ol> <li>A plugin provides the implementation artifacts for the sink connector</li> <li>A single worker initiates the sink connector instance</li> <li>The sink connector creates the tasks to stream data</li> <li>Tasks run in parallel to poll Kafka and return records</li> <li>Converters put the records into a format suitable for the external data system</li> <li>Transforms adjust the records, such as filtering or relabelling them</li> <li>The sink connector is managed using KafkaConnectors or the Kafka Connect API</li> </ol>"},{"location":"distributed-system/kafka/producer/","title":"Kafka producer","text":"<p>Kafka producer is a component of the Kafka ecosystem which is used to publish messages onto a Kafka topic.</p> <p></p>"},{"location":"distributed-system/kafka/producer/#message-keys","title":"Message Keys","text":"<p>Each event message contains an optional key and a value.</p> <p>In case the key (key=null) is not specified by the producer, messages are distributed evenly across partitions in a topic. This means messages are sent in a round-robin fashion (partition p0 then p1 then p2, etc... then back to p0 and so on...).</p> <p>If a key is sent (key != null), then all messages that share the same key will always be sent and stored in the same Kafka partition. A key can be anything to identify a message - a string, numeric value, binary value, etc.</p> <p></p>"},{"location":"distributed-system/kafka/producer/#kafka-message-anatomy","title":"Kafka Message Anatomy","text":"<p>Kafka messages are created by the producer. A Kafka message consists of the following elements:</p> <p></p> <ul> <li>Key: Mentioned above</li> <li>Value: represents the content of the message and can also be null. The value format is arbitrary and is then also serialized into binary format.</li> <li>Compression Type: Kafka messages may be compressed. The compression type can be specified as part of the message. Options are none, gzip, lz4, snappy, and zstd</li> <li>Headers: There can be a list of optional Kafka message headers in the form of key-value pairs. It is common to add headers to specify metadata about the message, especially for tracing.</li> <li>Partition + Offset: Once a message is sent into a Kafka topic, it receives a partition number and an offset id. The combination of topic+partition+offset uniquely identifies the message</li> <li>Timestamp: A timestamp is added either by the user or the system in the message.</li> </ul>"},{"location":"distributed-system/kafka/producer/#kafka-message-serializers","title":"Kafka Message Serializers","text":"<p>The data sent by the Kafka producers is serialized. This means that the data received by the Kafka consumers must be correctly deserialized in order to be useful within your application.Data being consumed must be deserialized in the same format it was serialized in.</p> <p></p>"},{"location":"distributed-system/kafka/producer/#sending-a-message-to-kafka","title":"Sending a message to Kafka","text":"<ol> <li>The producer passes the message to a configured list of interceptors. For example, an interceptor might mutate the message and return an updated version.</li> <li>Serializers convert record key and value to byte arrays</li> <li>Default or configured partitioner calculates topic partition if none is specified.</li> <li>The record accumulator appends the message to producer batches using a configured compression algorithm.</li> </ol> <p>At this point, the message is still in memory and not sent to the Kafka broker. Record Accumulator groups messages in memory by topic and partition.</p> <p></p> <p>Sender thread groups multiple batches with the same broker as a leader into requests and sends them. At this point, the message is sent to Kafka.</p> <p>There are 3 primary method of sending messages:</p> <ul> <li>Fire and forget: We send a message to the server and don't really care if it arrives successfully or not.</li> <li>Synchronous send: Technically, Kafka producer is always asynchronous, we send a message and the <code>send</code> method return a <code>Future</code> object. However, we use <code>get()</code> to wait on the <code>Future</code> and see if the <code>send()</code> as successful or not.</li> <li>Asynchronous send: We call the <code>send()</code> method with a callback function, which gets triggered when it receives a response from the Kafka server.</li> </ul>"},{"location":"distributed-system/kafka/producer/#handling-message-delivery-failure","title":"Handling message delivery failure","text":"<p>Send message synchronously</p> <pre><code>producer.send(...).get()\n</code></pre> <p>-&gt; affect to network throughput</p> <p>Producer callback</p> <pre><code>producer.send(..., (recordMetadata, e) -&gt; {\n    if (e != null) {\n        ...\n    }\n})\n</code></pre>"},{"location":"distributed-system/kafka/producer/#delivery-time","title":"Delivery time","text":""},{"location":"distributed-system/kafka/producer/#kafka-producer-configutation","title":"Kafka producer configutation","text":"<p>bootstrap.servers</p> <p>A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. This list does not need to include all brokers, since the producer will get more information after the initial connection. But it is recommended to include at least 2, so in case one broker goes down, the producer will still be able to connect to the cluster.</p> <p>key.serialized</p> <p>Serializer class for key that implements the <code>org.apache.kafka.common.serialization.Serializer</code> interface.</p> <p>value.serialized</p> <p>Serializer class for value that implements the <code>org.apache.kafka.common.serialization.Serializer</code> interface.</p> <p>client.id</p> <p>An id string to pass to the server when making requests. The purpose of this is to be able to track the source of requests beyond just ip/port by allowing a logical application name to be included in server-side request logging. acks</p> <p>The <code>acks</code> parameter controls how many partition replicas must receive the record before the producer can consider the write successful.</p> <ul> <li><code>0</code>: a producer will not wait for acknowledgment from brokers</li> <li><code>1</code>: a producer will wait only for the partition leader to write a message, without waiting for all followers</li> <li><code>all</code>, a producer will wait for all in-sync replicas to acknowledge the message. This comes at latency costs and represents the strongest available guarantee.</li> </ul> <p>Default: all</p> <p>max.block.ms</p> <p>The configuration controls how long the KafkaProducer's <code>send()</code>, <code>partitionsFor()</code>, <code>initTransactions()</code>, <code>sendOffsetsToTransaction()</code>, <code>commitTransaction()</code> and <code>abortTransaction()</code> methods will block</p> <p>Default: 1 minutes</p> <p>delivery.timeout.ms</p> <p>This configuration will limit the amount of time spent from the point a record is ready for sending (<code>send()</code> returned successfully and the record is placed in a batch)</p> <p>Default: 2 minutes</p> <p>request.timeout.ms</p> <p>The configuration controls the maximum amount of time the client will wait for the response of a request.</p> <p>Default: 30s</p> <p>retries</p> <p>Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error</p> <p>Default: 2147483647</p> <p>retry.backoff.ms</p> <p>The amount of time to wait before attempting to retry a failed request to a given topic partition.</p> <p>linger.ms</p> <p>This configutation controls the amount of time to wait for additional messages before sending the current batch. KafkaProducer sends a batch of messages either when the current batch is full (using <code>batch.size</code> configuration) or when the <code>linger.ms</code> limit is reached.</p> <p>Default: 0</p> <p>max.in.flight.requests.per.connection</p> <p>This controls how many message batches that the producer is allowed to sent before waiting for an ack from the broker. In other words this configuration is the maximum number of unacknowledged requests the client will send on a single connection before blocking.</p> <p>If <code>retries</code> are enabled, and <code>max.in.flight.requests.per.connection</code> is set greater than 1, and <code>enable.idempotence</code> is enabled there lies a risk of message re-ordering.</p> <p>Apache Kafka preserves the order of messages within a partition. Setting <code>retries</code> to non-zero and <code>max.in.flight.requests.per.connection</code> to more than 1 means that it is possible that the broker will fail to write the first batch of messages, succeed in writing the second (which as already in-flight) and then retry the first batch and succeed, thereby reversing the order.</p> <pre><code>Safety vs Throughput\n\nSetting max.in.flight.requests.per.connection=1 can significantly decrease your throughput\n</code></pre> <p>Default: 5</p> <p>enable.idempotence</p> <p>When set to 'true', the producer will ensure that exactly one copy of each message is written in the stream.</p> <p>Note that enabling idempotence requires <code>max.in.flight.requests.per.connection</code> to be less than or equal to 5 (with message ordering preserved for any allowable value), <code>retries</code> to be greater than 0, and <code>acks</code> must be 'all'.</p> <p>There is some case that the message is written to the broker more than one. For example, imagine that a broker received a record from the producer, wrote it to the disk and the record was successfully replicated to other broker, but then the first broker crashed before sending a response to the producer. The producer will wait until it reaches request timeout and then retry. The retry will go to the new leader that already has a copy of this record since the previous write was replicated successfully =&gt; you have a duplicated record.</p> <p>NOTE: Enable <code>enable.idempotence</code> ensures messages ordering within topic partition even <code>max.in.flight.requests.per.connection</code> configuration is larger than <code>1</code></p> <p>Default: true</p> <p>Please refer this site for more producer config</p>"},{"location":"distributed-system/kafka/reliable_data_delivery/","title":"Reliable Data Delivery","text":"<p>Reliability is a property of a system not of a single component so when we are talking about the reliability guarantees of Apache Kafka, we will need to keep the entire system and its use cases in mind</p>"},{"location":"distributed-system/kafka/reliable_data_delivery/#reliability-guarantees","title":"Reliability Guarantees","text":"<p>Apache Kafka guarantee:</p> <ul> <li>Kafka provides order guarantee of messages in a partition. If message B was written after message A, using the same producer in the same partition, then Kafka guarantee that the offset of message B will be higher than message A, and that consumers will read message B after message A.</li> <li>Produced messages are considered <code>committed</code> when they were written to the partition on all its in-sync replicas. Producers can choose to receive acknowledgements of sent messages when the message was fully committed, when it was written to the leader, or when it was sent over the network. Messages that are committed will not be lost as long as at least one replica remain alive.</li> <li>Consumers can only read messages that are committed.</li> </ul>"},{"location":"distributed-system/kafka/reliable_data_delivery/#replication","title":"Replication","text":"<p>Each Kafka topic is broken down into partitions, which are the basic data building blocks.</p> <p>A partition is stored on a single disk. Kafka guarantees the order of events within a partition, and a partition can be either online (available) or offline (unavailable).</p> <p>Each partition can have multiple replicas, one of which is a designated leader. All events are produced to the leader replica and are usually consumed from the leader replica as well. Other replicas just need to stay in sync with the leader and rep\u2010 licate all the recent events on time. If the leader becomes unavailable, one of the insync replicas becomes the new leader</p> <p>A replica is considered in sync if it is the leader for a partition, or if it is a follower that:</p> <ul> <li>Has an active session with ZooKeeper, meaning that it sent a heartbeat to ZooKeeper in the last 6 seconds (configurable).</li> <li>Fetched messages from the leader in the last 10 seconds (configurable).</li> <li>Fetched the most recent messages from the leader in the last 10 seconds. That is, it isn\u2019t enough that the follower is still getting messages from the leader; it must have had no lag at least once in the last 10 seconds (configurable).</li> </ul>"},{"location":"distributed-system/kafka/reliable_data_delivery/#broker","title":"Broker","text":"<p>There are three configuration parameters in the broker that change Kafka\u2019s behavior regarding reliable message storage.</p>"},{"location":"distributed-system/kafka/reliable_data_delivery/#replication-factor","title":"Replication Factor","text":"<p>The topic-level configuration is replication.factor. At the broker level, we control the <code>default.replication.factor</code> for automatically created topics.</p> <p>A replication factor of N allows us to lose N-1 brokers while still being able to read and write data to the topic. So a higher replication factor leads to higher availability, higher reliability, and fewer disasters.</p> <p>On the flip side, for a replication factor of N, we will need at least N brokers and we will store N copies of the data, meaning we will need N times as much disk space. We are basically trading availability for hardware.</p>"},{"location":"distributed-system/kafka/reliable_data_delivery/#unclean-leader-election","title":"Unclean Leader Election","text":"<p>This configuration is only available at the broker (and in practice, cluster-wide) level. The parameter name is <code>unclean.leader.election.enable</code>, and by default it is set to <code>false</code>.</p> <p>When the leader for a partition is no longer available, one of the in-sync replicas will be chosen as the new leader. This leader election is <code>clean</code> in the sense that it guarantees no loss of committed data\u2014by definition, committed data exists on all in-sync replicas.</p> <p>But what do we do when no in-sync replica exists except for the leader that just became unavailable?</p> <p>For example: The partition had three replicas, and the two followers became unavailable. In this situation, as producers continue writing to the leader, all the messages are acknowledged and committed (since the leader is the one and only in-sync replica). Now let\u2019s say that the leader becomes unavailable. In this scenario, if one of the out-of-sync followers starts first, we have an out-of-sync replica as the only available replica for the partition.</p> <p>In this scenario, we need to make a difficult decision:</p> <ul> <li>If we don\u2019t allow the out-of-sync replica to become the new leader, the partition will remain offline until we bring the old leader back to online</li> <li>If we do allow the out-of-sync replica to become the new leader, we are going to lose all messages that were written to the old leader while that replica was out of sync and also cause some inconsistencies in consumers</li> </ul> <p>By default, <code>unclean.leader.election.enable</code> is set to false, which will not allow out-of-sync replicas to become leaders.</p>"},{"location":"distributed-system/kafka/reliable_data_delivery/#minimum-in-sync-replicas","title":"Minimum In-Sync Replicas","text":"<p>Both the topic and the broker-level configuration are called <code>min.insync.replicas</code>.</p> <p>This configuration specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. If a topic has three replicas and we set <code>min.insync.replicas</code> to 2, then producers can only write to a partition in the topic if at least two out of the three replicas are in sync</p> <p>When all three replicas are in sync, everything proceeds normally. This is also true if one of the replicas becomes unavailable. However, if two out of three replicas are not available, the brokers will no longer accept produce requests. Instead, producers that attempt to send data will receive <code>NotEnoughReplicasException</code></p>"},{"location":"distributed-system/kafka/reliable_data_delivery/#producer","title":"Producer","text":"<p>Even if we configure the brokers in the most reliable configuration possible, the system as a whole can still potentially lose data if we don\u2019t configure the producers to be reliable as well.</p> <p>Here are two example scenarios to demonstrate this:</p> <ul> <li>We configured the brokers with three replicas, and unclean leader election is disabled. However, we configured the producer to send messages with acks=1.</li> <li>We configured the brokers with three replicas, and unclean leader election is disabled. However, we configured the producer to send messages with acks=all. Suppose that we are attempting to write a message to Kafka, but the leader for the partition we are writing to just crashed. Kafka will respond with <code>Leader not Available</code>. At this point, if the producer doesn\u2019t handle the error correctly and doesn\u2019t retry until the write is successful, the message may be lost.</li> </ul>"},{"location":"distributed-system/kafka/reliable_data_delivery/#send-acknowledgments","title":"Send Acknowledgments","text":"<p>Producers can choose between three different acknowledgment modes:</p> <ul> <li>ack=0</li> <li>ack=1</li> <li>ack=all</li> </ul>"},{"location":"distributed-system/kafka/reliable_data_delivery/#configuring-producer-retries","title":"Configuring Producer Retries","text":"<p>In general, when our goal is to never lose a message, our best approach is to configure the producer to keep trying to send the messages when it encounters a retriable error.</p> <p>Retrying to send a failed message includes a risk that both messages were successfully written to the broker, leading to duplicates. Retries and careful error handling can guarantee that each message will be stored at least once, but not exactly once</p>"},{"location":"distributed-system/kafka/reliable_data_delivery/#consumer","title":"Consumer","text":"<p>There are four consumer configuration properties that are important to understand in order to configure our consumer for a desired reliability behavior</p> <ul> <li><code>group.id</code></li> <li><code>auto.offset.reset</code>: This parameter controls what the consumer will do when no offsets were committed (e.g., when the consumer first starts) or when the consumer asks for offsets that don\u2019t exist in the broker. There are only two options here. If we choose <code>earliest</code>, the consumer will start from the beginning of the partition whenever it doesn\u2019t have a valid offset. If we choose <code>latest</code>, the consumer will start at the end of the partition.</li> <li><code>enable.auto.commit</code></li> <li><code>auto.commit.interval.ms</code></li> </ul>"},{"location":"distributed-system/kafka/reliable_data_delivery/#explicitly-committing-offsets-in-consumers","title":"Explicitly Committing Offsets in Consumers","text":""},{"location":"distributed-system/kafka/reliable_data_delivery/#always-commit-offsets-after-messages-were-processed","title":"Always commit offsets after messages were processed","text":"<p>Always commit offset once events are processed. It means your poll method should contain the processing logic like saving messages to DB or doing some translation logic before it calls <code>commit()</code></p>"},{"location":"distributed-system/kafka/reliable_data_delivery/#commit-frequency","title":"Commit Frequency","text":"<p>Calling <code>commit()</code> on every message will consume much resources on broker side as it increases the bookkeeping activity. Hence <code>commit()</code> should be called once all the messages from the <code>poll()</code> call get processed.</p> <p>The commit frequency has to balance requirements for performance and lack of duplicates. Committing after every message should only ever be done on very low-throughput topics.</p>"},{"location":"distributed-system/kafka/reliable_data_delivery/#rebalance","title":"Rebalance","text":"<p>This scenario can happen anytime as and when new consumers are added or existing consumers are dropped. But consumer code should gracefully handle <code>commit()</code> before moving out of the group. Therefore <code>commit()</code> method should be called in the finally block or finalize method.</p>"},{"location":"distributed-system/kafka/reliable_data_delivery/#consumer-retries","title":"Consumer Retries","text":"<p>In some cases, after calling poll and processing records, some records are not fully processed and will need to be processed later.  For example, we may try to write records from Kafka to a database but find that the database is not available at that moment and we need to retry later. Note that unlike traditional pub/sub messaging systems, Kafka consumers commit offsets and do not \u201cack\u201d individual messages. This means that if we failed to process record #30 and succeeded in processing record #31, we should not commit offset #31\u2014this would result in marking as processed all the records up to #31 including #30, which is usually not what we want. Instead, try following one of the following two patterns.</p> <ul> <li> <p>One option when we encounter a retriable error is to commit the last record we processed successfully. We\u2019ll then store the records that still need to be processed in a buffer (so the next poll won\u2019t override them), use the consumer <code>pause()</code> method to ensure that additional polls won\u2019t return data, and keep trying to process the records.</p> </li> <li> <p>A second option when encountering a retriable error is to write it to a separate topic and continue. A separate consumer group can be used to handle retries from the retry topic, or one consumer can subscribe to both the main topic and to the retry topic but pause the retry topic between retries. This pattern is similar to the deadletter-queue system used in many messaging systems</p> </li> </ul>"},{"location":"distributed-system/kafka/reliable_data_delivery/#consumers-may-need-to-maintain-state","title":"Consumers may need to maintain state","text":"<p>In some applications, we need to maintain state across multiple calls to poll. For example, if we want to calculate moving average, we\u2019ll want to update the average after every time we poll Kafka for new messages. If our process is restarted, we will need to not just start consuming from the last offset, but we\u2019ll also need to recover the matching moving average.</p>"},{"location":"distributed-system/webrtc/connecting/","title":"Connecting","text":""},{"location":"distributed-system/webrtc/connecting/#1-peer-to-peer-connection","title":"1. Peer to peer connection","text":"<p>WebRTC does\u2019t use a client/server model, it establishes peer-to-peer (P2P) connections. </p> <p></p>"},{"location":"distributed-system/webrtc/connecting/#2-how-does-it-work","title":"2. How does it work","text":""},{"location":"distributed-system/webrtc/connecting/#21-networking-real-world-constraints","title":"2.1 Networking real-world constraints","text":"<p>Most of the time the other WebRTC Agent will not even be in the same network. A typical call is usually between two WebRTC Agents in different networks with no direct connectivity.</p> <p></p> <p>For hosts in the same network it is very easy to connect. However, a host using <code>Router B</code> has no way to directly access anything behind <code>Router A</code>. How would you tell the different between <code>191.168.0.1</code> behind <code>Router A</code> and the same IP behind <code>Router B</code>? They are private IPs. A host using <code>Router B</code> could send traffic directly to <code>Router A</code>, but the request would end there. How does <code>Router A</code> know which host it should forward the message to?</p>"},{"location":"distributed-system/webrtc/connecting/#22-nat-mapping","title":"2.2 NAT Mapping","text":"<p>IP version 4 addresses are only 32 bit(4 byte) long, which provides 4.29 billion(2 to the power of 32 = 4,294,967,296) unique IP addresses. 4.29 billion address space not enough for give public IP address for all available hosts.</p> <p>To solve this issue NAT devices introduced. These devices would be responsible for maintaining a table mapping of local IP(private IP) and port tuples to one or more globally unique IP(public IP) and port tuples. By using this technology firewalls and routers allow multiple devices on a LAN with private IP addresses to share a single public IP address</p> <p>There are different types of NATs, but some of them allocates a public IP address and a port for UDP flows (what we need). When you want to create a P2P connection with a peer, a first challenge is therefore to discover what kind of NATs you are behind, and if they exist, to get an IP address and a port you can give to your contact.</p> <p></p>"},{"location":"distributed-system/webrtc/connecting/#23-stun","title":"2.3 STUN","text":"<p>Session Traversal Utilities for NAT (STUN) protocol enables a device to discover its public IP address.</p> <p>STUN relies on a simple observation: when you talk to a server on the internet from a NATed client, the server sees the public <code>ip:port</code> that your NAT device created for you, not your LAN <code>ip:port</code>. So, the server can tell you what <code>ip:port</code> it saw. That way, you know what traffic from your LAN <code>ip:port</code> looks like on the internet, you can tell your peers about that mapping, and now they know where to send packets.</p> <p></p>"},{"location":"distributed-system/webrtc/connecting/#24-nat-implementations","title":"2.4 NAT implementations","text":"<p>NATs are not all implemented the same way and may differ in how they allow packets to go through. Some NAT implementations, like the One-to-one NATs, will allow a P2P connection to be established. Some, like the symmetric ones, don\u2019t.</p> <p>One-to-one NAT (or Full Cone NAT)</p> <p>In this implementation, once an internal IP address/ port duo is mapped to the external IP adress/ port duo, all packages arriving to the external address/ port, no matter where they are coming from, will be sent through to the original internal one.</p> <p>Symmetric NAT</p> <p>A symmetric NAT is one where all requests from the same internal IP address and port, to a specific destination IP address and port, are mapped to the same external IP address and port. If the same host sends a packet with the same source address and port, but to a different destination, a different mapping is used. Furthermore, only the external host that receives a packet can send a UDP packet back to the internal host.</p> <p></p>"},{"location":"distributed-system/webrtc/connecting/#25-turn","title":"2.5 TURN","text":"<p>TURN (Traversal Using Relays around NAT) is the solution when direct connectivity isn\u2019t possible. It could be because a symmetric NAT is in use!</p> <p>WebRTC agent bypass the Symmetric NAT restriction by opening a connection with a TURN server and relaying all information through that server. You would create a connection with a TURN server and tell all peers to send packets to the server which will then be forwarded to you. This obviously comes with some overhead so it is only used if there are no other alternatives.</p> <p></p>"},{"location":"distributed-system/webrtc/connecting/#26-ice","title":"2.6 ICE","text":"<p>ICE (Interactive Connectivity Establishment) is how  WebRTC connects two Agents. ICE is a protocol for establishing connectivity. It determines all the possible routes between the two peers and then ensures you stay connected. </p> <p>These routes are known as <code>Candidate Pairs</code>, which is a pairing of a local and remote transport address. This is where STUN and TURN come into play with ICE. These addresses can be your <code>local IP Address plus a port</code>, <code>NAT mapping</code>, or <code>Relayed Transport Address</code> (TURN). Each side gathers all the addresses they want to use, exchanges them, and then attempts to connect!</p> <p>An ICE Agent is either Controlling or Controlled. The Controlling Agent is the one that decides the selected Candidate Pair. Usually, the peer sending the offer is the controlling side.</p> <p>Each side must have a <code>user fragment</code> and a <code>password</code>. These two values must be exchanged before connectivity checks can even begin. The <code>user fragment</code> is sent in plain text and is useful for demuxing multiple ICE Sessions. The <code>password</code> is used to generate a MESSAGE-INTEGRITY attribute. At the end of each STUN packet, there is an attribute that is a hash of the entire packet using the <code>password</code> as a key. This is used to authenticate the packet and ensure it hasn\u2019t been tampered with.</p>"},{"location":"distributed-system/webrtc/example/","title":"Example","text":""},{"location":"distributed-system/webrtc/example/#starting-a-call","title":"Starting a call","text":"<pre><code>function invite(evt) {\n  if (myPeerConnection) {\n    alert(\"You can't start a call because you already have one open!\");\n  } else {\n    createPeerConnection();\n\n    navigator.mediaDevices.getUserMedia({\n      audio: true, // We want an audio track\n      video: true // ...and we want a video track\n    })\n    .then(function(localStream) {\n      document.getElementById(\"local_video\").srcObject = localStream;\n      localStream.getTracks().forEach(track =&gt; myPeerConnection.addTrack(track, localStream));\n    })\n    .catch(handleGetUserMediaError);\n  }\n}\n\n\nfunction handleGetUserMediaError(e) {\n  switch(e.name) {\n    case \"NotFoundError\":\n      alert(\"Unable to open your call because no camera and/or microphone\" +\n            \"were found.\");\n      break;\n    case \"SecurityError\":\n    case \"PermissionDeniedError\":\n      // Do nothing; this is the same as the user canceling the call.\n      break;\n    default:\n      alert(\"Error opening your camera and/or microphone: \" + e.message);\n      break;\n  }\n\n  closeVideoCall();\n}\n</code></pre> <p>In the above example, we create peer connection via <code>createPeerConnection</code> method. Once the RTCPeerConnection has been created, we request access to the user's camera and microphone by calling <code>MediaDevices.getUserMedia()</code> which is exposed to us through the <code>MediaDevices.getUserMedia</code> property.</p> <p>We attach the incoming stream to the local preview <code>&lt;video&gt;</code> element by setting the element's srcObject property.</p> <p>We then iterate over the tracks in the stream, calling <code>addTrack()</code> to add each track to the <code>RTCPeerConnection</code>. Even though the connection is not fully established yet, you can begin sending data when you feel it's appropriate to do so. Media received before the ICE negotiation is completed may be used to help ICE decide upon the best connectivity approach to take, thus aiding in the negotiation process.</p> <p>Note that for native apps, such as a phone application, you should not begin sending until the connection has been accepted at both ends, at a minimum, to avoid inadvertently sending video and/or audio data when the user isn't prepared for it.</p>"},{"location":"distributed-system/webrtc/example/#creating-the-peer-connection","title":"Creating the peer connection","text":"<pre><code>function createPeerConnection() {\n  myPeerConnection = new RTCPeerConnection({\n      iceServers: [     // Information about ICE servers - Use your own!\n        {\n          urls: \"stun:stun.stunprotocol.org\"\n        }\n      ]\n  });\n\n  myPeerConnection.onicecandidate = handleICECandidateEvent;\n  myPeerConnection.ontrack = handleTrackEvent;\n  myPeerConnection.onnegotiationneeded = handleNegotiationNeededEvent;\n  myPeerConnection.onremovetrack = handleRemoveTrackEvent;\n  myPeerConnection.oniceconnectionstatechange = handleICEConnectionStateChangeEvent;\n  myPeerConnection.onicegatheringstatechange = handleICEGatheringStateChangeEvent;\n  myPeerConnection.onsignalingstatechange = handleSignalingStateChangeEvent;\n}\n</code></pre> <p>The createPeerConnection() function is used by both the caller and the callee to construct their RTCPeerConnection objects, their respective ends of the WebRTC connection.</p> <p>When using the RTCPeerConnection() constructor, we will specify an RTCConfiguration-compliant object providing configuration parameters for the connection. This is an array of objects describing STUN and/or TURN servers for the ICE layer to use when attempting to establish a route between the caller and the callee.</p> <p>After creating the RTCPeerConnection, we set up handlers for the events that matter to us. The first three of these event handlers are required; you have to handle them to do anything involving streamed media with WebRTC.</p>"},{"location":"distributed-system/webrtc/example/#starting-negotiation","title":"Starting negotiation","text":"<p>Once the caller has created its  <code>RTCPeerConnection</code>, created a media stream, and added its tracks to the connection, the browser will deliver a <code>negotiationneeded</code> event to the <code>RTCPeerConnection</code> to indicate that it's ready to begin negotiation with the other peer</p> <pre><code>function handleNegotiationNeededEvent() {\n  myPeerConnection.createOffer().then(function(offer) {\n    return myPeerConnection.setLocalDescription(offer);\n  })\n  .then(function() {\n    sendToServer({\n      name: myUsername,\n      target: targetUsername,\n      type: \"video-offer\",\n      sdp: myPeerConnection.localDescription\n    });\n  })\n  .catch(reportError);\n}\n</code></pre> <p>To start the negotiation process, we need to create and send an SDP offer to the peer we want to connect to. This offer includes a list of supported configurations for the connection, including information about the media stream we've added to the connection locally, and any ICE candidates gathered by the ICE layer already. We create this offer by calling <code>myPeerConnection.createOffer()</code>.</p> <p>When <code>createOffer()</code> succeeds, we pass the created offer information into <code>myPeerConnection.setLocalDescription()</code>, which configures the connection and media configuration state for the caller's end of the connection.</p> <p>We know the description is valid, and has been set, this is when we send our offer to the other peer by creating a new \"video-offer\" message containing the local description, then sending it through our signaling server to the callee.</p>"},{"location":"distributed-system/webrtc/example/#handling-the-invitation","title":"Handling the invitation","text":"<p>When the offer arrives, the callee's <code>handleVideoOfferMsg()</code> function is called with the \"video-offer\" message that was received</p> <pre><code>function handleVideoOfferMsg(msg) {\n  var localStream = null;\n\n  targetUsername = msg.name;\n  createPeerConnection();\n\n  var desc = new RTCSessionDescription(msg.sdp);\n\n  myPeerConnection.setRemoteDescription(desc).then(function () {\n    return navigator.mediaDevices.getUserMedia(mediaConstraints);\n  })\n  .then(function(stream) {\n    localStream = stream;\n    document.getElementById(\"local_video\").srcObject = localStream;\n\n    localStream.getTracks().forEach(track =&gt; myPeerConnection.addTrack(track, localStream));\n  })\n  .then(function() {\n    return myPeerConnection.createAnswer();\n  })\n  .then(function(answer) {\n    return myPeerConnection.setLocalDescription(answer);\n  })\n  .then(function() {\n    var msg = {\n      name: myUsername,\n      target: targetUsername,\n      type: \"video-answer\",\n      sdp: myPeerConnection.localDescription\n    };\n\n    sendToServer(msg);\n  })\n  .catch(handleGetUserMediaError);\n}\n</code></pre> <p>This code is very similar to what we did in the <code>invite()</code> function.  It starts by creating and configuring an RTCPeerConnection using our createPeerConnection() function. Then it takes the SDP offer from the received \"video-offer\" message and uses it to create a new RTCSessionDescription object representing the caller's session description.</p> <p>That session description is then passed into myPeerConnection.setRemoteDescription(). This establishes the received offer as the description of the remote (caller's) end of the connection.</p> <p>Once the answer has been created using myPeerConnection.createAnswer(), the description of the local end of the connection is set to the answer's SDP by calling myPeerConnection.setLocalDescription(), then the answer is transmitted through the signaling server to the caller to let them know what the answer is.</p>"},{"location":"distributed-system/webrtc/example/#sending-ice-candidates","title":"Sending ICE candidates","text":"<p>The ICE negotiation process involves each peer sending candidates to the other, repeatedly, until it runs out of potential ways it can support the <code>RTCPeerConnection</code>'s media transport needs.</p> <p>Once <code>setLocalDescription()</code>'s fulfillment handler has run, the ICE agent begins sending <code>icecandidate</code> events to the <code>RTCPeerConnection</code> one for each potential configuration it discovers.</p> <pre><code>function handleICECandidateEvent(event) {\n  if (!e.candidate) {\n    // All candidates have been sent\n    return\n  }\n\n  sendToServer({\n    type: \"new-ice-candidate\",\n    target: targetUsername,\n    candidate: event.candidate\n  });\n}\n</code></pre>"},{"location":"distributed-system/webrtc/example/#receiving-ice-candidates","title":"Receiving ICE candidates","text":"<pre><code>function handleNewICECandidateMsg(msg) {\n  var candidate = new RTCIceCandidate(msg.candidate);\n\n  myPeerConnection.addIceCandidate(candidate)\n    .catch(reportError);\n}\n</code></pre> <p>This function constructs an <code>RTCIceCandidate</code> object by passing the received SDP into its constructor, then delivers the candidate to the ICE layer by passing it into <code>myPeerConnection.addIceCandidate()</code>.</p>"},{"location":"distributed-system/webrtc/example/#receiving-new-streams","title":"Receiving new streams","text":"<p>When new tracks are added to the <code>RTCPeerConnection</code>\u2014 either by calling its <code>addTrack()</code> method or because of renegotiation of the stream's format\u2014a track event is set to the <code>RTCPeerConnection</code> for each track added to the connection. </p> <pre><code>function handleTrackEvent(event) {\n  document.getElementById(\"received_video\").srcObject = event.streams[0];\n  document.getElementById(\"hangup-button\").disabled = false;\n}\n</code></pre> <p>The incoming stream is attached to the \"received_video\" <code>&lt;video&gt;</code> element.</p>"},{"location":"distributed-system/webrtc/example/#handling-the-removal-of-tracks","title":"Handling the removal of tracks","text":"<p>Your code receives a <code>removetrack</code> event when the remote peer removes a track from the connection by calling RTCPeerConnection.removeTrack(). </p> <pre><code>function handleRemoveTrackEvent(event) {\n  var stream = document.getElementById(\"received_video\").srcObject;\n  var trackList = stream.getTracks();\n\n  if (trackList.length == 0) {\n    closeVideoCall();\n  }\n}\n</code></pre>"},{"location":"distributed-system/webrtc/example/#ending-the-call","title":"Ending the call","text":"<p>When the user clicks the \"Hang Up\" button to end the call, the hangUpCall() function is called:</p> <pre><code>function hangUpCall() {\n  closeVideoCall();\n  sendToServer({\n    name: myUsername,\n    target: targetUsername,\n    type: \"hang-up\"\n  });\n}\n</code></pre> <pre><code>function closeVideoCall() {\n  var remoteVideo = document.getElementById(\"received_video\");\n  var localVideo = document.getElementById(\"local_video\");\n\n  if (myPeerConnection) {\n    myPeerConnection.ontrack = null;\n    myPeerConnection.onremovetrack = null;\n    myPeerConnection.onremovestream = null;\n    myPeerConnection.onicecandidate = null;\n    myPeerConnection.oniceconnectionstatechange = null;\n    myPeerConnection.onsignalingstatechange = null;\n    myPeerConnection.onicegatheringstatechange = null;\n    myPeerConnection.onnegotiationneeded = null;\n    if (remoteVideo.srcObject) {\n      remoteVideo.srcObject.getTracks().forEach(track =&gt; track.stop());\n    }\n\n    if (localVideo.srcObject) {\n      localVideo.srcObject.getTracks().forEach(track =&gt; track.stop());\n    }\n\n    myPeerConnection.close();\n    myPeerConnection = null;\n  }\n\n  remoteVideo.removeAttribute(\"src\");\n  remoteVideo.removeAttribute(\"srcObject\");\n  localVideo.removeAttribute(\"src\");\n  remoteVideo.removeAttribute(\"srcObject\");\n\n  document.getElementById(\"hangup-button\").disabled = true;\n  targetUsername = null;\n}\n</code></pre>"},{"location":"distributed-system/webrtc/example/#dealing-with-state-changes","title":"Dealing with state changes","text":""},{"location":"distributed-system/webrtc/example/#ice-connection-state","title":"ICE connection state","text":"<p><code>iceconnectionstatechange</code> events are sent to the <code>RTCPeerConnection</code> by the ICE layer when the connection state changes (such as when the call is terminated from the other end).</p> <pre><code>function handleICEConnectionStateChangeEvent(event) {\n  switch(myPeerConnection.iceConnectionState) {\n    case \"closed\":\n    case \"failed\":\n      closeVideoCall();\n      break;\n  }\n}\n</code></pre>"},{"location":"distributed-system/webrtc/example/#ice-signaling-state","title":"ICE signaling state","text":"<pre><code>function handleSignalingStateChangeEvent(event) {\n  switch(myPeerConnection.signalingState) {\n    case \"closed\":\n      closeVideoCall();\n      break;\n  }\n};\n</code></pre>"},{"location":"distributed-system/webrtc/example/#ice-gathering-state","title":"ICE gathering state","text":"<p><code>icegatheringstatechange</code> events are used to let you know when the ICE candidate gathering process state changes</p> <pre><code>function handleICEGatheringStateChangeEvent(event) {\n  // Our sample just logs information to console here,\n  // but you can do whatever you need.\n}\n</code></pre>"},{"location":"distributed-system/webrtc/media_communication/","title":"Media communication","text":"<p>WebRTC uses RTP and RTCP for media communication.</p>"},{"location":"distributed-system/webrtc/media_communication/#1-rtp","title":"1. RTP","text":"<p>The Real-time Transport Protocol(RTP) is a network protocol used to deliver streaming audio and video media over the internet.</p>"},{"location":"distributed-system/webrtc/media_communication/#2-rtcp","title":"2. RTCP","text":"<p>RTCP stands for Real-time Transport Control Protocol. RTCP works hand in hand with RTP. RTP does the delivery of the actual data, whereas RTCP is used to send control packets to participants in a call. The primary function is to provide feedback on the quality of service being provided by RTP. </p> <p>Every RTCP packet has the following structure:</p> <pre><code> 0                   1                   2                   3\n 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|V=2|P|    RC   |       PT      |             length            |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                            Payload                            |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n</code></pre> <p>Version (V) </p> <p><code>Version</code> is always <code>2</code></p> <p>Padding (P)</p> <p><code>Padding</code> is a bool that controls if the payload has padding. The last byte of the payload contains a count of how many padding bytes was added</p> <p>Reception Report Count (RC)</p> <p>The number of reports in this packet. A single RTCP packet can contain multiple events.</p> <p>Packet Type (PT)</p> <p>Unique Identifier for what type of RTCP Packet this is.</p> <ul> <li><code>192</code> Full INTRA-frame Request (FIR)</li> <li><code>193</code> Negative ACKnowledgements (NACK)</li> <li><code>200</code> Sender Report</li> <li><code>201</code> Receiver Report</li> <li><code>205</code> Generic RTP Feedback</li> <li><code>206</code> Payload Specific Feedback</li> </ul> <p>Full INTRA-frame Request (FIR) and Picture Loss Indication (PLI)</p> <p>Both FIR and PLI messages serve a similar purpose. These messages request a full key frame from the sender</p> <p>PLI is used when partial frames were given to the decoder, but it was unable to decode them. This could happen because you had lots of packet loss, or maybe the decoder crashed.</p> <p>FIR shall not be used when packets or frames are lost. That is PLIs job. FIR requests a key frame for reasons other than packet loss - for example when a new member enters a video conference. They need a full key frame to start decoding video stream, the decoder will be discarding frames until key frame arrives.</p> <p>Negative Acknowledgment</p> <p>A NACK requests that a sender re-transmits a single RTP packet. This is usually caused by an RTP packet getting lost, but could also happen because it is late.</p> <p>Sender and Receiver Reports</p> <p>These reports are used to send statistics between agents. This communicates the amount of packets actually received and jitter.</p> <p>The reports can be used for diagnostics and congestion control.</p>"},{"location":"distributed-system/webrtc/securing/","title":"Securing","text":""},{"location":"distributed-system/webrtc/securing/#1-dtls","title":"1. DTLS","text":"<p>DTLS(Datagram Transport Layer Security) is a variant of the TLS protocol that runs over UDP instead of TCP. In WebRTC it is used as a way to generate symmetric encryption keys for use in SRTP as well as encyption of the datachannel. The DTLS handshake takes place after ICE has found a path through the NAT maze.</p>"},{"location":"distributed-system/webrtc/securing/#dtls-handshake","title":"DTLS handshake","text":"<p>ClientHello</p> <p>ClientHello is the initial message sent by the client. It contains a list of attributes that tell the server the ciphers and features the client supports.</p> <p>HelloVerifyRequest</p> <p>HelloVerifyRequest is sent by the server to the client. It is to make sure that the client intended to send the request. The Client then re-sends the ClientHello, but with a token provided in the HelloVerifyRequest.</p> <p>ServerHello</p> <p>ServerHello is the response by the server for the configuration of this session. It contains what cipher will be used when this session is over. It also contains the server random data.</p> <p>Certificate</p> <p>Certificate contains the certificate for the Client or Server. After the handshake is over we will make sure this certificate when hashed matches the fingerprint in the <code>SessionDescription</code>.</p> <p>ServerKeyExchange/ClientKeyExchange</p> <p>These messages are used to transmit the public key. On startup, the client and server both generate a keypair. After the handshake these values will be used to generate the Pre-Master Secret.</p> <p>CertificateRequest</p> <p>A CertificateRequest is sent by the server notifying the client that it wants a certificate.</p> <p>ServerHelloDone</p> <p>ServerHelloDone notifies the client that the server is done with the handshake.</p> <p>CertificateVerify</p> <p>CertificateVerify is how the sender proves that it has the private key sent in the Certificate message.</p> <p>ChangeCipherSpec</p> <p>ChangeCipherSpec informs the receiver that everything sent after this message will be encrypted.</p> <p>Finished</p> <p>Finished is encrypted and contains a hash of all messages. This is to assert that the handshake was not tampered with.</p>"},{"location":"distributed-system/webrtc/securing/#2-srtp","title":"2. SRTP","text":"<p>SRTP is a protocol designed specifically for encrypting RTP packets. To start an SRTP session you specify your keys and cipher. Unlike DTLS it has no handshake mechanism. All the configuration and keys were generated during the DTLS handshake.</p>"},{"location":"distributed-system/webrtc/signaling/","title":"Signaling","text":"<p>When a WebRTC Agent starts it has no idea who it is going to communicate with and what they are going to communicate about.  Signaling is used to bootstrap the call so that two WebRTC agents can start communicating.</p> <p>Signaling messages are just text. The WebRTC agents don\u2019t care how they are transported. They are commonly shared via Websockets, but that is not a requirement.</p>"},{"location":"distributed-system/webrtc/signaling/#1-how-does-webrtc-signaling-work","title":"1. How does WebRTC signaling work?","text":"<p>WebRTC uses an existing protocol called the Session Description Protocol. Via this protocol, the two WebRTC Agents will share all the state required to establish a connection.</p>"},{"location":"distributed-system/webrtc/signaling/#2-what-is-the-session-description-protocol-sdp","title":"2. What is the Session Description Protocol (SDP)","text":"<p>The SDP is a key/value protocol wiht a newline after each value. A SDP contains zero or more Media Descriptions.</p> <p>A Media Description usually maps to a single stream of media. So if you wanted to describe a call with three video streams and two audio tracks you would have five Media Descriptions.</p>"},{"location":"distributed-system/webrtc/signaling/#22-how-to-read-the-sdp","title":"2.2. How to read the SDP","text":"<p>Every line in a SDP will start with a single character, this is your key. </p> <p>The SDP defines all they keys that are valid. You can ONLY use letters for keys as defined in the protocol.</p> <p>Eg:</p> <pre><code>a=my-sdp-value\na=second-value\n</code></pre> <p>SDP keys is using in WebRTC:</p> <ul> <li><code>v</code> version, should be equal to <code>0</code></li> <li><code>o</code> origin, contains a unique ID useful for renegotiations</li> <li><code>s</code> session name, should be equal to <code>-</code></li> <li><code>t</code> timing, should be equal to <code>0 0</code></li> <li><code>m</code> media description (<code>m=&lt;media&gt; &lt;port&gt; &lt;proto&gt; &lt;fmt&gt; ...</code>) </li> <li><code>a</code> attribute, a free text field</li> <li><code>c</code> connection data, shoudl be equal to <code>IN IP4 0.0.0.0</code></li> </ul> <p>Media Descriptions in SDP:</p> <p>A Media Description definition contains list of formats. These formats map to RTP Payload Types. The actual codec is then defined by an <code>Attribute</code> with the value <code>rtpmap</code> in the Media Description.</p> <p>Eg:</p> <pre><code>v=0\no=- 0 0 IN IP4 127.0.0.1\ns=-\nc=IN IP4 127.0.0.1\nt=0 0\nm=audio 4000 RTP/AVP 111\na=rtpmap:111 OPUS/48000/2\nm=video 4002 RTP/AVP 96\na=rtpmap:96 VP8/90000\n</code></pre> <p>In the above example, we have 2 Media Descriptions:</p> <ul> <li>Audio format in port <code>4000</code>, proto <code>RTP/AVP</code>, fmt <code>111</code>. The attribute maps the Payload Type <code>111</code> to <code>Opus</code></li> <li>Video format in port <code>4002</code>, proto <code>RTP/AVP</code>, fmt <code>96</code>. The attribute maps the Payload Type <code>96</code> to <code>VP8</code></li> </ul> <p>Note: In the above example <code>v</code>, <code>o</code>, <code>s</code>, <code>c</code>, <code>t</code> are defined, but they do not affect the WebRTC session.</p>"},{"location":"distributed-system/webrtc/signaling/#3-how-sdp-and-webrtc-work-together","title":"3. How SDP and WebRTC work together","text":""},{"location":"distributed-system/webrtc/signaling/#31-what-are-offers-and-answers","title":"3.1 What are offers and answers","text":"<p>WebRTC uses an offer/answer model. All this means is that one WebRTC Agent makes an \u201cOffer\u201d to start a call, and the other WebRTC Agents \u201cAnswers\u201d if it is willing to accept what has been offered.</p> <p>This gives the answerer a chance to reject unsupported codecs in the Media Descriptions. This is how two peers can understand what formats they are willing to exchange.</p> <p></p>"},{"location":"distributed-system/webrtc/signaling/#32-transceivers-are-for-sending-and-receiving","title":"3.2 Transceivers are for sending and receiving","text":"<p>Transceivers is a WebRTC specific concept that you will see in the API. What it is doing is exposing the \u201cMedia Description\u201d to the JavaScript API. Each Media Description becomes a Transceiver. Every time you create a Transceiver a new Media Description is added to the local Session Description.</p> <p>Each Media Description in WebRTC will have a direction attribute. There are four valid values:</p> <ul> <li><code>send</code></li> <li><code>recv</code></li> <li><code>sendrecv</code></li> <li><code>inactive</code></li> </ul>"},{"location":"distributed-system/webrtc/signaling/#33-sdp-values-used-by-webrtc","title":"3.3 SDP Values used by WebRTC","text":"<p>group:BUNDLE</p> <p>Bundling is an act of running multiple types of traffic over one connection. Some WebRTC implementations use a dedicated connection per media stream. Bundling should be preferred.</p> <p>fingerprint:sha-256</p> <p>This is a hash of the certificate a peer is using for DTLS. After the DTLS handshake is completed, you compare this to the actual certificate to confirm you are communicating with whom you expect.</p> <p>setup:</p> <p>This controls the DTLS Agent behavior. This determines if it runs as a client or server after ICE has connected. The possible values are:</p> <ul> <li><code>setup:active</code> - Run as DTLS Client</li> <li><code>setup:passive</code> - Run as DTLS Server</li> <li><code>setup:actpass</code> - Ask the other WebRTC Agent to choose</li> </ul> <p>ice-ufrag</p> <p>This is the user fragment value for the ICE Agent. Used for the authentication of ICE Traffic.</p> <p>ice-pwd</p> <p>This is the password for the ICE Agent. Used for authentication of ICE Traffic.</p> <p>rtpmap</p> <p>This value is used to map a specific codec to an RTP Payload Type.</p> <p>fmtp</p> <p>Defines additional values for one Payload Type. This is useful to communicate a specific video profile or encoder setting.</p> <p>candidate</p> <p>This is an ICE Candidate that comes from the ICE Agent. This is one possible address that the WebRTC Agent is available on</p> <p>ssrc</p> <p>A Synchronization Source (SSRC) defines a single media stream track.</p> <p><code>label</code> is the ID for this individual stream. <code>mslabel</code> is the ID for a container that can have multiple streams inside it.</p>"},{"location":"distributed-system/webrtc/signaling/#34-example-of-a-webrtc-session-description","title":"3.4 Example of a WebRTC Session Description","text":"<pre><code>v=0\no=- 3546004397921447048 1596742744 IN IP4 0.0.0.0\ns=-\nt=0 0\na=fingerprint:sha-256 0F:74:31:25:CB:A2:13:EC:28:6F:6D:2C:61:FF:5D:C2:BC:B9:DB:3D:98:14:8D:1A:BB:EA:33:0C:A4:60:A8:8E\na=group:BUNDLE 0 1\nm=audio 9 UDP/TLS/RTP/SAVPF 111\nc=IN IP4 0.0.0.0\na=setup:active\na=mid:0\na=ice-ufrag:CsxzEWmoKpJyscFj\na=ice-pwd:mktpbhgREmjEwUFSIJyPINPUhgDqJlSd\na=rtcp-mux\na=rtcp-rsize\na=rtpmap:111 opus/48000/2\na=fmtp:111 minptime=10;useinbandfec=1\na=ssrc:350842737 cname:yvKPspsHcYcwGFTw\na=ssrc:350842737 msid:yvKPspsHcYcwGFTw DfQnKjQQuwceLFdV\na=ssrc:350842737 mslabel:yvKPspsHcYcwGFTw\na=ssrc:350842737 label:DfQnKjQQuwceLFdV\na=msid:yvKPspsHcYcwGFTw DfQnKjQQuwceLFdV\na=sendrecv\na=candidate:foundation 1 udp 2130706431 192.168.1.1 53165 typ host generation 0\na=candidate:foundation 2 udp 2130706431 192.168.1.1 53165 typ host generation 0\na=candidate:foundation 1 udp 1694498815 1.2.3.4 57336 typ srflx raddr 0.0.0.0 rport 57336 generation 0\na=candidate:foundation 2 udp 1694498815 1.2.3.4 57336 typ srflx raddr 0.0.0.0 rport 57336 generation 0\na=end-of-candidates\nm=video 9 UDP/TLS/RTP/SAVPF 96\nc=IN IP4 0.0.0.0\na=setup:active\na=mid:1\na=ice-ufrag:CsxzEWmoKpJyscFj\na=ice-pwd:mktpbhgREmjEwUFSIJyPINPUhgDqJlSd\na=rtcp-mux\na=rtcp-rsize\na=rtpmap:96 VP8/90000\na=ssrc:2180035812 cname:XHbOTNRFnLtesHwJ\na=ssrc:2180035812 msid:XHbOTNRFnLtesHwJ JgtwEhBWNEiOnhuW\na=ssrc:2180035812 mslabel:XHbOTNRFnLtesHwJ\na=ssrc:2180035812 label:JgtwEhBWNEiOnhuW\na=msid:XHbOTNRFnLtesHwJ JgtwEhBWNEiOnhuW\na=sendrecv\n</code></pre> <p>This is what we know from the above message:</p> <ul> <li>We have two media sections, one audio and one video.</li> <li>Both of them are sendrecv transceivers. We are getting two streams, and we can send two back.</li> <li>We have ICE Candidates and Authentication details, so we can attempt to connect.</li> <li>We have a certificate fingerprint, so we can have a secure call.</li> </ul>"},{"location":"distributed-system/webrtc/webrtc/","title":"WebRTC","text":"<p>In WebRTC we have 4 steps:</p> <ul> <li>Signaling</li> <li>Connecting</li> <li>Securing</li> <li>Communicating</li> </ul> <p>These four steps happen sequentially. The prior step must be 100% successful for the subsequent one to even begin.</p>"},{"location":"distributed-system/webrtc/webrtc/#webrtc-topologies","title":"WebRTC Topologies","text":""},{"location":"distributed-system/webrtc/webrtc/#one-to-one","title":"One-To-One","text":"<p>One-to-One is the first connection type you will use with WebRTC. You connect two WebRTC Agents directly and they can send bi-directional media and data.</p> <p></p>"},{"location":"distributed-system/webrtc/webrtc/#full-mesh","title":"Full Mesh","text":"<p>Full mesh is the answer if you want to build a conference call or a multiplayer game. In this topology each user establishes a connection with every other user directly</p> <p>In a Full Mesh topology each user is connected directly. That means you have to encode and upload video independently for each member of the call. The network conditions between each connection will be different, so you can\u2019t reuse the same video.</p> <p></p>"},{"location":"distributed-system/webrtc/webrtc/#hybrid-mesh","title":"Hybrid Mesh","text":"<p>Hybrid Mesh is an alternative to Full Mesh that can alleviate some of the Full Mesh\u2019s issues. In a Hybrid Mesh connections aren\u2019t established between every user. Instead, media is relayed through peers in the network. This means that the creator of the media doesn\u2019t have to use as much bandwidth to distribute media.</p> <p>This does have some downsides. In this set up, the original creator of the media has no idea who its video is being sent too, and if it arrived successfully. You also will have an increase in latency with every hop in your Hybrid Mesh network.</p> <p></p>"},{"location":"distributed-system/webrtc/webrtc/#mcu","title":"MCU","text":"<p>The MCU architecture assumes that each conference participant sends his or her stream to the MCU. The MCU decodes each received stream, rescales it, composes a new stream from all received streams, encodes it, and sends a single to all other participants.</p> <p>The main disadvantage of MCU is its cost, as It decodes and re-encodes streams to compose the final stream, hence requires significant computing power by the MCU.</p> <p></p>"},{"location":"distributed-system/webrtc/webrtc/#selective-forwarding-unit","title":"Selective Forwarding Unit","text":"<p>Selective Forwarding Units (SFUs) are the most popular modern approach. In the SFU architecture, every participant sends his or her media stream to a centralized server (SFU) and receives streams from all other participants via the same central server. The architecture allows the call participant to send multiple media streams to the SFU, where the SFU may decide which of the media streams should be forwarded to the other call participants.</p> <p>Unlike in the MCU architecture, the SFU does not need to decode and re-encode received streams, but simply acts as a forwarder of streams between call participants. The device endpoints need to be more intelligent and have more computing power than in the MCU architecture.</p> <p></p>"},{"location":"distributed-system/zookeeper/fundamentals/","title":"ZooKeeper","text":"<p>ZooKeeper is a centralized service for providing configuration information, naming, synchronization and group services over large clusters in distributed systems.</p>"},{"location":"distributed-system/zookeeper/fundamentals/#use-cases","title":"Use cases","text":"<p>Naming service: maps the names of network resources to their respective network addresses</p> <p>Configuration management: Managing application configuration that can be shared across servers in a cluster. The idea is to maintain any configuration in a centralized place so that all servers will see any change in configuration files/data.</p> <p>Leader election: Electing a leader in a multi-node cluster. You might need a leader to maintain a single point for an update request or distributing tasks from leader to worker nodes.</p> <p>Locks in distributed systems: distributed locks enables different systems to operate on a shared resource in a mutually exclusive way.</p> <p>Manage cluster membership: Maintain and detect if any server leaves or joins a cluster and store other complex information of a cluster.</p>"},{"location":"distributed-system/zookeeper/fundamentals/#zookeeper-architecture","title":"ZooKeeper architecture","text":"<p>Client: Clients, one of the nodes in our distributed application cluster access information from the server. For a particular time interval, every client sends a message to the server to let the sever know that the client is alive.</p> <p>Server: Server, one of the nodes in our ZooKeeper ensemble, provides all the services to clients. Gives acknowledgement to client to inform that the server is alive.</p> <p>Ensemble: Group of ZooKeeper servers. The minimum number of nodes that is required to form an ensemble is 3.</p> <p>Leader: Server node which performs automatic recovery if any of the connected node failed. Leaders are elected on service startup.</p> <p>Follower: Server node which follows leader instruction.</p>"},{"location":"distributed-system/zookeeper/fundamentals/#data-model","title":"Data model","text":"<p>Zookeeper keeps internal data structure which is called znode. Znodes are in shape of directory. znode can contain either data and subnodes (each znode can store upto 1MB of data)</p> <p></p> <p>Every znode in the ZooKeeper data model maintains a stat structure. A stat simply provides the metadata of a znode</p> <p>Version number Every znode has a version number, which means every time the data associated with the znode changes, its corresponding version number would also increased. The use of version number is important when multiple zookeeper clients are trying to perform operations over the same znode.</p> <p>Action Control List (ACL) ACL is basically an authentication mechanism for accessing the znode. It governs all the znode read and write operations.</p> <p>Timestamp Timestamp represents time elapsed from znode creation and modification. It is usually represented in milliseconds. ZooKeeper identifies every change to the znodes from \u201cTransaction ID\u201d (zxid). Zxid is unique and maintains time for each transaction so that you can easily identify the time elapsed from one request to another request.</p> <p>Data length Total amount of the data stored in a znode is the data length. You can store a maximum of 1MB of data.</p>"},{"location":"distributed-system/zookeeper/fundamentals/#types-of-znodes","title":"Types of Znodes","text":""},{"location":"distributed-system/zookeeper/fundamentals/#persistence-znodes","title":"Persistence znodes","text":"<p>Persistence znode is alive even after the client, which created that particular znode, is disconnected. By default, all znodes are persistent unless otherwise specified.</p> <p>Example: Solr Cloud, uses these znodes to store server configuration and schema of database/collections.</p>"},{"location":"distributed-system/zookeeper/fundamentals/#ephemeral-znode","title":"Ephemeral znode","text":"<p>Ephemeral znodes are active until the client is alive. When a client gets disconnected from the ZooKeeper ensemble, then the ephemeral znodes get deleted automatically. For this reason, only ephemeral znodes are not allowed to have a children further.</p> <p>It is created using -e flag</p> <p>Example: Let\u2019s say you want to maintain a list of active servers in a cluster. So, you create a parent Znode <code>/live_servers</code>. Under it, you keep creating child Znode for every new server in the cluster. At any point, if a server crashes/dies, child Znode belonging to the respective server will be deleted. Other servers will get a notification of this deletion if they are watching the znode <code>\\live_servers</code>.</p>"},{"location":"distributed-system/zookeeper/fundamentals/#ephemeral-sequential-znode","title":"Ephemeral Sequential Znode","text":"<p>It is the same as ephemeral Znode, the only difference is Zookeeper attaches a sequential number as a suffix, and if any new sibling Znode of the same type is created, it will be assigned a number higher than the previous one.</p> <p>This type of znode is created using -e -s flag.</p> <p>For example, if a znode with path /myapp is created as a sequential znode, ZooKeeper will change the path to /myapp0000000001 and set the next sequence number as 0000000002.</p> <p>This type of znode could be used in the leader election algorithm.</p> <p>Example:</p> <p>Say I have a parent node <code>/election</code>, and for any new node that joins the cluster, I add an ephemeral sequential Znode to this <code>/election</code> node. We can consider a server as the leader if any server that created the znode has the least sequential number attached to it.</p> <p>So, even if a leader goes down, the zookeeper will delete the corresponding Znode created by the leader server and notify the client applications, then that client fetches the new lowermost sequence node and considers that as a new leader</p>"},{"location":"distributed-system/zookeeper/fundamentals/#persistent-sequential-znode","title":"Persistent Sequential Znode","text":"<p>This is a persistent node with a sequence number attached to its name as a suffix.</p>"},{"location":"distributed-system/zookeeper/fundamentals/#sessions","title":"Sessions","text":"<p>Sessions are very important for the operation of ZooKeeper. Requests in a session are executed in FIFO order. Once a client connects to a server, the session will be established and a session id is assigned to the client.</p> <p>The client sends heartbeats at a particular time interval to keep the session valid. If the ZooKeeper ensemble does not receive heartbeats from a client for more than the period (session timeout) specified at the starting of the service, it decides that the client died.</p> <p>Session timeouts are usually represented in milliseconds. When a session ends for any reason, the ephemeral znodes created during that session also get deleted.</p>"},{"location":"distributed-system/zookeeper/fundamentals/#watches","title":"Watches","text":"<p>Watches are a simple mechanism for the client to get notifications about the changes in the ZooKeeper ensemble. Clients can set watches while reading a particular znode. Watches send a notification to the registered client for any of the znode (on which client registers) changes.</p> <p>Znode changes are modification of data associated with the znode or changes in the znode\u2019s children.</p>"},{"location":"distributed-system/zookeeper/leader_election/","title":"Leader Election","text":"<p>Let us analyze how a leader node can be elected in a ZooKeeper ensemble. Consider there are N number of nodes in a cluster. The process of leader election is as follows</p> <ul> <li>All the nodes create a sequential, ephemeral znode with the same path, <code>/app/leader_election/guid_</code>.</li> <li>ZooKeeper ensemble will append the 10-digit sequence number to the path and the znode created will be <code>/app/leader_election/guid_0000000001</code>, <code>/app/leader_election/guid_0000000002</code>, etc.</li> <li>For a given instance, the node which creates the smallest number in the znode becomes the leader and all the other nodes are followers.</li> <li>Each follower node watches the znode having the next smallest number. For example, the node which creates znode <code>/app/leader_election/guid_0000000008</code> will watch the znode <code>/app/leader_election/guid_0000000007</code> and the node which creates the znode <code>/app/leader_election/guid_0000000007</code> will watch the znode <code>/app/leader_election/guid_0000000006</code>.</li> <li>If the leader goes down, then its corresponding znode <code>/app/leader_electionN</code> gets deleted.</li> <li>The next in line follower node will get the notification through watcher about the leader removal.</li> <li>The next in line follower node will check if there are other znodes with the smallest number. If none, then it will assume the role of the leader. Otherwise, it finds the node which created the znode with the smallest number as leader.</li> <li>Similarly, all other follower nodes elect the node which created the znode with the smallest number as leader.</li> </ul>"},{"location":"distributed-system/zookeeper/workflow/","title":"ZooKeeper Workflow","text":"<p>Once a ZooKeeper ensemble starts, it will wait for the clients to connect. Clients will connect to one of the nodes in the ZooKeeper ensemble. It may be a leader or a follower node.</p>"},{"location":"distributed-system/zookeeper/workflow/#read-request","title":"Read request","text":"<p>If a client wants to read a particular znode, it sends a read request to the node with the znode path and the node returns the requested znode by getting it from its own database. For this reason, reads are fast in ZooKeeper ensemble.</p>"},{"location":"distributed-system/zookeeper/workflow/#write-request","title":"Write request","text":"<p>If a client wants to store data in the ZooKeeper ensemble, it sends the znode path and the data to the server. The connected server will forward the request to the leader and then the leader will reissue the writing request to all the followers.</p> <p>If only a majority of the nodes respond successfully, then the write request will succeed and a successful return code will be sent to the client. Otherwise, the write request will fail. The strict majority of nodes is called as Quorum.</p>"},{"location":"distributed-system/zookeeper/workflow/#nodes-in-a-zookeeper-ensemble","title":"Nodes in a ZooKeeper Ensemble","text":"<ul> <li> <p>If we have a single node, then the ZooKeeper ensemble fails when that node fails. It contributes to \u201cSingle Point of Failure\u201d and it is not recommended in a production environment.</p> </li> <li> <p>If we have two nodes and one node fails, we don\u2019t have majority as well, since one out of two is not a majority.</p> </li> <li> <p>If we have three nodes and one node fails, we have majority and so, it is the minimum requirement. It is mandatory for a ZooKeeper ensemble to have at least three nodes in a live production environment.</p> </li> <li> <p>If we have four nodes and two nodes fail, it fails again and it is similar to having three nodes. The extra node does not serve any purpose and so, it is better to add nodes in odd numbers, e.g., 3, 5, 7.</p> </li> </ul> <p>We know that a write process is expensive than a read process in ZooKeeper ensemble, since all the nodes need to write the same data in its database. So, it is better to have less number of nodes (3, 5 or 7) than having a large number of nodes for a balanced environment.</p>"},{"location":"distributed-system/zookeeper/workflow/#workflow-components","title":"Workflow components","text":"<p>Write: Write process is handled by the leader node. The leader forwards the write request to all the znodes and waits for answers from the znodes. If half of the znodes reply, then the write process is complete.</p> <p>Read: Reads are performed internally by a specific connected znode, so there is no need to interact with the cluster.</p> <p>Database It is used to store data in zookeeper. Each znode has its own database and every znode has the same data at every time with the help of consistency.</p> <p>Leader: Leader is the Znode that is responsible for processing write requests.</p> <p>Follower: Followers receive write requests from the clients and forward them to the leader znode.</p> <p>Request Processor: Present only in leader node. It governs write requests from the follower node.</p> <p>Atomic broadcasts: Responsible for broadcasting the changes from the leader node to the follower nodes.</p>"},{"location":"networking/connecting_to_the_internet/","title":"Connecting to the Internet","text":""},{"location":"networking/connecting_to_the_internet/#the-internets-nuts-and-bolts","title":"The Internet\u2019s Nuts and Bolts","text":"<p>The internet is essentially made of hosts/end systems, communication links, and packet switches.</p> <p> </p>"},{"location":"networking/connecting_to_the_internet/#connecting-modem-router-switches-and-end-hosts","title":"Connecting Modem, Router, Switches, and End Hosts","text":"<p>Modem </p> <p>Modem (modulator-demodulator) is the device is a device connecting your home to your internet service provider (ISP), like Vietel, FPT... through a physical connection (fiber, phone line...). The modem translates the digital 1s and 0s from your computer into analog information for the cable or telephone wire to carry out to the world, and translates incoming analog signals in the same way. A modem has a single coaxial port for the cable connection from your ISP and a single ethernet port to link the Internet port on your router. Modems bring the Internet to your home</p> <p>Router </p> <p>Standalone modems aren't able to send data to multiple devices simultaneously. They usually only have one Ethernet port, and only produce one IP address, which identifies your location to the internet. A router connects to all your home's devices (and links them to each other)\u2014through Ethernet cables or Wi-Fi\u2014and then connects to the modem. Your modem receives information from the internet, sends it to the router, and the router sends it to the computer that asked for it. . The router creates a local area network (LAN) within your house or office. Some modems also include router. Routers bring the Internet to your devices. Router operates at layer 3.</p> <p>Access Point </p> <p>Today, though, we have the ability to connect all devices to your home network over Wi-Fi. To do that, you need something to broadcast that wireless signal. A wireless access point connects to your router, usually over Ethernet, and communicates with your Ethernet-less devices over wireless frequencies. Most home users have routers with wireless access points built in, but standalone access points are still common for businesses, since you can pair multiple access points together to extend your network over a large area.</p> <p>Switch </p> <p>All routers come with built-in Ethernet ports, but depending on the size and class of router you buy, there may not be enough to plug in all your devices, especially in the age of smart home tech, which often require numerous, hard-wired base stations. If you run out of Ethernet ports on your router, a switch can add more Ethernet ports to your network. You just plug your extra devices into the switch, plug the switch into your router, and they'll appear on your network. Switch operates at layer 2.</p> <p>Note: you need a router in order to use a switch. A switch can't assign IP addresses or create a network like your router can\u2014it merely acts as a traffic cop for the signals coming through.</p>"},{"location":"networking/connecting_to_the_internet/#the-internet-hierarchy","title":"The Internet Hierarchy","text":"<p>Internet Service Provider (ISP) is a company which provides internet connection to end user, but there are basically three levels of ISP. There are 3 levels of ISP: Tier-1 ISP, Tier-2 ISP, and Tier-3 ISP. </p> <p>Tier 1 ISPs</p> <p>Tier 1 ISPs are the world's highest level of internet service providers. They own and operate the infrastructure that serves as the internet's backbone. These firms have huge networks of high-capacity fibre optic connections that traverse continents and cross seas to connect the world's main data centres. AT&amp;T, Sprint, Level 3 Communications, and Verizon are examples of Tier 1 ISPs.</p> <p>Tier 2 ISPs or Regional ISP</p> <p>Tier 2 ISPs provides internet services to a specific region or area. They connect Tier 1 networks to individual users or small businesses in different locations. These ISPs typically have smaller networks compared to Tier 1 providers, but they still play an important role in the hierarchy of the ISP network.</p> <p>Tier 3 ISPs</p> <p>Tier 3 ISPs are the smallest players in the ISP hierarchy, but they play an essential role in providing internet access to local communities. These providers cater to a specific customer base within a limited geographical area that can range from small towns to individual neighborhoods. They offer local connectivity, usually through cable or DSL lines, and are responsible for maintaining their own network infrastructure.</p> <p>Internet Exchange Points (IXP)</p> <p>An IXP is a physical location similar to a data center, where multiple ISPs and network operators interconnect their networks. By connecting to an IXP, each ISP can exchange traffic with all other ISPs that are connected to the IXP. These connections can be local, regional or international, depending on which ISPs participate.</p>"},{"location":"networking/connecting_to_the_internet/#the-internet-routing-hierarchy","title":"The Internet Routing Hierarchy","text":"<p>So how do packets find their way across the Internet? Does every computer connected to the Internet know where the other computers are? Do packets simply get <code>broadcast</code> to every computer on the Internet? The answer to both the preceeding questions is <code>no</code>. No computer knows where any of the other computers are, and packets do not get sent to every computer. The information used to get packets to their destinations are contained in routing tables kept by each router connected to the Internet.</p> <p>Routers are packet switches. A router is usually connected between networks to route packets between them. Each router knows about it's sub-networks and which IP addresses they use. </p> <p> </p> <p>When a packet arrives at a router, the router examines the IP address put there by the IP protocol layer on the originating computer. The router checks it's routing table. If the network containing the IP address is found, the packet is sent to that network. If the network containing the IP address is not found, then the router sends the packet on a default route, usually up the backbone hierarchy to the next router. Hopefully the next router will know where to send the packet.</p> <p>If it does not, again the packet is routed upwards until it reaches a NSP backbone. The routers connected to the NSP backbones hold the largest routing tables and here the packet will be routed to the correct backbone, where it will begin its journey downward through smaller and smaller networks until it finds it's destination.</p>"},{"location":"networking/deep_packet_inspection/","title":"Deep Packet Inspection","text":"<p>Deep Packet Inspection (DPI) is an advanced network traffic analysis technique used to inspect the contents of data packets as they travel through a network. Unlike traditional packet inspection, which only examines packet headers, DPI analyzes the entire packet, including the payload. This allows DPI to detect and take action against security threats, enforce policies, and optimize network performance.</p> <p>DPI operates across multiple layers of the OSI model, primarily at Layer 3 (Network), Layer 4 (Transport), and Layer 7 (Application).</p>"},{"location":"networking/deep_packet_inspection/#how-deep-packet-inspection-works","title":"How Deep Packet Inspection Works","text":"<p>Step 1: Packet Capture</p> <ul> <li>DPI begins with capturing network packets. These packets contain information about the communication between devices.</li> <li>Each packet carries data, including the source and destination IP addresses, port numbers, and payload.</li> </ul> <p></p> <p>Step 2: Header Analysis</p> <ul> <li>The system first examines the header (IP, TCP/UDP, and application-layer headers) to classify basic information, such as source and destination addresses, ports, and protocol type (HTTP, HTTPS, FTP, etc.).</li> <li>This step is similar to how traditional firewalls work.</li> </ul> <p>Step 3: Payload Inspection</p> <p>Unlike traditional firewalls, DPI decodes the packet\u2019s payload to analyze its contents:</p> <ul> <li>DPI examines the payload to identify the protocol being used (e.g., HTTP, HTTPS, FTP, etc.).</li> <li>It can also detect specific applications or services (e.g., streaming video, VoIP calls, file downloads).</li> </ul> <p>For example, if the packet is an HTTP request, DPI can extract:</p> <ul> <li>The URL being accessed.</li> <li>Any keywords (e.g., login credentials, credit card numbers).</li> <li>The file type being downloaded (e.g., .exe, .mp4).</li> </ul> <p>Step 4: Signature Matching</p> <ul> <li>DPI compares packet data against a database of known attack patterns, viruses, or predefined rules (e.g., Snort rules for Intrusion Detection Systems).</li> <li>Example: If a packet contains a known malware signature, it gets flagged or blocked.</li> </ul> <p>Step 5. Content Inspection</p> <ul> <li>DPI can inspect the actual content within packets.</li> <li>It can identify keywords, URLs, or even malware signatures.</li> <li>This level of inspection allows DPI to enforce security policies or prioritize traffic.</li> </ul> <p>Step 6: Policy Enforcement &amp; Action</p> <p>Once DPI completes its analysis, the system takes appropriate actions:</p> <ul> <li>Allow: If the packet is normal and follows security policies, it is forwarded.</li> <li>Block: If the packet is malicious (e.g., contains malware), it is dropped.</li> <li>Rate-limit: If traffic is excessive (e.g., video streaming consuming bandwidth), the system can throttle it.</li> <li>Redirect: If sensitive content is detected (e.g., unauthorized data transfer), DPI can redirect it to a logging or security system.</li> </ul>"},{"location":"networking/deep_packet_inspection/#use-cases-of-deep-packet-inspection","title":"Use Cases of Deep Packet Inspection","text":"<ol> <li> <p>Network Security</p> </li> <li> <p>Detect and block malware, viruses, and suspicious activities.</p> </li> <li>Prevent DDoS attacks by filtering malicious traffic.</li> <li> <p>Identify phishing attacks by inspecting URLs inside packets.</p> </li> <li> <p>Quality of Service (QoS)</p> </li> <li> <p>Identify and prioritize traffic for critical applications (VoIP, online meetings).</p> </li> <li> <p>Throttle non-essential services (e.g., P2P file sharing, torrents) to conserve bandwidth.</p> </li> <li> <p>Bandwidth Management</p> </li> <li> <p>By analyzing traffic patterns, DPI helps manage bandwidth effectively.</p> </li> <li> <p>It can throttle or block bandwidth-intensive applications during peak hours.</p> </li> <li> <p>Government Surveillance &amp; Censorship</p> </li> <li> <p>DPI can monitor and filter specific types of content (e.g., block websites, enforce regional restrictions).</p> </li> </ol>"},{"location":"networking/deep_packet_inspection/#how-dpi-works-at-each-osi-layer","title":"How DPI Works at Each OSI Layer","text":"OSI Layer DPI Functionality at This Layer Layer 3 (Network Layer) - Examines IP headers to determine the source and destination IP addresses.  - Can block or allow traffic based on IP address filtering. Layer 4 (Transport Layer) - Analyzes TCP/UDP headers to inspect port numbers and connection status.  - Can detect unusual connection patterns (e.g., port scanning, SYN floods). Layer 7 (Application Layer) - Inspects the payload (content) of the packet, analyzing application-layer protocols like HTTP, HTTPS, FTP, and DNS.  - Detects malware, data leaks, unauthorized applications, and policy violations.  - Can filter content based on keywords, file types, or specific URLs."},{"location":"networking/deep_packet_inspection/#reference","title":"Reference","text":"<ul> <li>Deep packet inspection</li> <li>Deep packet inspection (DPI)</li> <li>Unraveling the Secrets of Deep Packet Inspection: Decoding Network Traffic, Security, and Optimization</li> </ul>"},{"location":"networking/install_certbot_on_debian/","title":"Install Certbot on Debian with Cloudflare DNS","text":"<p>This guide will help you install Certbot on a Debian system and configure it to use the Cloudflare DNS plugin to manage SSL certificates.</p>"},{"location":"networking/install_certbot_on_debian/#step-1-sudo-apt-install-snapd","title":"Step 1: sudo apt install snapd","text":"<p>First, update your package list and install snapd:</p> <pre><code>sudo apt update\nsudo apt update\n</code></pre> <p>Once snapd is installed, exit the ssh terminal and re-login to ensure snap\u2019s paths are updated properly.</p>"},{"location":"networking/install_certbot_on_debian/#step-2-update-snapd","title":"Step 2: Update snapd","text":"<pre><code>sudo snap install core\n</code></pre>"},{"location":"networking/install_certbot_on_debian/#step-3-remove-certbot-auto-and-any-certbot-os-packages","title":"Step 3: Remove certbot-auto and any Certbot OS packages","text":"<p>If you have previously installed Certbot via other methods, such as using apt, you'll need to remove those versions to avoid conflicts:</p> <pre><code>sudo apt-get remove certbot\n</code></pre>"},{"location":"networking/install_certbot_on_debian/#step-4-install-certbot","title":"Step 4: Install certbot","text":"<p>Now, install Certbot via the snap package:</p> <pre><code>sudo snap install --classic certbot\n</code></pre> <p>To ensure the certbot command is available globally, create a symbolic link to the snap binary:</p> <pre><code>sudo ln -s /snap/bin/certbot /usr/bin/certbot\n</code></pre> <p>Run this command on the command line on the machine to acknowledge that the installed plugin will have the same classic containment as the Certbot snap.</p> <pre><code>sudo snap set certbot trust-plugin-with-root=ok\n</code></pre>"},{"location":"networking/install_certbot_on_debian/#step-5-install-cloudflare-dns-plugin","title":"Step 5: Install Cloudflare DNS plugin","text":"<p>To manage DNS challenges with Cloudflare, install the Cloudflare DNS plugin:</p> <pre><code>sudo snap install certbot-dns-cloudflare\n</code></pre>"},{"location":"networking/install_certbot_on_debian/#step-6-set-up-credentials","title":"Step 6: Set up credentials","text":"<p>Go to the Cloudflare dashboard and create a new API token. The Token needed by Certbot requires <code>Zone:DNS:Edit</code> permissions for only the zones you need certificates for. Copy the new API token value and save it in a file at <code>~/.secrets/certbot/cloudflare.ini</code></p> <pre><code># Cloudflare API token used by Certbot\ndns_cloudflare_api_token = 0123456789abcdef0123456789abcdef01234567\n</code></pre> <p>Make sure the file has restricted permissions</p> <pre><code>chmod 600 ~/.secrets/certbot/cloudflare.ini\n</code></pre>"},{"location":"networking/install_certbot_on_debian/#step-7-request-ssl-certificates","title":"Step 7: Request SSL Certificates","text":"<p>You can now use Certbot to request certificates for your domains using the Cloudflare DNS plugin.</p> <p>For a single domain (e.g., example.com):</p> <pre><code>certbot certonly \\\n  --dns-cloudflare \\\n  --dns-cloudflare-credentials ~/.secrets/certbot/cloudflare.ini \\\n  -d example.com\n</code></pre> <p>For multiple domains (e.g., example.com and www.example.com):</p> <pre><code>certbot certonly \\\n  --dns-cloudflare \\\n  --dns-cloudflare-credentials ~/.secrets/certbot/cloudflare.ini \\\n  -d example.com \\\n  -d www.example.com\n</code></pre> <p>For a wildcard certificate (e.g., *.example.com):</p> <pre><code>certbot certonly \\\n  --dns-cloudflare \\\n  --dns-cloudflare-credentials ~/.secrets/certbot/cloudflare.ini \\\n  -d *.example.com\n</code></pre> <p>If you're using Apache and want Certbot to automatically configure SSL for your sites, add the <code>-i apache</code> flag:</p> <pre><code>certbot \\\n  --dns-cloudflare \\\n  --dns-cloudflare-credentials ~/.secrets/certbot/cloudflare.ini \\\n  -i apache\n  -d *.example.com\n</code></pre> <p>After obtaining the SSL certificate, you need to configure Apache to serve your site over HTTPS. Ignore this step if you run certbot with <code>-i apache</code> option.</p> <ol> <li>Redirect HTTP to HTTPS: Add the following rewrite rules to the  block in your Apache configuration file (e.g., /etc/apache2/sites-available/example.com.conf): <pre><code>&lt;VirtualHost *:80&gt;\n    ServerName example.com\n    ServerAlias www.example.com\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =*.example.com.vn [OR]\n    RewriteCond %{SERVER_NAME} =wildcard.example.com\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n&lt;/VirtualHost&gt;\n</code></pre> <ol> <li>Enable SSL: In the  block, specify the paths to your SSL certificates: <pre><code>&lt;VirtualHost *:443&gt;\n    ServerName example.com\n    DocumentRoot /var/www/example.com\n\n    SSLCertificateFile /etc/letsencrypt/live/example.com/fullchain.pem\n    SSLCertificateKeyFile /etc/letsencrypt/live/example.com/privkey.pem\n    Include /etc/letsencrypt/options-ssl-apache.conf\n&lt;/VirtualHost&gt;\n</code></pre>"},{"location":"networking/install_certbot_on_debian/#step-8-test-automatic-renewal","title":"Step 8: Test automatic renewal","text":"<p>Certbot certificates are valid for 90 days, but you can set up automatic renewal. First, test if automatic renewal works by running a dry run:</p> <pre><code>sudo certbot renew --dry-run\n</code></pre> <p>To check the status of the renewal timer, use the following command:</p> <pre><code>systemctl list-timers\n</code></pre>"},{"location":"networking/nat/","title":"NAT","text":"<p>Network address translation (NAT) is the process of mapping private IP addresses to a single public IP address while information is being transferred via a router or NAT firewall. NAT is used by organizations with multiple devices needing access to the internet via a single public IP address.</p> <p></p>"},{"location":"networking/nat/#source-nat-snat","title":"Source NAT (SNAT)","text":"<p>SNAT changes the source IP address (internal users) of outgoing packets, typically used to:</p> <ul> <li>Share Internet Access: Allows multiple devices in a private network to use a single public IP address.</li> <li>Protect Internal IPs: Masks private IP addresses from the external network.</li> <li>Enable Routing Compliance: Adjusts source IP to meet ISP or routing requirements.</li> </ul> <p>How SNAT works:</p> <ul> <li>The source IP is replaced with a public IP (e.g., the router's IP).</li> <li>A mapping table tracks the original IP/port to correctly route returning traffic.</li> </ul> <p></p>"},{"location":"networking/nat/#destination-nat-dnat","title":"Destination NAT (DNAT)","text":"<p>DNAT is a technique that modifies the destination IP address and/or port of incoming packets. It is commonly used to route traffic from an external source to an internal resource behind a firewall or router.</p> <p>Primary Use Cases:</p> <ul> <li>Port Forwarding: Redirect incoming traffic to specific devices or services in a private network.<ul> <li>Example: Redirect HTTP requests on 203.0.113.1:80 to a web server at 192.168.1.100:80.</li> </ul> </li> <li>Load Balancing: Distribute traffic across multiple servers in a backend pool.</li> <li>Reverse Proxying: Forward requests from public endpoints to internal services.</li> </ul> <p>How DNAT works:</p> <ol> <li>A packet arrives at the router/firewall with a public destination IP.</li> <li>The router uses DNAT rules to change the destination IP and/or port to an internal IP and port.</li> <li>The packet is forwarded to the new destination.</li> <li>When the internal server responds, the router translates the destination back to the original public IP and port.</li> </ol> <p></p>"},{"location":"networking/nat/#nat-hairpinning","title":"NAT hairpinning","text":"<p>NAT Hairpinning, also known as NAT Loopback, is a feature in some routers that allows devices within a private network to access a service hosted on the same private network using the public IP address or domain name of the service.</p> <p>How it works:</p> <ol> <li>A device inside the network sends a request to the router using the public IP address or domain of the service (e.g., a web server).</li> <li>The router recognizes that the public IP resolves back to a service inside the same private network.</li> <li>The router <code>hairpins</code> the traffic, meaning it:<ul> <li>Forwards the request back into the private network to the correct internal device (via DNAT).</li> <li>Ensures that return traffic is properly routed back to the requesting device by modifying the packet headers as needed (via SNAT).</li> </ul> </li> </ol> <p></p>"},{"location":"networking/ssh_tunnel/","title":"SSH Tunnel","text":"<p>SSH tunneling is a method of transporting arbitrary networking data over an encrypted SSH connection.</p> <p>It can be used to: - Add encryption to legacy application - VPN</p>"},{"location":"networking/ssh_tunnel/#local-port-forwarding","title":"Local Port Forwarding","text":"<p>When a user needs to access a resource or service located on a remote server but is unable to do so directly because of firewall settings, network configurations or private network limitations, local port forwarding is utilized.</p> <pre><code>ssh -L [local_addr:]local_port:remote_addr:remote_port [user@]sshd_addr\n</code></pre> <p>What it actually means is:</p> <ul> <li>On your machine, the SSH client will start listening on local_port (likely, on localhost).</li> <li>Any traffic to this port will be forwarded to the <code>remote_private_addr:remote_port</code> on the machine you SSH-ed to.</li> </ul> <p> </p>"},{"location":"networking/ssh_tunnel/#remote-port-forwarding-reverse-tunneling","title":"Remote Port Forwarding (Reverse Tunneling)","text":"<p>Remote port forwarding, also known as reverse tunneling, is used when a user needs to allow external access to a service or application hosted on their local machine, typically behind a firewall or router, and make it accessible to a service or application running on a remote server.</p> <p>Remote port forwarding reroutes traffic from a specified port on the remote server to a designated port on the local machine. This is in contrast to local port forwarding, which forwards data from a local machine to a remote server.</p> <pre><code>ssh -R [remote_addr:]remote_port:local_addr:local_port [user@]gateway_addr\n</code></pre> <p>For example, suppose a user wants to allow access to a web server (running on port <code>8080</code>) hosted on their local machine (with private IP <code>192.168.1.10</code>) from a remote server with public IP <code>123.45.67.89</code>. The user can use remote port forwarding to redirect traffic from the remote server's port <code>80</code> to their local machine\u2019s port <code>8080</code>.</p> <pre><code>ssh -R 80:192.168.1.10:8080 user@123.45.67.89\n</code></pre> <p> </p>"},{"location":"networking/ssh_tunnel/#dynamic-port-forwarding","title":"Dynamic Port Forwarding","text":"<p>Dynamic port forwarding enables users to create a secure tunnel between their local machine and a remote SSH server, turning the SSH server into a proxy server.</p> <p>This technique enables users to establish a dynamic SOCKS proxy over an SSH connection. Dynamic port forwarding creates a general-purpose encrypted tunnel that can redirect traffic from many ports and applications over the SSH connection. This is different from local and remote port forwarding, which redirects specific ports to specific destinations.</p> <pre><code>ssh -D [local_port] [username]@[ssh_server]\n</code></pre> <p>After initiating the SSH connection with dynamic port forwarding, a SOCKS proxy is created on the specified local port (e.g., <code>1080</code>) on the client machine. Applications or services on the client machine can be configured to use this SOCKS proxy (<code>localhost:port</code>) as a gateway to route their network traffic through the SSH tunnel to the remote server. The SSH server forwards the traffic to the final destination addresses, acting as a mediator for all the connections initiated through the SOCKS proxy.</p>"},{"location":"networking/ssh_tunnel/#troubleshoot","title":"Troubleshoot","text":"<p>If you have problem while connect to ssh tunnel. Please make sure that both configurations <code>AllowTcpForwarding</code> and <code>GatewayPorts</code> are set to <code>yes</code></p>"},{"location":"networking/ssh_tunnel/#references","title":"References","text":"<ul> <li>SSH Tunneling Explained</li> <li>A Visual Guide to SSH Tunnels: Local and Remote Port Forwarding</li> </ul>"},{"location":"networking/tcp/","title":"TCP","text":""},{"location":"networking/tcp/#1-tcp-3-way-handshake","title":"1. TCP 3-way handshake","text":"<p>Step 1: The client sends a message with the SYN flag = 1 + random sequence number (SEQ = x) to the server.</p> <p>Step 2: After receiving the client's synchronization request, the server sends a message with the SYN flag = 1, ACK flag = 1, Ack number = Client's sequence number + 1 (x + 1) and server random sequence number (SEQ = y).</p> <p>Step 3: After receiving the SYN from the server, the client sends a message to server with ACK flag = 1, Ack number = Server sequence number + 1 (y + 1), SEQ = x + 1</p> <ul> <li>The first two handshakes SYN packets cannot carry data. They are using to establish the sequence number for both client and server.</li> <li>The third handshake to make sure that the client already received the SYN-ACK message from the server and it can carry data.</li> </ul>"},{"location":"networking/tcp/#2-tcp-waving-4-times","title":"2. TCP waving 4 times","text":"<p>Step 1: The client sends to the server a segment with the FIN flag = 1.</p> <p>Step 2: After receiving the client's FIN segment, the server immediately send ACK message to the client.</p> <p>Step 3: The server sends a segment with the FIN flag = 1 to the client.</p> <p>Step 4: After receiving the server's FIN segment, the client immediately send ACK message to the server and enters TIME_WAIT state. After the wait, the connection formally closes and all resources on the client side (including port numbers and buffer data) are released.</p> <ul> <li>TCP connection is full-duplex. There are two channels (read and write), and we must close these channels separately.</li> <li>After the step 2 the server can still send data to the client (connection is in haft-open state).</li> <li>The TIME_WAIT state lets the client resend the final acknowledgment in case the ACK is lost.</li> </ul>"},{"location":"networking/understanding_socket_and_port_in_tcp/","title":"Understanding socket and port in TCP","text":""},{"location":"networking/understanding_socket_and_port_in_tcp/#what-defines-a-unique-connection","title":"What defines a unique connection ?","text":"<p>There are 5 elements that identify a connection:</p> <ol> <li>Protocol (TCP/ UDP...)</li> <li>Source IP address</li> <li>Source port</li> <li>Target IP address</li> <li>Target port</li> </ol>"},{"location":"networking/understanding_socket_and_port_in_tcp/#does-tcp-listen-on-one-port-and-talk-on-another-port","title":"Does TCP listen on one port and talk on another port ?","text":"<p>No. TCP listens on 1 port and talk on that same port. If clients make multiple TCP connection to server. It\u2019s the client OS that must generate different random source ports, so that server can see them as unique connections</p>"},{"location":"networking/understanding_socket_and_port_in_tcp/#what-is-the-maximum-number-of-concurrent-tcp-connections-that-a-server-can-handle-in-theory","title":"What is the maximum number of concurrent TCP connections that a server can handle, in theory ?","text":"<p>A single listening port can accept more than one connection simultaneously</p> <p>If a client has many connections to the same port on the same destination, then four elements are the same (1) (2) (4) (5). Only source port varies to differentiate the different connections. Port are 16-bit numbers, therefore the maximum number of connections any given client can have to any given host port is 64K.</p> <p>However, multiple clients can each have up to 64K connections to some server\u2019s port, and if the server has multiple ports or either is multi-homed then you can multiply that further</p> <p>So the real limit is file descriptors. Each individual socket connection is given a file descriptor, so the limit is really the number of file descriptors that the system has been configured to allow and resources to handle. The maximum limit is typically up over 300K, but is configurable e.g. with sysctl.</p>"},{"location":"networking/understanding_socket_and_port_in_tcp/#concurrent-connection-request-vs-concurrent-open-connection","title":"Concurrent connection request vs Concurrent open connection","text":"<p>When clients want to make TCP connection with server, this request will be queued in server \u2018s backlog queue. This backlog queue size is small (about 5\u201310), and this size limits the number of concurrent connection requests. However, server quickly pick connection request from that queue and accept it. Connection request which is accepted are called open connection. The number of concurrent open connections is limited by server \u2018s resources allocated for file descriptor.</p>"},{"location":"networking/understanding_socket_and_port_in_tcp/#why-connection-is-rejected-after-successful-tcp-handshake","title":"Why connection is rejected after successful TCP handshake ?","text":"<p>When server receive connection request from client (by receiving SYN), it will then response with SYN, ACK, hence cause successful TCP handshake. But this request are stills in backlog queue.</p> <p>If the application process exceeds the limit of max file descriptors it can use, then when server calls accept, then it realizes that there are no file descriptors available to be the allocated for the socket and fails the accept call and the TCP connection sending a FIN to other side.</p>"},{"location":"networking/understanding_socket_and_port_in_tcp/#what-are-active-and-passive-socket","title":"What are active and passive socket ?","text":"<p>Sockets come in two primary flavors:</p> <ul> <li>An active socket is connected to a remote active socket via an open data connection\u2026</li> <li>A passive socket is not connected, but rather awaits an incoming connection, which will spawn a new active socket once a connection is established</li> </ul> <p>Each port can have a single passive socket binded to it, await\u00ading in\u00adcom\u00ading con\u00adnec\u00adtions, and mul\u00adti\u00adple active sockets, each cor\u00adre\u00adspond\u00ading to an open con\u00adnec\u00adtion on the port.</p>"},{"location":"networking/home_network/connecting-cloudflare-warp-with-wireguard/","title":"Connecting to Cloudflare WARP with WireGuard","text":"<ol> <li>Install wgcf</li> </ol> <pre><code>brew install wgcf\n</code></pre> <ol> <li>Register new account</li> </ol> <pre><code>wgcf register\n</code></pre> <p>The new account will be saved under <code>wgcf-account.toml</code></p> <ol> <li>Generate WireGuard profile</li> </ol> <pre><code>wgcf generate\n</code></pre> <p>The WireGuard profile will be saved under <code>wgcf-profile.conf</code></p> <ol> <li>Execute script mikrotik, the script is generated at https://mikrotik.dinhhuy258.dev/wireguard</li> </ol> <pre><code># Create Wireguard interface\n/interface wireguard\nadd name=warp-wireguard \\\n    private-key=\"private-key\" \\\n    listen-port=13233 \\\n    mtu=1280\n\n# Add a peer\n/interface wireguard peers\nadd name=warp-peer \\\n    interface=warp-wireguard \\\n    public-key=\"public-key\" \\\n    endpoint-address=engage.cloudflareclient.com \\\n    endpoint-port=2408 \\\n    allowed-address=0.0.0.0/0,::/0 \\\n    preshared-key=\"\"\n\n# Create address\n/ip address\nadd interface=warp-wireguard address=172.16.0.2/32\n\n# Create routing table\n/routing table\nadd disabled=no fib name=warp-wireguard\n\n# Create route\n/ip route\nadd disabled=no \\\n    dst-address=0.0.0.0/0 \\\n    gateway=warp-wireguard \\\n    routing-table=warp-wireguard \\\n    suppress-hw-offload=no\n\n# Create NAT rule\n/ip firewall nat\nadd chain=srcnat \\\n    out-interface=warp-wireguard \\\n    action=masquerade \\\n    comment=\"Cloudflare WARP's Wireguard\"\n\n# Create routing rule\n/routing rule\nadd action=lookup dst-address=0.0.0.0/0 table=warp-wireguard\n</code></pre>"},{"location":"networking/home_network/install-home-assistant-on-proxmox/","title":"Install Home Assistant on Proxmox","text":""},{"location":"networking/home_network/install-home-assistant-on-proxmox/#obtain-the-vm-image","title":"Obtain the VM image","text":"<ol> <li>Go to Home Assistant website and copy <code>KVM/Proxmox</code> link.</li> </ol> <ol> <li>Go to proxmox console and run following commands</li> </ol> <pre><code>wget &lt;ha_download_address&gt;\n\nunxz &lt;/path/to/file.qcow2.xz&gt;\n</code></pre>"},{"location":"networking/home_network/install-home-assistant-on-proxmox/#create-the-vm","title":"Create the VM","text":"<ol> <li> <p>General</p> </li> <li> <p>Select your <code>VM name</code> and <code>ID</code></p> </li> <li> <p>Select <code>Start at boot</code></p> </li> <li> <p>OS</p> </li> <li> <p>Select <code>Do not use any media</code></p> </li> <li> <p>System</p> </li> <li> <p>Change <code>Machine</code> to <code>q35</code></p> </li> <li>Change <code>BIOS</code> to <code>OVMF (UEFI)</code></li> <li>Select the <code>EFI storage</code> (typically <code>local-lvm</code>)</li> <li> <p>Uncheck <code>Pre-Enroll keys</code></p> </li> <li> <p>Disks</p> </li> <li> <p>Delete the SCSI drive and any other disks</p> </li> <li> <p>CPU</p> </li> <li> <p>Set minimum 2 cores</p> </li> <li> <p>Memory</p> </li> <li> <p>Set minimum 4096 MB</p> </li> <li> <p>Network</p> </li> <li> <p>Leave default unless you have special requirements (static, VLAN, etc)</p> </li> </ol> <p>Confirm and finish. Do NOT* start the VM yet.</p>"},{"location":"networking/home_network/install-home-assistant-on-proxmox/#add-the-image-to-the-vm","title":"Add the image to the VM","text":"<p>In the proxmox console run following commands</p> <pre><code>qm importdisk &lt;VM_ID&gt; &lt;/path/to/file.qcow2&gt; &lt;EFI location&gt;\n</code></pre> <p>For example</p> <pre><code>qm importdisk 105 /home/user/haos_ova-12.0.qcow2 local-lvm\n</code></pre>"},{"location":"networking/home_network/install-home-assistant-on-proxmox/#configure-ha-vm","title":"Configure HA VM","text":"<ol> <li> <p>Select your HA VM</p> </li> <li> <p>Go to the <code>Hardware</code> tab</p> </li> <li> <p>Select the <code>Unused Disk</code> and click the <code>Edit</code> button</p> </li> <li>Check the <code>Discard</code> box if you're using an SSD then click <code>Add</code></li> <li>Select the <code>Options</code> tab</li> <li>Select <code>Boot Order</code> and hit <code>Edit</code></li> <li>Check the newly created drive (likely <code>scsi0</code>) and uncheck everything else</li> </ol>"},{"location":"networking/home_network/install-home-assistant-on-proxmox/#finish-up","title":"Finish Up","text":"<ol> <li>Start the VM</li> <li>Check the shell of the VM. If it booted up correctly, you should be greeted with the link to access the Web UI.</li> </ol>"},{"location":"networking/home_network/install-xpenology-on-proxmox/","title":"Install XPEnology on Proxmox","text":""},{"location":"networking/home_network/install-xpenology-on-proxmox/#obtain-the-vm-image","title":"Obtain the VM image","text":"<ol> <li> <p>Go to Arc Loader website and copy <code>arc-{version}.img.zip</code> link.</p> </li> <li> <p>Go to proxmox console and run following commands</p> </li> </ol> <pre><code>wget &lt;arc_loader_download_address&gt;\n\nunzip &lt;/path/to/arc.img.zip&gt;\n</code></pre>"},{"location":"networking/home_network/install-xpenology-on-proxmox/#create-the-vm","title":"Create the VM","text":"<ol> <li> <p>General</p> </li> <li> <p>Select your <code>VM name</code> and <code>ID</code></p> </li> <li> <p>Select <code>Start at boot</code></p> </li> <li> <p>OS</p> </li> <li> <p>Select <code>Do not use any media</code></p> </li> <li> <p>System</p> </li> <li> <p>Leave default</p> </li> <li> <p>Disks</p> </li> <li> <p>Delete the SCSI drive and any other disks</p> </li> <li> <p>CPU</p> </li> <li> <p>Set minimum 2 cores</p> </li> <li> <p>Memory</p> </li> <li> <p>Set minimum 4096 MB</p> </li> <li> <p>Network</p> </li> <li> <p>Leave default unless you have special requirements (static, VLAN, etc)</p> </li> </ol> <p>Confirm and finish. Do *NOT** start the VM yet.</p>"},{"location":"networking/home_network/install-xpenology-on-proxmox/#add-the-image-to-the-vm","title":"Add the image to the VM","text":"<p>In the proxmox console run following commands</p> <pre><code>qm importdisk &lt;VM_ID&gt; &lt;/path/to/arc.img&gt; &lt;EFI location&gt;\n</code></pre> <p>For example</p> <pre><code>qm importdisk 106 /home/user/arc.img local-lvm\n</code></pre>"},{"location":"networking/home_network/install-xpenology-on-proxmox/#configure-xpenology-vm","title":"Configure XPEnology VM","text":"<p>Select your XPEnology VM and go to the <code>Hardware</code> tab. Select the <code>Unused Disk</code> and click the <code>Edit</code> button selecting <code>SATA</code> as <code>Bus/Device</code>.</p> <p></p> <p>Finally, we just need to set the <code>sata0</code> drive as the boot drive</p> <p></p>"},{"location":"networking/home_network/install-xpenology-on-proxmox/#configuring-arc","title":"Configuring Arc","text":"<p>Time to boot the VM and configure Arc. You can use the <code>Console</code> tab on the UI to interact with it.</p> <p></p> <p>On first boot, it will automatically boot into <code>Config Mode</code> which is exactly what we need</p> <p></p> <p>Let\u2019s start by choosing the Synology model we want to impersonate, in this case I will choose <code>DS920+</code>, then choose the latest DSM version and DSM build and press <code>OK</code></p> <p></p> <p>Select <code>No</code>.</p> <p></p> <p>Select <code>No - Install with random Serial/Mac</code></p> <p>Arc allows for a series of <code>add-ons</code> for things like removing limitations of DSM, or allow for extra hardware. I leave it default for now.</p> <p></p> <p>We\u2019re ready to build the loader - select <code>Yes</code></p> <p></p> <p>After patching the DSM download, we get a final confirmation before rebooting</p> <p></p> <p>Once rebooted, the default boot option will be <code>DSM mode</code></p> <p></p>"},{"location":"networking/home_network/install-xpenology-on-proxmox/#configure-hdd","title":"Configure HDD","text":""},{"location":"networking/home_network/install-xpenology-on-proxmox/#adding-a-storage-device-to-xpenology-vm-in-proxmox","title":"Adding a Storage Device to XPEnology VM in Proxmox","text":"<p>To add a storage device to your XPEnology VM in proxmox, follow these steps:</p> <p>Step 1: List available disks</p> <p>Open the proxmox console and run the following command to list all available disks in your system:</p> <pre><code>lsblk |awk 'NR==1{print $0\" DEVICE-ID(S)\"}NR&gt;1{dev=$1;printf $0\" \";system(\"find /dev/disk/by-id -lname \\\"*\"dev\"\\\" -printf \\\" %p\\\"\");print \"\";}'|grep -v -E 'part|lvm'\n</code></pre> <p>This command outputs a list of disks along with their corresponding device IDs. The output will look something like this:</p> <pre><code>NAME                         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS DEVICE-ID(S)\nsda                            8:0    0   1.8T  0 disk   /dev/disk/by-id/wwn-0x50014ee26bd596b7 /dev/disk/by-id/ata-WDC_WD20EFPX-68C4TN0_WD-WX32D94J0KY1\nnvme0n1                      259:0    0 238.5G  0 disk   /dev/disk/by-id/nvme-eui.e8238fa6bf530001001b448b4cbb7a08 /dev/disk/by-id/nvme-WD_PC_SN740_SDDPNQD-256G-1006_234222800596 /dev/disk/by-id/nvme-WD_PC_SN740_SDDPNQD-256G-1006_234222800596_1\n</code></pre> <p>Step 2: Identify the disk to passthrough</p> <p>Locate the hard drive ID that you want to passthrough to your XPEnology VM. For example, if you want to passthrough the following disk <code>ata-WDC_WD20EFPX-68C4TN0_WD-WX32D94J0KY1</code></p> <p>Step 3: Passthrough the disk to XPEnology VM</p> <pre><code>qm set &lt;VM_ID&gt; -sata3 /dev/disk/by-id/ata-WDC_WD20EFPX-68C4TN0_WD-WX32D94J0KY1\n</code></pre> <p>Replace <code>&lt;VM_ID&gt;</code> with your VM\u2019s ID.</p> <p>Step 4: Restart the VM</p> <p>Finally, restart the VM to apply the changes.</p>"},{"location":"networking/home_network/install-xpenology-on-proxmox/#passthrough-hdd-serial-number","title":"Passthrough HDD Serial Number","text":"<p>Install lshw to verify the HDD serial:</p> <pre><code>apt install lshw\n</code></pre> <p>Execute following command:</p> <pre><code>lshw -C disk\n</code></pre> <p>Sample output:</p> <pre><code>*-disk\n       description: ATA Disk\n       product: WDC WD20EFPX-68C\n       vendor: Western Digital\n       physical id: 0.0.0\n       bus info: scsi@0:0.0.0\n       logical name: /dev/sda\n       version: 0A81\n       serial: WD-WX32D94J0KY1\n       size: 1863GiB (2TB)\n       configuration: ansiversion=5 logicalsectorsize=512 sectorsize=4096\n  *-namespace:0\n       description: NVMe disk\n       physical id: 0\n       logical name: hwmon1\n  *-namespace:1\n       description: NVMe disk\n       physical id: 2\n       logical name: /dev/ng0n1\n  *-namespace:2\n       description: NVMe disk\n       physical id: 1\n       bus info: nvme@0:1\n       logical name: /dev/nvme0n1\n       size: 238GiB (256GB)\n       capabilities: gpt-1.00 partitioned partitioned:gpt\n       configuration: guid=3cc91a1e-becf-49ff-8fd8-7366acbacc0e logicalsectorsize=512 sectorsize=512 wwid=eui.e8238fa6bf530001001b448b4cbb7a08\n</code></pre> <p>Open <code>/etc/pve/qemu-server/&lt;VM_ID&gt;.conf</code>, locate the <code>sata3</code> configuration, and add the serial:</p> <pre><code>sata3: /dev/disk/by-id/ata-WDC_WD20EFPX-68C4TN0_WD-WX32D94J0KY1,size=1953514584K,serial=WD-WX32D94J0KY1\n</code></pre>"},{"location":"networking/home_network/install-xpenology-on-proxmox/#note","title":"Note","text":"<p>You can assign static IP address for XPEnology VM in mikrotik using following script</p> <pre><code>/ip dhcp-server lease\nadd address=192.168.0.6 mac-address=BC:24:11:BC:C0:26 server=dhcp-bridge-lan comment=\"NAS\"\n</code></pre>"},{"location":"networking/home_network/install-xpenology-on-proxmox/#references","title":"References","text":"<ul> <li>https://pve.proxmox.com/wiki/PassthroughPhysical_Disk_to_Virtual_Machine(VM)</li> </ul>"},{"location":"networking/home_network/setup_ecmp_on_mikrotik_router/","title":"Setup ECMP on Mikrotik router","text":""},{"location":"networking/home_network/setup_ecmp_on_mikrotik_router/#ecmp","title":"ECMP","text":"<p>ECMP (Equal-Cost Multi-Path) is a routing technique used in networking where multiple paths to a destination have the same cost. ECMP allows routers to load-balance traffic across these equal-cost paths, improving bandwidth utilization and redundancy.</p> <p>Benefits of ECMP</p> <ul> <li>Increased Bandwidth and Throughput: ECMP allows for the aggregation of bandwidth across multiple network links, effectively multiplying the capacity and enabling higher throughput for data traffic.</li> <li>Load Balancing: Traffic is distributed across multiple paths, which helps to avoid congestion and optimizes the usage of network resources. This distribution can help preven any single path from becoming a bottleneck.</li> <li>Redundancy: By having multiple paths available, ECMP enhances the network\u2019s ability to remain operational even if one or more paths fail.</li> <li>Scalability: As network demands grow, ECMP can handle increased traffic by simply adding more paths without needing significant changes to the network\u2019s core architecture.</li> </ul> <p>In a network with multiple paths to a destination, without ECMP, only one path is selected, leaving the other paths unused. This leads to all traffic being routed through a single path, causing bottlenecks and offering no redundancy if the path fails. After applying ECMP, traffic is evenly distributed across all equal-cost paths, reducing congestion and ensuring that if one path fails, the other paths can still carry the traffic, providing both better performance and fault tolerance.</p> <p>How ECMP works</p> <p>When traffic arrives at the router, the router applies a hashing algorithm to various fields in the packet (such as source/destination IP, source/destination port, etc.) to determine which of the available equal-cost paths to use. This ensures that packets belonging to the same flow (e.g., a TCP session) always take the same path to avoid out-of-order delivery, while different flows may take different paths. This load-balancing mechanism enhances bandwidth utilization and provides redundancy, ensuring that the network can continue operating efficiently even if one path fails.</p> <p>ECMP typically uses a hashing algorithm to determine the path for each flow, but other algorithms like round-robin, per-packet load balancing, or weighted round-robin can also be used to distribute traffic across multiple paths, depending on the network configuration. These algorithms offer different methods for balancing the load and managing traffic flows in the network.</p>"},{"location":"networking/home_network/setup_ecmp_on_mikrotik_router/#setup-ecmp","title":"Setup ECMP","text":"<ol> <li>Update the ip hash policy settings from <code>l3</code> (layer-3 hashing of src IP, dst IP) to <code>l4</code> (layer-3 hashing of src IP, dst IP). This allows for more efficient load balancing across multiple paths.</li> </ol> <pre><code>/ip settings\nset ipv4-multipath-hash-policy=l4\n/ipv6 settings\nset multipath-hash-policy=l4\n</code></pre> <ol> <li>Adjust the route distances in the <code>main</code> routing table to ensure that the distances are set incrementally.</li> </ol> <p>If the distances are not incremented, the <code>main</code> table will not show a <code>+</code> sign, indicating <code>ECMP</code> is not active. By default, the router selects the route with the lowest distance, and other paths are only used as fallback routes. We configure the <code>main</code> table this way because we do not want to apply <code>ECMP</code> in the main table. Some routing policies require specific traffic to avoid <code>ECMP</code> to prevent frequent IP changes, which could cause issues when connecting to certain services like banking...</p> <ol> <li>Create a new routing table for ECMP.</li> </ol> <pre><code>/routing table\nadd fib name=ecmp\n</code></pre> <ol> <li>Add routes to the new ECMP routing table.</li> </ol> <pre><code>/ip route\nadd dst-address=0.0.0.0/0 gateway=ether1-pppoe-out1 routing-table=ecmp\nadd dst-address=0.0.0.0/0 gateway=ether1-pppoe-out2 routing-table=ecmp\nadd dst-address=0.0.0.0/0 gateway=ether1-pppoe-out3 routing-table=ecmp\nadd dst-address=0.0.0.0/0 gateway=ether1-pppoe-out4 routing-table=ecmp\nadd dst-address=0.0.0.0/0 gateway=ether1-pppoe-out5 routing-table=ecmp\n</code></pre> <p>Make sure the distance values for all routes in the ECMP table are the same to enable equal-cost multi-path routing.</p> <ol> <li>Add routing rules to control traffic behavior.</li> </ol> <p>This rule should be put at the top.</p> <pre><code>/routing rule\nadd action=lookup disabled=no min-prefix=0 table=main\n</code></pre> <p>Route all traffic that should bypass ECMP to the main table.</p> <pre><code>/routing rule\nadd action=lookup dst-address=0.0.0.0/0 src-address=192.168.0.244/32 table=main comment=\"Thuy's IP\"\n\n/routing rule\nadd action=lookup dst-address=x.x.x.x/32 src-address=192.168.0.0/24 table=wireguard\n</code></pre> <p>Define rules for routing traffic that should use ECMP.</p> <pre><code>/routing rule\nadd action=lookup dst-address=0.0.0.0/0 src-address=192.168.0.0/24 table=ecmp\n</code></pre>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/","title":"Setup Wireguard VPN on Mikrotik router","text":"<p>In this document, we will set up a Wireguard VPN on a MikroTik router and configure the tunnel for use with a specific IP only.</p> <p>Please note: Enabling fasttrack along with Wireguard may cause slow requests to Wireguard. The current solution is to remove fasttrack rule.</p>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#setup-wireguard-vpn","title":"Setup Wireguard VPN","text":""},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#prepare-wireguard-client-file","title":"Prepare Wireguard client file","text":"<p>The WireGuard client configuration file should be formatted as follows:</p> <pre><code>[Interface]\nPrivateKey = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAddress = x.x.x.x/24\nDNS = 8.8.8.8, 8.8.4.4\n\n[Peer]\nPublicKey = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nPresharedKey = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nAllowedIPs = 0.0.0.0/0, ::/0\nPersistentKeepalive = 0\nEndpoint = x.x.x.x:x\n</code></pre> <ul> <li>PrivateKey: The private key of the client.</li> <li>Address: The IP address (with subnet) assigned to the client.</li> <li>DNS: The DNS servers to use.</li> <li>PublicKey: The public key of the server.</li> <li>PresharedKey: The preshared key for added security.</li> <li>AllowedIPs: Defines which IP addresses can be routed through the tunnel.</li> <li>Endpoint: The IP and port of the WireGuard server.</li> </ul>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#create-wireguard-interface-on-mikrotik","title":"Create WireGuard Interface on MikroTik","text":""},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#1-create-the-wireguard-interface","title":"1. Create the WireGuard Interface","text":"<p>In the <code>Private Key</code> field, input the <code>PrivateKey</code> from your client configuration file.</p>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#2-add-the-peer","title":"2. Add the Peer","text":"<p>Fill in the peer information using the details from the client configuration:</p> <ul> <li>Public Key: The server's public key.</li> <li>Endpoint: The IP address and port of the WireGuard server.</li> <li>Allowed IPs: Enter the address ranges allowed to use this tunnel (e.g., 0.0.0.0/0 for all traffic).</li> <li>Preshared Key: The preshared key (if used).</li> <li>Interface: The WireGuard interface created earlier.</li> </ul> <p>Note: Leave the <code>Private Key</code> empty</p> <p></p>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#3-create-address-list","title":"3. Create Address list","text":"<ol> <li>Go to IP &gt; Addresses.</li> <li>Create a new address list, the <code>Address</code> value can be obtained in the Wireguard client file, <code>Interface</code> value is the Wireguard interface.</li> </ol>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#routing-for-wireguard-traffic","title":"Routing for Wireguard traffic","text":""},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#1-create-a-routing-table","title":"1. Create a Routing Table","text":"<p>Go to <code>Routing</code> -&gt; <code>Tables</code></p> <p></p>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#2-create-route","title":"2. Create Route","text":"<p>Go to <code>IP</code> -&gt; <code>Routes</code></p> <p>Add a new route with the following settings:</p> <ul> <li>Dst Address: <code>0.0.0.0/0</code></li> <li>Gateway: The Wireguard interface</li> <li>Routing Table: Add a new route with the following settings:</li> </ul> <p></p>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#3-create-nat-rule","title":"3. Create NAT rule","text":"<p>Go to <code>IP</code> -&gt; <code>Firewall</code> -&gt; select tab <code>NAT</code></p> <p>Create a new NAT rule with the following settings:</p> <ul> <li>Chain: srcnat</li> <li>Out. Interface: The WireGuard interface</li> <li>Action: masquarage</li> </ul> <p>Note: Ensure this NAT rule is set as the second priority, below any existing PPPoE NAT rules.</p> <p></p> <p></p>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#4-create-mangle-rule","title":"4. Create Mangle rule","text":"<p>Create a mangle rule with the following concept: all packets destined for the target address will be routed to the WireGuard client. You can find the target IP address by using <code>host google.com</code> or <code>nslookup google.com</code>.</p> <p> </p>"},{"location":"networking/home_network/setup_wireguard_vpn_on_mikrotik_router/#mikrotik-script","title":"Mikrotik script","text":"<pre><code># Create Wireguard interface\n/interface wireguard\nadd name=my-company-wireguard \\\n    private-key=\"wireguard_interface_private_key\" \\\n    listen-port=13231 \\ \n    mtu=1420\n\n# Add a peer\n/interface wireguard peers\nadd name=my-company-peer \\\n    interface=my-company-wireguard \\\n    public-key=\"peer_public_key\" \\\n    endpoint-address=peer_endpoint_address \\\n    endpoint-port=peer_endpoint_port \\\n    allowed-address=0.0.0.0/0,::/0 \\\n    preshared-key=\"peer_preshared_key\"\n\n# Create address list\n/ip address\nadd interface=my-company-wireguard address=interface_address network=interface_address_network\n\n# Create routing table\n/routing table\nadd disabled=no fib name=output-my-company-wireguad\n\n# Create route\n/ip route\nadd disabled=no \\\n    dst-address=0.0.0.0/0 \\\n    gateway=my-company-wireguard \\\n    routing-table=output-my-company-wireguad \\\n    suppress-hw-offload=no\n\n# Create NAT rule\n/ip firewall nat\nadd chain=srcnat \\\n    out-interface=my-company-wireguard \\\n    action=masquerade \\\n    comment=\"My company's Wireguard\"\n\n# Create address list\n/ip firewall address-list\nadd list=my_company_address_list address=x.x.x.x comment=\"My company page\"\nadd list=my_company_address_list address=y.y.y.y comment=\"My company page\"\n\n# Create mangle firewall\n/ip firewall mangle\nadd action=mark-routing \\\n    chain=prerouting \\\n    dst-address-list=my_company_address_list \\\n    new-routing-mark=output-my-company-wireguad \\\n    passthrough=no \\\n    comment=\"My company Wireguard\"\n</code></pre>"},{"location":"networking/home_network/ssh-into-proxmox/","title":"SSH into Proxmox","text":""},{"location":"networking/home_network/ssh-into-proxmox/#enable-ssh-password-authentication-in-proxmox","title":"Enable SSH Password Authentication in Proxmox","text":"<ol> <li>Set a root password</li> </ol> <pre><code>sudo passwd root\n</code></pre> <p>You will be prompted to enter and confirm a new password for the root user.</p> <ol> <li>Enable SSH password authentication</li> </ol> <p>Edit the SSH configuration file <code>/etc/ssh/sshd_config</code> to allow root login. Change the line <code>PermitRootLogin without-password</code> to <code>PermitRootLogin yes</code></p> <ol> <li>Restart the SSH service</li> </ol> <pre><code>systemctl restart sshd\n</code></pre>"},{"location":"networking/home_network/ssh-into-proxmox/#ssh-into-proxmox-lxc-container","title":"SSH into Proxmox LXC Container","text":"<pre><code>lxc-attach --name &lt;VM_ID&gt;\n</code></pre> <p>Replace <code>&lt;VM_ID&gt;</code> with the actual ID of the container you wish to access. You will need to enter the root password you set earlier if prompted.</p>"},{"location":"networking/home_network/unlock_old_zte_modem/","title":"Unlock old ZTE modem","text":""},{"location":"networking/home_network/unlock_old_zte_modem/#change-serial-number-in-unlock-zte-modem","title":"Change serial number in unlock ZTE modem","text":"<ol> <li> <p>Download and Install Tools</p> </li> <li> <p>Download the ZTE modem tools from here.</p> </li> <li> <p>Access the Modem via Telnet</p> </li> <li> <p>Open your terminal.</p> </li> <li>Execute the following command to open the telnet session:      <pre><code>python3 zte_factroymode.py --user admin --pass 12345678 --ip 192.168.2.1 --port 80 telnet open\n</code></pre></li> <li> <p>This command will generate the username and password required for the telnet connection.</p> </li> <li> <p>Connect via Telnet</p> </li> <li> <p>In the terminal, type:      <pre><code>telnet\n</code></pre></p> </li> <li>Then, connect to the modem by entering:      <pre><code>open 192.168.2.1\n</code></pre></li> <li> <p>Use the credentials generated in the previous step to log in.</p> </li> <li> <p>Change the Serial Number</p> </li> <li> <p>To change the modem's serial number to <code>ZTEG00e0c8d7</code>, execute the following commands in the telnet session:      <pre><code>setmac 1 512 ZTEG00e0c8d7\nsetmac 1 2177 00e0c8d7\nreboot\n</code></pre></p> </li> <li> <p>Verify the Change</p> </li> <li> <p>After the modem reboots, log back into the modem's interface.</p> </li> <li>Navigate to the modem information section to verify that the serial number has been updated successfully.</li> </ol>"},{"location":"networking/home_network/vpn/vless/","title":"VLESS","text":"<p>VLESS (short for \"VMess Less\") is a modern, lightweight transport protocol designed for secure communication between clients and servers. It is widely used in VPN and proxy solutions, especially within the Xray and V2Ray frameworks, to bypass censorship and enhance privacy</p>"},{"location":"networking/home_network/vpn/vless/#how-vless-works","title":"How VLESS Works","text":"<ol> <li>Client-Server Model</li> </ol> <p>VLESS operates on a client-server model:</p> <ul> <li>The client connects to a VLESS server using a unique user ID.</li> <li> <p>The server verifies the ID and establishes a session.</p> </li> <li> <p>No Native Encryption</p> </li> </ul> <p>Unlike VMess, VLESS does not encrypt traffic by itself. Instead, it relies on TLS or XTLS for encryption and obfuscation, ensuring data confidentiality and making traffic harder to detect</p> <ol> <li>Stateless Operation</li> </ol> <p>VLESS is stateless, meaning the server does not store session data, which improves scalability and reduces resource usage.</p> <ol> <li>Simple Configuration</li> </ol> <p>A typical VLESS configuration requires:</p> <ul> <li>Server address and port</li> <li>User ID (UUID or custom string)</li> <li>Transport protocol (e.g., TCP, WebSocket)</li> <li>TLS/XTLS settings for encryption</li> </ul>"},{"location":"networking/home_network/vpn/vless/#vless-with-reality-secure-stealthy-tunneling","title":"VLESS with REALITY: Secure, Stealthy Tunneling","text":"<p>Combining VLESS with REALITY creates a powerful, censorship-resistant VPN/proxy tunnel. Here\u2019s how they work together:</p>"},{"location":"networking/home_network/vpn/vless/#roles-of-each-component","title":"Roles of Each Component","text":"<ul> <li>VLESS: A lightweight, stateless proxy protocol that handles authentication (via UUID) and data framing between client and server. It does not provide encryption itself and is designed to be efficient and flexible</li> <li>Reality: An advanced transport security layer that encrypts and obfuscates the traffic. Reality mimics real TLS handshakes to make proxy traffic indistinguishable from normal HTTPS traffic, providing strong privacy and making detection and blocking extremely difficult</li> </ul>"},{"location":"networking/home_network/vpn/vless/#how-the-combination-works","title":"How the Combination Works","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant Server\n\n    %% Step 1: Client initiates connection\n    Client-&gt;&gt;Server: Connect to Server:443 (TCP)\n\n    %% Step 2: Reality handshake\n    Client-&gt;&gt;Server: Reality handshake (mimics TLS handshake)\n    Server--&gt;&gt;Client: Reality handshake response (mimics TLS)\n\n    %% Step 3: Secure channel established\n    Note over Client,Server: Encrypted channel established (Reality)\n\n    %% Step 4: VLESS authentication\n    Client-&gt;&gt;Server: Send VLESS header (includes UUID)\n\n    %% Step 5: Server verifies UUID\n    Server--&gt;&gt;Client: Authentication success/failure\n\n    %% Step 6: Data transfer\n    Client-&gt;&gt;Server: Encrypted VLESS data (proxied traffic)\n    Server--&gt;&gt;Client: Encrypted VLESS data (proxied response)</code></pre> <p>Key Exchange Process</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Server\n\n    %% Step 1: Client generates ephemeral key pair\n    Client-&gt;&gt;Client: Generate ephemeral X25519 key pair&lt;br&gt;(private_client, public_client)\n\n    %% Step 2: Client sends public key to server\n    Client-&gt;&gt;Server: ClientHello&lt;br&gt;(Includes public_client in key_share extension)\n\n    %% Step 3: Server computes shared secret\n    Server-&gt;&gt;Server: Use static private_server + public_client&lt;br&gt;Compute shared_secret = X25519(private_server, public_client)\n\n    %% Step 4: Client computes shared secret\n    Client-&gt;&gt;Client: Use ephemeral private_client + pre-configured public_server&lt;br&gt;Compute shared_secret = X25519(private_client, public_server)\n\n    %% Step 5: Session key derivation\n    Note over Client,Server: Both derive session keys&lt;br&gt;using HKDF(shared_secret)\n\n    %% Step 6: Encrypted communication\n    Client-&gt;&gt;Server: Encrypted data with session key\n    Server-&gt;&gt;Client: Encrypted data with session key</code></pre>"},{"location":"networking/home_network/vpn/xtls_reality/","title":"How Does XTLS REALITY Work?","text":""},{"location":"networking/home_network/vpn/xtls_reality/#what-is-sni","title":"What is SNI?","text":"<p>Server Name Indication (SNI) is an extension to the TLS (Transport Layer Security) protocol that allows a client (such as a web browser) to specify the hostname (domain name) it wants to connect to during the initial TLS handshake. This enables a server to present the correct SSL/TLS certificate for the requested domain, even when multiple domains are hosted on the same IP address and port</p> <p>Before SNI, each HTTPS website with its own SSL certificate required a unique IP address. This was inefficient and contributed to the exhaustion of IPv4 addresses. SNI solves this problem by allowing multiple domains, each with their own certificate, to share a single IP address. This makes web hosting more efficient, cost-effective, and scalable, and helps conserve valuable IP resources.</p> <p>Without SNI, a server wouldn't know which certificate to present during the handshake, potentially causing errors or security warnings for users</p> <p>How SNI Works</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Server\n\n    Client-&gt;&gt;Server: Initiate TCP connection\n    Client-&gt;&gt;Server: Send TLS Client Hello (includes SNI: domain name)\n    Server-&gt;&gt;Server: Check SNI field for requested domain\n    Server-&gt;&gt;Client: Present matching SSL/TLS certificate\n    Client-&gt;&gt;Server: Complete TLS handshake\n    Client-&gt;&gt;Server: Send HTTPS request\n    Server--&gt;&gt;Client: Send HTTPS response</code></pre>"},{"location":"networking/home_network/vpn/xtls_reality/#sni-whitelist","title":"SNI Whitelist","text":"<p>SNI whitelist refers to a security or network policy mechanism that uses the Server Name Indication (SNI) field in the TLS handshake to determine which HTTPS connections are allowed or bypassed by a firewall, proxy, or other security device.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant Firewall/Proxy\n    participant Server\n\n    Client-&gt;&gt;Firewall/Proxy: Initiate TCP connection\n    Client-&gt;&gt;Firewall/Proxy: Send TLS Client Hello (includes SNI)\n    Firewall/Proxy-&gt;&gt;Firewall/Proxy: Inspect SNI field\n    alt SNI matches whitelist\n        Firewall/Proxy-&gt;&gt;Server: Forward TLS handshake (no interception)\n        Server--&gt;&gt;Client: Complete TLS handshake\n        Client-&gt;&gt;Server: Send HTTPS request\n        Server--&gt;&gt;Client: Send HTTPS response\n    else SNI not in whitelist\n        Firewall/Proxy-&gt;&gt;Client: Intercept or block connection\n    end</code></pre> <p>[!NOTE] SNI whitelisting only works when the SNI is transmitted in plaintext. If a client uses TLS 1.3 with ESNI (Encrypted SNI) or ECH (Encrypted Client Hello), the SNI field is encrypted and cannot be inspected by intermediaries, rendering SNI whitelisting ineffective. In response, some censorship systems (such as the Great Firewall of China) block all connections using ESNI or ECH to maintain control over accessible domains</p>"},{"location":"networking/home_network/vpn/xtls_reality/#tls-handshake","title":"TLS Handshake","text":"<ol> <li> <p>Client sends ClientHello (plaintext)</p> </li> <li> <p>Includes the client's key share for key exchange (e.g., ECDHE public value) and SNI.</p> </li> <li> <p>Server sends ServerHello (plaintext)</p> </li> <li> <p>Includes the server's key share for key exchange (e.g., ECDHE public value).</p> </li> <li> <p>Both client and server compute the shared secret (pre-master secret)</p> </li> <li> <p>Using their own private key and the other party's public key, both sides compute the same pre-master secret via ECDHE</p> </li> <li> <p>Both sides create the master key and session keys</p> </li> <li> <p>The client and server use the shared secret from the key exchange, combined with random numbers sent earlier, to generate the main encryption keys using a secure algorithm (HKDF).</p> </li> <li> <p>This step happens before any encrypted handshake messages are sent.</p> </li> <li> <p>Server sends encrypted handshake messages</p> </li> <li> <p>Server sends EncryptedExtensions, Certificate, CertificateVerify, and Finished messages, all encrypted with the handshake keys.</p> </li> <li> <p>Client verifies server and sends its own encrypted messages</p> </li> <li> <p>Client verifies server certificate and Finished message, and (if requested) sends its own Certificate and CertificateVerify, then sends its Finished message\u2014all encrypted.</p> </li> <li> <p>Subsequent handshake messages are encrypted</p> </li> <li> <p>Both sides use the derived session keys to encrypt all further application data.</p> </li> </ol> <p>[!NOTE] The preMasterKey is only used for internal calculation of the master key and session keys; it is not used to encrypt, authenticate, or exchange any data directly over the network.</p>"},{"location":"networking/home_network/vpn/xtls_reality/#xray-core-implementation","title":"Xray-core Implementation","text":""},{"location":"networking/home_network/vpn/xtls_reality/#client","title":"Client","text":"<p>In Xray-core, all packages responsible for encapsulating and transmitting proxy traffic are placed in the transport/internet directory, and the reality package is no exception:</p> <pre><code>Xray-core/\n|-- transport/\n| |-- internet/\n| | |-- reality/\n| | | |-- config.go\n| | | |-- config.pb.go\n| | | |-- config.proto\n| | | |-- reality.go\n</code></pre> <p>The key to initiating a REALITY connection is in reality.go, where func6 UClient is called when other packages initiate a REALITY connection, and is the most important functional entry point for the client part of the package:</p> <pre><code>func UClient(c net.Conn, config *Config, ctx context.Context, dest net.Destination) (net.Conn, error)\n</code></pre> <p>The <code>UClient</code> function is a complex implementation that creates a REALITY client connection. This function establishes a TLS connection using the uTLS library with REALITY protocol enhancements, which includes custom handshake modifications and anti-detection mechanisms.</p> <p>Initial Setup</p> <pre><code>localAddr := c.LocalAddr().String()\nuConn := &amp;UConn{}\nutlsConfig := &amp;utls.Config{\n    VerifyPeerCertificate:  uConn.VerifyPeerCertificate,\n    ServerName:             config.ServerName,\n    InsecureSkipVerify:     true,\n    SessionTicketsDisabled: true,\n    KeyLogWriter:           KeyLogWriterFromConfig(config),\n}\n</code></pre> <ul> <li>Creates a uTLS configuration with custom certificate verification (<code>VerifyPeerCertificate</code> verify the validity of the server certificate and the server identity.)</li> </ul> <p>Fingerprint Setup</p> <pre><code>fingerprint := tls.GetFingerprint(config.Fingerprint)\nuConn.UConn = utls.UClient(c, utlsConfig, *fingerprint)\n</code></pre> <ul> <li>Gets a TLS fingerprint to mimic specific browsers/clients</li> <li>Golang's ClientHello has a very unique fingerprint that stands out particularly on mobile clients where Go isn't popular. This makes circumvention tools built with standard Go TLS libraries easily identifiable and blockable with minimal collateral damage. We need to randomized fingerprint to avoid ClientHello request getting blocked.</li> </ul> <p>Custom Handshake</p> <p>REALITY clients use the Session ID field space in TLS Client Hello to covertly mark the client so that the server can distinguish between censors and legitimate REALITY clients. The Session ID field was originally used for the 0-RTT session resumption mechanism of TLS1.2. Although TLS1.3 switched to the session resumption mechanism based on PSK (Pre-shared Key), in order to maintain compatibility with TLS1.2 as much as possible, the Session ID field was retained while TLS1.3 was deprecated. Therefore, the Session ID used for each TLS1.3 connection should be randomly generated.</p> <pre><code>// Generate the default ClientHello and provide it to uConn\n// In this step, the client's TLS key pair is also generated\nuConn.BuildHandshakeState()\n// Assign the pointer of the generated default ClientHello to hello\nhello := uConn.HandshakeState.Hello\n// Assign a 32-byte empty slice (dynamic array) to SessionId\n// And fill it to the 40th-71th bytes of ClientHello, occupying the unit position\nhello.SessionId = make([]byte, 32)\ncopy(hello.Raw[39:], hello.SessionId) // the fixed location of `Session ID`\n// And fill it to the 40th-71th bytes of ClientHello, occupying the unit position\nhello.SessionId[0] = core.Version_x\nhello.SessionId[1] = core.Version_y\nhello.SessionId[2] = core.Version_z\nhello.SessionId[3] = 0 // reserved\n// Fill the current Unix timestamp into the 5th-8th bytes of SessionId\nbinary.BigEndian.PutUint32(hello.SessionId[4:], uint32(time.Now().Unix()))\n// Fill shortId from the 9th byte of SessionId\ncopy(hello.SessionId[8:], config.ShortId)\n\n// Convert the REALITY public key to a usable public key object\npublicKey, err := ecdh.X25519().NewPublicKey(config.PublicKey)\n\n// ...\n\n// Use the X25519 private key and REALITY public key in the client TLS key pair generated by BuildHandshakeState() to enter the ECDH algorithm to calculate the shared key.\nuConn.AuthKey, _ = uConn.HandshakeState.State13.KeyShareKeys.Ecdhe.ECDH(publicKey)\n\n// ...\n\n// Input the key into HKDF (key derivation function based on HMAC) to calculate preMasterKey.\nif _, err := hkdf.New(sha256.New, uConn.AuthKey, hello.Random[:20], []byte(\"REALITY\")).Read(uConn.AuthKey); err != nil {\n    return nil, err\n}\n\n// Uses AES-GCM to encrypt the session ID\nblock, _ := aes.NewCipher(uConn.AuthKey)\naead, _ := cipher.NewGCM(block)\naead.Seal(hello.SessionId[:0], hello.Random[20:], hello.SessionId[:16], hello.Raw)\n// Copy the final SessionId to bytes 40-72 of ClientHello\ncopy(hello.Raw[39:], hello.SessionId)\n\n// ...\n\n// Performs the actual TLS handshake with the modified ClientHello.\nif err := uConn.HandshakeContext(ctx); err != nil {\n    return nil, err\n}\n</code></pre> <p>Anti-Detection Spider Mechanism</p> <p>If the connection isn't verified (meaning it's likely being inspected), the function launches a sophisticated anti-detection mechanism:</p> <ol> <li>HTTP/2 Client Setup</li> </ol> <pre><code>client := &amp;http.Client{\n    Transport: &amp;http2.Transport{\n        DialTLSContext: func(ctx context.Context, network, addr string, cfg *gotls.Config) (net.Conn, error) {\n            errors.LogInfo(ctx, fmt.Sprintf(\"REALITY localAddr: %v\\tDialTLSContext\\n\", localAddr))\n            return uConn, nil\n        },\n    },\n}\n</code></pre> <ul> <li>Creates an HTTP/2 client that reuses the TLS connection</li> <li> <p>This simulates normal browser behavior</p> </li> <li> <p>Web Crawling Simulation</p> </li> </ul> <pre><code>prefix := []byte(\"https://\" + uConn.ServerName)\nmaps.Lock()\nif maps.maps == nil {\n    maps.maps = make(map[string]map[string]struct{})\n}\npaths := maps.maps[uConn.ServerName]\nif paths == nil {\n    paths = make(map[string]struct{})\n    paths[config.SpiderX] = struct{}{}\n    maps.maps[uConn.ServerName] = paths\n}\nfirstURL := string(prefix) + getPathLocked(paths)\nmaps.Unlock()\nget := func(first bool) {\n    var (\n        req  *http.Request\n        resp *http.Response\n        err  error\n        body []byte\n    )\n    if first {\n        req, _ = http.NewRequest(\"GET\", firstURL, nil)\n    } else {\n        maps.Lock()\n        req, _ = http.NewRequest(\"GET\", string(prefix)+getPathLocked(paths), nil)\n        maps.Unlock()\n    }\n    if req == nil {\n        return\n    }\n    req.Header.Set(\"User-Agent\", fingerprint.Client) // TODO: User-Agent map\n    if first &amp;&amp; config.Show {\n        errors.LogInfo(ctx, fmt.Sprintf(\"REALITY localAddr: %v\\treq.UserAgent(): %v\\n\", localAddr, req.UserAgent()))\n    }\n    times := 1\n    if !first {\n        times = int(crypto.RandBetween(config.SpiderY[4], config.SpiderY[5]))\n    }\n    for j := 0; j &lt; times; j++ {\n        if !first &amp;&amp; j == 0 {\n            req.Header.Set(\"Referer\", firstURL)\n        }\n        req.AddCookie(&amp;http.Cookie{Name: \"padding\", Value: strings.Repeat(\"0\", int(crypto.RandBetween(config.SpiderY[0], config.SpiderY[1])))})\n        if resp, err = client.Do(req); err != nil {\n            break\n        }\n        defer resp.Body.Close()\n        req.Header.Set(\"Referer\", req.URL.String())\n        if body, err = io.ReadAll(resp.Body); err != nil {\n            break\n        }\n        maps.Lock()\n        for _, m := range href.FindAllSubmatch(body, -1) {\n            m[1] = bytes.TrimPrefix(m[1], prefix)\n            if !bytes.Contains(m[1], dot) {\n                paths[string(m[1])] = struct{}{}\n            }\n        }\n        req.URL.Path = getPathLocked(paths)\n        if config.Show {\n            errors.LogInfo(ctx, fmt.Sprintf(\"REALITY localAddr: %v\\treq.Referer(): %v\\n\", localAddr, req.Referer()))\n            errors.LogInfo(ctx, fmt.Sprintf(\"REALITY localAddr: %v\\tlen(body): %v\\n\", localAddr, len(body)))\n            errors.LogInfo(ctx, fmt.Sprintf(\"REALITY localAddr: %v\\tlen(paths): %v\\n\", localAddr, len(paths)))\n        }\n        maps.Unlock()\n        if !first {\n            time.Sleep(time.Duration(crypto.RandBetween(config.SpiderY[6], config.SpiderY[7])) * time.Millisecond) // interval\n        }\n    }\n}\nget(true)\nconcurrency := int(crypto.RandBetween(config.SpiderY[2], config.SpiderY[3]))\nfor i := 0; i &lt; concurrency; i++ {\n    go get(false)\n}\n</code></pre> <ul> <li>Maintains a map of discovered URLs for each server</li> <li>Performs GET requests to simulate real browsing</li> <li>Extracts links from responses to build a realistic browsing pattern</li> <li>Uses random delays and multiple concurrent requests</li> <li> <p>Sets realistic headers including User-Agent and Referer</p> </li> <li> <p>Realistic Browsing Behavior:</p> </li> <li> <p>Random cookie padding</p> </li> <li>Multiple concurrent requests</li> <li>Link following with proper referer headers</li> <li>Random timing intervals</li> </ul>"},{"location":"networking/home_network/vpn/xtls_reality/#server","title":"Server","text":"<p>The key to the REALITY server processing TLS handshake is func Server in the file <code>tls.go</code>.</p> <ol> <li>Client Hello Analysis: Reads and validates the TLS Client Hello message</li> <li>Key Exchange: Uses X25519 key exchange with the client's key share</li> <li>Session ID Decryption:</li> <li>Derives an authentication key using HKDF</li> <li>Decrypts the client's session ID using AES-GCM</li> <li>Extracts client metadata (version, timestamp, short ID)</li> <li>Validation: Checks if the client meets configured criteria:</li> <li>Client version within allowed range</li> <li>Timestamp within acceptable time difference</li> <li>Valid short ID</li> </ol> <p>Based on authentication results:</p> <ul> <li>Valid Client: Processes the TLS handshake and establishes a secure tunnel</li> <li>Invalid Client: Forwards traffic directly to the target (appears as normal traffic)</li> </ul>"},{"location":"networking/home_network/vpn/xtls_reality/#references","title":"References","text":"<ul> <li>https://blog.cloudflare.com/handshake-encryption-endgame-an-ech-update/</li> <li>https://objshadow.pages.dev/en/posts/how-reality-works/</li> </ul>"},{"location":"programming/git/git_bisect/","title":"Find bugs with the git bisect command","text":"<p>Git Bisect requires just two pieces information before it can start the bug hunt with you:</p> <ul> <li>A revision where things were definitely good.</li> <li>A revision where the bug is present.</li> </ul> <p></p> <pre><code>git bisect start\ngit bisect bad HEAD\ngit bisect good fcd61994\n...\ngit bisect bad\ngit bisect good\ngit bisect reset # Exit the git bisect state and return to the original branch\n</code></pre> <p>We can automate the process of running git bisect by using script</p> <pre><code>#!/bin/bash\n# Run the test\ncommand_to_run_your_code\n\n# Save the exit code\nexit_code=$?\n\n# Exit with a non-zero exit code if any of the tests fail\nif [ $exit_code -ne 0 ]; \nthen\n  exit 1\nfi\n\n# Exit with a zero exit code if the tests succeed\nexit 0\n</code></pre> <pre><code>git bisect run test.sh\ngit bisect run yarn lint\n</code></pre>"},{"location":"programming/git/git_commands/","title":"Git commands","text":""},{"location":"programming/git/git_commands/#1-configuration","title":"1. Configuration","text":""},{"location":"programming/git/git_commands/#check-git-configuration","title":"Check git configuration","text":"<pre><code>git config -l\n</code></pre>"},{"location":"programming/git/git_commands/#setup-git-username-email","title":"Setup git username, email","text":""},{"location":"programming/git/git_commands/#repository-configuration","title":"Repository configuration","text":"<pre><code>git config user.name \"Huy Duong\"\ngit config user.email \"huy.duongdinh@gmail.com\"\n</code></pre>"},{"location":"programming/git/git_commands/#global-configuration","title":"Global configuration","text":"<pre><code>git config --global user.name \"Huy Duong\"\ngit config --global user.email \"huy.duongdinh@gmail.com\"\n</code></pre>"},{"location":"programming/git/git_commands/#2-branch","title":"2. Branch","text":""},{"location":"programming/git/git_commands/#list-branches","title":"List branches","text":""},{"location":"programming/git/git_commands/#local","title":"Local","text":"<pre><code>git branch\n</code></pre>"},{"location":"programming/git/git_commands/#remote","title":"Remote","text":"<pre><code>git branch -r\n</code></pre>"},{"location":"programming/git/git_commands/#rename-branch","title":"Rename branch","text":""},{"location":"programming/git/git_commands/#local_1","title":"Local","text":"<pre><code>git branch -m &lt;new_branch_name&gt;\n</code></pre>"},{"location":"programming/git/git_commands/#remote_1","title":"Remote","text":"<pre><code>git push origin --delete &lt;old_branch_name&gt;\ngit push origin -u &lt;new_branch_name&gt;\n</code></pre>"},{"location":"programming/git/git_commands/#delete-branch","title":"Delete branch","text":""},{"location":"programming/git/git_commands/#local_2","title":"Local","text":"<pre><code>git branch -d &lt;branch_name&gt;\n</code></pre>"},{"location":"programming/git/git_commands/#remote_2","title":"Remote","text":"<pre><code>git push origin --delete &lt;branch_name&gt;\n</code></pre>"},{"location":"programming/git/git_commands/#create-branch","title":"Create branch","text":"<pre><code>git branch -b &lt;branch_name&gt;\n</code></pre>"},{"location":"programming/git/git_commands/#3-revert","title":"3. Revert","text":""},{"location":"programming/git/git_commands/#revert-specific-commit","title":"Revert specific commit","text":"<pre><code>git revert [--no-commit] &lt;commit_id&gt;\n</code></pre>"},{"location":"programming/git/git_commands/#revert-file-to-specific-commit","title":"Revert file to specific commit","text":"<pre><code>git checkout &lt;commit_id&gt; -- &lt;file&gt;\n</code></pre>"},{"location":"programming/git/git_commands/#4-rebase","title":"4. Rebase","text":""},{"location":"programming/git/git_commands/#rebase-interactive-mode","title":"Rebase interactive mode","text":"<pre><code>git rebase -i &lt;branch_name&gt;\n</code></pre>"},{"location":"programming/git/git_commands/#5-submodule","title":"5. Submodule","text":""},{"location":"programming/git/git_commands/#add-submodule","title":"Add submodule","text":"<pre><code>git submodule add &lt;remote_url&gt;\n\ngit submodule add &lt;remote_url&gt; &lt;folder&gt;\n</code></pre>"},{"location":"programming/git/git_commands/#update-submodule","title":"Update submodule","text":"<pre><code>git submodule update --init --recursive\n</code></pre>"},{"location":"programming/git/git_commands/#adjust-commit-date","title":"Adjust commit date","text":"<pre><code>git commit --amend --date=\"Wed Apr 19 18:00 2023 +0700\" --no-edit\n</code></pre>"},{"location":"programming/golang/array_and_slice/","title":"Array and slice","text":""},{"location":"programming/golang/array_and_slice/#1-array","title":"1. Array","text":"<p>Arrays are fixed-length sequences of items of the same type. Arrays in Go can be created using the following syntaxes:</p> <pre><code>[N]Type\n[N]Type{value1, value2, ..., valueN}\n[...]Type{value1, value2, ..., valueN}\n</code></pre> <p>For example, the type <code>[4]int</code> represents an array of four integers.</p> <p></p>"},{"location":"programming/golang/array_and_slice/#2-slice","title":"2. Slice","text":""},{"location":"programming/golang/array_and_slice/#what-is-slice","title":"What is slice","text":"<p>A Slice is a segment of an array. Slices build on arrays and provide more power, flexibility, and convenience compared to arrays. Just like arrays, Slices are indexable and have a length. But unlike arrays, they can be resized.</p> <p>Slices can be created using the following syntaxes:</p> <pre><code>make([]Type, length, capacity)\nmake([]Type, length)\n[]Type{}\n[]Type{value1, value2, ..., valueN}\n</code></pre> <p>A slice of type T is declared using <code>[]T</code>. The slice is declared just like an array except that we do not specify any size in the brackets <code>[]</code>.</p> <p>Since a slice is a segment of an array, we can create a slice from an array.</p> <p>To create a slice from an array a, we specify two indices <code>low</code> (lower bound) and <code>high</code> (upper bound) separated by a colon -</p> <pre><code>a[low:high]\n</code></pre> <p>The above expression selects a slice from the array <code>a</code>. The resulting slice includes all the elements starting from index <code>low</code> to <code>high</code>, but excluding the element at index <code>high</code>.</p> <p>The <code>low</code> and <code>high</code> indices in the slice expression are optional. The default value for low is <code>0</code>, and high is the length of the slice.</p> <p>A slice can also be created by slicing an existing slice.</p>"},{"location":"programming/golang/array_and_slice/#modifying-the-slice","title":"Modifying the slice","text":"<p>Slices are reference types. They refer to an underlying array. Modifying the elements of a slice will modify the corresponding elements in the referenced array. Other slices that refer the same array will also see those modifications.</p>"},{"location":"programming/golang/array_and_slice/#length-and-capacity-of-a-slice","title":"Length and Capacity of a Slice","text":"<p>A slice consists of three things -</p> <ul> <li>A pointer (reference) to an underlying array.</li> <li>The length of the segment of the array that the slice contains.</li> <li>The capacity (the maximum size up to which the segment can grow).</li> </ul> <pre><code>type SliceHeader struct {\n    Data uintptr\n    Len  int\n    Cap  int\n}\n</code></pre> <p></p> <p>Let\u2019s consider the following array and the slice obtained from it as an example -</p> <pre><code>var a = [6]int{10, 20, 30, 40, 50, 60}\nvar s = [1:4]\n</code></pre> <p>Here is how the slice s in the above example is represented -</p> <p></p> <p>The length of the slice is the number of elements in the slice, which is 3 in the above example.</p> <p>The capacity is the number of elements in the underlying array starting from the first element in the slice. It is 5 in the above example.</p> <p>You can find the length and capacity of a slice using the built-in functions <code>len()</code> and <code>cap()</code></p>"},{"location":"programming/golang/array_and_slice/#zero-value-of-slices","title":"Zero value of slices","text":"<p>The zero value of a slice is nil. A nil slice doesn\u2019t have any underlying array, and has a length and capacity of 0</p> <pre><code>package main\nimport \"fmt\"\n\nfunc main() {\n    var s []int\n    fmt.Printf(\"s = %v, len = %d, cap = %d\\n\", s, len(s), cap(s))\n\n    if s == nil {\n        fmt.Println(\"s is nil\")\n    }\n}\n</code></pre> <p>Output:</p> <pre><code>s = [], len = 0, cap = 0\ns is nil\n</code></pre>"},{"location":"programming/golang/array_and_slice/#slice-functions","title":"Slice functions","text":""},{"location":"programming/golang/array_and_slice/#copy","title":"copy","text":"<p>The copy() function copies elements from one slice to another. Its signature looks like this -</p> <pre><code>func copy(dst, src []T) int\n</code></pre> <p>It takes two slices - a destination slice, and a source slice. It then copies elements from the source to the destination and returns the number of elements that are copied.</p> <p>The number of elements copied will be the minimum of len(src) and len(dst).</p>"},{"location":"programming/golang/array_and_slice/#append","title":"append","text":"<p>The append() function appends new elements at the end of a given slice. Following is the signature of append function.</p> <pre><code>func append(s []T, x ...T) []T\n</code></pre> <p>It takes a slice and a variable number of arguments x \u2026T. It then returns a new slice containing all the elements from the given slice as well as the new elements.</p> <p>If the given slice doesn\u2019t have sufficient capacity to accommodate new elements then a new underlying array is allocated with bigger capacity. All the elements from the underlying array of the existing slice are copied to this new array, and then the new elements are appended.</p> <p>However, if the slice has enough capacity to accommodate new elements, then the append() function re-uses its underlying array and appends new elements to the same array.</p>"},{"location":"programming/golang/array_and_slice/#iterating-over-a-slice","title":"Iterating over a slice","text":""},{"location":"programming/golang/array_and_slice/#using-for-loop","title":"Using for loop","text":"<pre><code>package main\nimport \"fmt\"\n\nfunc main() {\n    countries := []string{\"India\", \"America\", \"Russia\", \"England\"}\n\n    for i := 0; i &lt; len(countries); i++ {\n        fmt.Println(countries[i])\n    }\n}\n</code></pre>"},{"location":"programming/golang/array_and_slice/#using-range","title":"Using range","text":"<pre><code>package main\nimport \"fmt\"\n\nfunc main() {\n    primeNumbers := []int{2, 3, 5, 7, 11, 13, 17, 19, 23, 29}\n\n    for index, number := range primeNumbers {\n        fmt.Printf(\"PrimeNumber(%d) = %d\\n\", index+1, number)\n    }\n}\n</code></pre>"},{"location":"programming/golang/context/","title":"Context","text":""},{"location":"programming/golang/context/#1-introduction","title":"1. Introduction","text":"<p>In Go servers, each incoming request is handled in its own goroutine. Request handlers often start additional goroutines to access backends such as databases and RPC services. The set of goroutines working on a request typically needs access to request-specific values such as the identity of the end user, authorization tokens, and the request's deadline. When a request is canceled or times out, all the goroutines working on that request should exit quickly so the system can reclaim any resources they are using.</p> <p>A context package makes it easy to pass request-scoped values, cancelation signals, and deadlines across API boundaries to all the goroutines involved in handling a request.</p> <p></p> <p>A context package will help you in following problems:</p> <ul> <li>Let\u2019s say that you started a function and you need to pass some common parameters to the downstream functions. You cannot pass these common parameters each as an argument to all the downstream functions.</li> <li>You started a goroutine which in turn start more goroutines and so on. Suppose the task that you were doing is no longer needed. Then how to inform all child goroutines to gracefully exit so that resources can be freed up.</li> <li>A task should be finished within a specified timeout of say 2 seconds. If not it should gracefully exit or return.</li> <li>A task should be finished within a deadline eg it should end before 5 pm. If not finished then it should gracefully exit and return.</li> </ul> <p>Use cases:</p> <ul> <li>To pass data to the downstream. Eg. a HTTP request creates a <code>request_id</code>, <code>request_user</code> which needs to be passed around to all downstream functions for distributed tracing.</li> <li>When you want to halt the operation in the midway \u2013 A HTTP request should be stopped because the client disconnected.</li> <li>When you want to halt the operation within a specified time from start i.e with timeout \u2013 Eg- a HTTP request should be completed in 2 sec or else should be aborted.</li> <li>When you want to halt an operation before a certain time \u2013 Eg. A cron is running that needs to be aborted in 5 mins if not completed.</li> </ul>"},{"location":"programming/golang/context/#2-context","title":"2. Context","text":"<p>The core of the context package is the Context type:</p> <pre><code>// A Context carries a deadline, cancelation signal, and request-scoped values\n// across API boundaries. Its methods are safe for simultaneous use by multiple\n// goroutines.\ntype Context interface {\n    // Done returns a channel that is closed when this Context is canceled\n    // or times out.\n    Done() &lt;-chan struct{}\n\n    // Err indicates why this context was canceled, after the Done channel\n    // is closed.\n    Err() error\n\n    // Deadline returns the time when this Context will be canceled, if any.\n    Deadline() (deadline time.Time, ok bool)\n\n    // Value returns the value associated with key or nil if none.\n    Value(key interface{}) interface{}\n}\n</code></pre>"},{"location":"programming/golang/context/#3-derived-contexts","title":"3. Derived contexts","text":"<p>The context package provides functions to derive new Context values from existing ones. These values form a tree: when a Context is canceled, all Contexts derived from it are also canceled.</p> <p><code>Background</code> is the root of any Context tree; it is never canceled:</p> <pre><code>// Background returns an empty Context. It is never canceled, has no deadline,\n// and has no values. Background is typically used in main, init, and tests,\n// and as the top-level Context for incoming requests.\nfunc Background() Context\n</code></pre> <p><code>TODO</code> is a non-nil, empty Context. Code should use context.TODO when it's unclear which Context to use or it is not yet available</p> <pre><code>// TODO returns a non-nil, empty Context. Code should use context.TODO when\n// it's unclear which Context to use or it is not yet available (because the\n// surrounding function has not yet been extended to accept a Context\n// parameter).\nfunc TODO() Context\n</code></pre> <p>A derived context is can be created in 4 ways:</p> <pre><code>// A CancelFunc cancels a Context.\ntype CancelFunc func()\n\n// WithCancel returns a copy of parent whose Done channel is closed as soon as\n// parent.Done is closed or cancel is called.\nfunc WithCancel(parent Context) (ctx Context, cancel CancelFunc)\n\n// WithTimeout returns a copy of parent whose Done channel is closed as soon as\n// parent.Done is closed, cancel is called, or timeout elapses. The new\n// Context's Deadline is the sooner of now+timeout and the parent's deadline, if\n// any. If the timer is still running, the cancel function releases its\n// resources.\nfunc WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)\n\n// WithValue returns a copy of parent whose Value method returns val for key.\nfunc WithValue(parent Context, key interface{}, val interface{}) Context\n\n// WithDeadline returns a copy of the parent context with the deadline adjusted to be no later than d\nfunc WithDeadline(parent Context, d time.Time) (Context, CancelFunc)\n</code></pre>"},{"location":"programming/golang/context/#4-context-examples","title":"4. Context examples","text":"<p>We need cancellation to prevent our system from doing unnecessary work.</p> <p>Consider the common situation of an HTTP server making a call to a database, and returning the queried data to the client:</p> <p></p> <p>The timing diagram, if everything worked perfectly, would look like this:</p> <p></p> <p>But, what would happen if the client cancelled the request in the middle? This could happen if, for example, the client closed their browser mid-request.</p> <p>Without cancellation, the application server and database would continue to do their work, even though the result of that work would be wasted:</p> <p></p> <p>Ideally, we would want all downstream components of a process to halt, if we know that the process (in this example, the HTTP request) halted:</p> <p></p> <p>For example, lets consider an HTTP server that takes two seconds to process an event. If the request gets cancelled before that, we want to return immediately:</p> <pre><code>func main() {\n    // Create an HTTP server that listens on port 8000\n    http.ListenAndServe(\":8000\", http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        ctx := r.Context()\n        // This prints to STDOUT to show that processing has started\n        fmt.Fprint(os.Stdout, \"processing request\\n\")\n        // We use `select` to execute a piece of code depending on which\n        // channel receives a message first\n        select {\n        case &lt;-time.After(2 * time.Second):\n            // If we receive a message after 2 seconds\n            // that means the request has been processed\n            // We then write this as the response\n            w.Write([]byte(\"request processed\"))\n        case &lt;-ctx.Done():\n            // If the request gets cancelled, log it\n            // to STDERR\n            fmt.Fprint(os.Stderr, \"request cancelled\\n\")\n        }\n    }))\n}\n</code></pre> <p>An example using <code>WithCancel</code> context</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n)\n\nfunc task(ctx context.Context) {\n    i := 1\n    for {\n        select {\n        case &lt;-ctx.Done():\n            fmt.Println(\"Gracefully exit\")\n            fmt.Println(ctx.Err())\n            return\n        default:\n            fmt.Println(i)\n            time.Sleep(time.Second * 1)\n            i++\n        }\n    }\n}\n\nfunc main() {\n    ctx := context.Background()\n    cancelCtx, cancelFunc := context.WithCancel(ctx)\n    go task(cancelCtx)\n    time.Sleep(time.Second * 3)\n    cancelFunc()\n    time.Sleep(time.Second * 1)\n}\n</code></pre> <p>An example using <code>WithValue</code> context</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"github.com/google/uuid\"\n    \"net/http\"\n)\n\n//HelloWorld hellow world handler\nfunc HelloWorld(w http.ResponseWriter, r *http.Request) {\n    msgID := \"\"\n    if m := r.Context().Value(\"msgId\"); m != nil {\n        if value, ok := m.(string); ok {\n            msgID = value\n        }\n    }\n    w.Header().Add(\"msgId\", msgID)\n    w.Write([]byte(\"Hello, world\"))\n}\n\nfunc inejctMsgID(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        msgID := uuid.New().String()\n        ctx := context.WithValue(r.Context(), \"msgId\", msgID)\n        req := r.WithContext(ctx)\n        next.ServeHTTP(w, req)\n\n    })\n}\n\nfunc main() {\n    helloWorldHandler := http.HandlerFunc(HelloWorld)\n    http.Handle(\"/welcome\", inejctMsgID(helloWorldHandler))\n    http.ListenAndServe(\":8080\", nil)\n}\n</code></pre>"},{"location":"programming/golang/context/#5-bestpractices-and-caveats","title":"5. BestPractices and Caveats","text":"<ul> <li>Do not store a context within a struct type</li> <li>Context should flow through your program. For example, in case of an HTTP request, a new context can be created for each incoming request which can be used to hold a request_id or put some common information in the context like currently logged in user which might be useful for that particular request.</li> <li>Always pass context as the first argument to a function.</li> <li>Whenever you are not sure whether to use the context or not, it is better to use the context.ToDo() as a placeholder.</li> <li>Only the parent goroutine or function should the cancel context. Therefore do not pass the cancelFunc to downstream goroutines or functions. Golang will allow you to pass the cancelFunc around to child goroutines but it is not a recommended practice</li> </ul>"},{"location":"programming/golang/goroutine/","title":"Goroutine","text":""},{"location":"programming/golang/goroutine/#1-concurrency-vs-parallelism","title":"1. Concurrency vs Parallelism","text":"<p>Concurrency is when two or more tasks can start, run, and complete in overlapping time periods. It doesn't necessarily mean they'll ever both be running at the same instant.</p> <p></p> <p>Parallelism is when tasks literally run at the same time, e.g., on a multicore processor.</p> <p></p>"},{"location":"programming/golang/goroutine/#2-data-races-and-race-conditions","title":"2. Data races and race conditions","text":"<p>Data races is a situation, in which at least two threads access a shared variable at the same time. At least one thread tries to modify the variable.</p> <p>Race condition is a situation, in which the result of an operation depends on the interleaving of certain individual operations.</p> <p></p>"},{"location":"programming/golang/goroutine/#3-deadlocks","title":"3. Deadlocks","text":""},{"location":"programming/golang/goroutine/#what-is-a-deadlock","title":"What is a deadlock?","text":"<p>A deadlock occurs when all processes are blocked while waiting for each other and the program cannot proceed further.</p>"},{"location":"programming/golang/goroutine/#coffman-conditions","title":"Coffman conditions","text":"<p>There are four conditions, knows as the Coffman conditions that must be present simultaneously for a deadlock to occur:</p> <ul> <li>Mutual exclusion: A concurrent process holds at least one resource at any time making it non-sharable</li> <li>Hold and wait: A concurrent process holds a resource and is waiting for an additional resource</li> <li>No preemption: A resource held by a concurrent process cannot be taken away by the system. It can only be freed by the process holding it.</li> <li>Circle wait: A concurrent process must be waiting on a chain of other concurrent processes such that P1 is waiting on P2, P2 on P3 and so on, and there exists a Pn which is waiting for P1.</li> </ul> <p>In order to prevent deadlocks, we need to make sure that at least one of the conditions stated above should not hold.</p>"},{"location":"programming/golang/goroutine/#3-starvation","title":"3. Starvation","text":"<p>Starvation describes a situation where a thread is unable to gain regular access to shared resources and is unable to make progress. This happens when shared resources are made unavailable for long periods by greedy threads.</p> <p>For example, suppose an object provides a synchronized method that often takes a long time to return. If one thread invokes this method frequently, other threads that also need frequent synchronized access to the same object will often be blocked.</p>"},{"location":"programming/golang/goroutine/#4-goroutine","title":"4. Goroutine","text":"<p>A goroutine is a lightweight thread managed by the Go runtime.</p> <p>You can create a goroutine by using the following syntax</p> <pre><code>go f(x, y, z)\n</code></pre> <p>The current goroutine evaluates the input parameters to the function which are executed in the new goroutine. <code>main()</code> function is a goroutine which was invoked by the implicity created goroutine managed by Go runtime.</p>"},{"location":"programming/golang/goroutine/#5-channels","title":"5. Channels","text":"<p>Channel is a pipe between goroutines to synchronize excution and communicate by sending/receiving data</p> <pre><code>channelName := make(chan datatype)\n</code></pre> <p>The datatype is the type of data that you will pass on your channel.</p> <p>Eg:</p> <pre><code>channelName := make(chan int)\n</code></pre>"},{"location":"programming/golang/goroutine/#sending-on-a-channel","title":"Sending on a channel","text":"<pre><code>channelName&lt;-data\n</code></pre>"},{"location":"programming/golang/goroutine/#receiving-on-a-channel","title":"Receiving on a channel","text":"<pre><code>data := &lt;-channelName\n</code></pre> <p>By default, sends and receives block until the other side is ready. This allows goroutines to synchronize without explicit locks or condition variables.</p>"},{"location":"programming/golang/goroutine/#closing-a-channel","title":"Closing a channel","text":"<pre><code>close(channelName)\n</code></pre> <p>A sender can close a channel to indicate that no more values will be sent. Receivers can test whether a channel has been closed by assigning a second parameter to the receive expression:</p> <pre><code>v, ok := &lt;-ch\n</code></pre> <p><code>ok</code> is <code>false</code> if there are no more values to receive and the channel is closed.</p> <p>The loop <code>for i := range c</code> receives values from the channel repeatedly until it is closed.</p>"},{"location":"programming/golang/goroutine/#6-buffered-channels","title":"6. Buffered channels","text":"<p>Buffered channels are channels with a capacity/buffer. They can created with the following syntax:</p> <pre><code>channelName := make(chan datatype, capacity)\n</code></pre> <p>Sends to a buffered channel block only when the buffer is full. Receives block when the buffer is empty.</p>"},{"location":"programming/golang/goroutine/#7-select","title":"7. Select","text":"<p>The select statement lets a goroutine wait on multiple communication operations.</p> <p>A select blocks until one of its cases can run, then it executes that case. It chooses one at random if multiple are ready.</p> <pre><code>select {\n    case mess1 := &lt;-channel1:\n      fmt.Println(mess1)\n    case mess2 := &lt;-channel2:\n      fmt.Println(mess2)\n}\n</code></pre>"},{"location":"programming/golang/goroutine/#default-selection","title":"Default selection","text":"<p>The <code>default</code> case in a <code>select</code> is run if no other case is ready.</p> <pre><code>select {\n    case mess := &lt;-channel:\n      fmt.Println(mess)\n    default:\n      time.Sleep(50 * time.Millisecond)\n}\n</code></pre>"},{"location":"programming/golang/goroutine/#empty-select","title":"Empty select","text":"<pre><code>select {}\n</code></pre> <p>The empty select will block forever as there is no case statement to execute. It is similar to an empty <code>for {}</code> statement. On most supported Go architectures, the empty select will yield CPU. An empty for-loop won't, i.e. it will \"spin\" on 100% CPU.</p>"},{"location":"programming/golang/goroutine/#select-statement-with-timeout","title":"Select statement with timeout","text":"<p>Timeout in select can be achieved by using <code>After()</code> function of <code>time</code> package. Below is the signature of <code>After()</code> function.</p> <pre><code>func After(d Duration) &lt;-chan Time\n</code></pre> <p>The <code>After</code> function waits for <code>d</code> duration to finish and then it returns the current time on a channel.</p> <p>Example:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"time\"\n)\n\nfunc main() {\n    news := make(chan string)\n    go newsFeed(news)\n\n    printAllNews(news)\n}\n\nfunc printAllNews(news chan string) {\n    for {\n        select {\n        case n := &lt;-news:\n            fmt.Println(n)\n        case &lt;-time.After(time.Second * 1):\n            fmt.Println(\"Timeout: News feed finished\")\n            return\n        }\n    }\n}\n\nfunc newsFeed(ch chan string) {\n    for i := 0; i &lt; 2; i++ {\n        time.Sleep(time.Millisecond * 400)\n        ch &lt;- fmt.Sprintf(\"News: %d\", i+1)\n    }\n}\n</code></pre>"},{"location":"programming/golang/goroutine/#8-waitgroups","title":"8. WaitGroups","text":"<p>A WaitGroup blocks a program an waits for a set of goroutines to finish before moving to the next steps of excutions.</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"sync\"\n)\n\nfunc main() {\n    waitgroup := new(sync.WaitGroup)\n    waitgroup.Add(2)\n\n    go func() {\n        fmt.Println(\"Hello world 1\")\n        waitgroup.Done()\n    }()\n\n    go func() {\n        fmt.Println(\"Hello world 2\")\n        waitgroup.Done()\n    }()\n\n    waitgroup.Wait()\n\n    fmt.Println(\"Finished Execution\")\n}\n</code></pre>"},{"location":"programming/golang/goroutine/#9-mutex","title":"9. Mutex","text":""},{"location":"programming/golang/goroutine/#critical-section","title":"Critical section","text":"<p>When a program runs concurrently, the parts of code which modify shared resources should not be accessed by multiple Goroutines at the same time. This section of code that modifies shared resources is called critical section</p>"},{"location":"programming/golang/goroutine/#what-is-mutex","title":"What is mutex","text":"<p>A mutex prevents other processes from entering a critical section of data while a process occupies it.</p> <p>Go's standard library provides mutual exclusion with sync.Mutex and its two methods:</p> <ul> <li><code>Lock</code></li> <li><code>Unlock</code></li> </ul>"},{"location":"programming/golang/goroutine/#rwmutex","title":"RWMutex","text":"<p>A RWMutex is a reader/writer mutual exclusion lock. The lock can be held by an arbitrary number of readers or a single writer.</p> <ul> <li><code>Lock</code>: locks for writing. If the lock is already locked for reading or writing, Lock blocks until the lock is available.</li> <li><code>Unlock</code>: unlocks writing lock.</li> <li><code>RLock</code>: locks for reading. It should not be used for recursive read locking; a blocked Lock call excludes new readers from acquiring the lock.</li> <li><code>RUnlock</code>: RUnlock undoes a single RLock call; it does not affect other simultaneous readers.</li> </ul>"},{"location":"programming/golang/goroutine/#9-once-pool-cond","title":"9. Once, Pool, Cond","text":""},{"location":"programming/golang/goroutine/#once","title":"Once","text":"<p>Once is an object that performs an action only once.</p> <p>Implement a singleton pattern in Go:</p> <pre><code>package main\n\nimport (\n   \"fmt\"\n   \"sync\"\n)\n\ntype DbConnection struct {}\n\nvar (\n   dbConnOnce sync.Once\n   conn *DbConnection\n)\n\nfunc GetConnection() *DbConnection {\n   dbConnOnce.Do(func() {\n      conn = &amp;DbConnection{}\n      fmt.Println(\"Inside\")\n   })\n   fmt.Println(\"Outside\")\n   return conn\n}\n\nfunc main()  {\n   for i := 0; i&lt;5; i++ {\n      _ = GetConnection()\n      /*\n         Result is ...\n         Inside\n         Outside\n         Outside\n         Outside\n         Outside\n         Outside\n      */\n   }\n}\n</code></pre>"},{"location":"programming/golang/goroutine/#pool","title":"Pool","text":"<p>A Pool is a set of temporary objects that may be individually saved and retrieved. Pool's purpose is to cache allocated but unused items for later reuse, relieving pressure on the garbage collector.</p> <p>The public methods are:</p> <ul> <li><code>Get() interface{}</code> to retrieve an element</li> <li><code>Put(interface{})</code> to add an element</li> </ul> <pre><code>pool := &amp;sync.Pool{\n  New: func() interface{} {\n    return NewConnection()\n  },\n}\n\nconnection := pool.Get().(*Connection)\n</code></pre> <p>When shall we use sync.Pool? There are two use-cases:</p> <ul> <li>The first one is when we have to reuse shared and long-live objects like a DB connection for example.</li> <li>The second one is to optimize memory allocation.</li> </ul> <p>Eg:</p> <pre><code>func writeFile(pool *sync.Pool, filename string) error {\n    // Gets a buffer object\n    buf := pool.Get().(*bytes.Buffer)\n    // Returns the buffer into the pool\n    defer pool.Put(buf)\n\n    // Reset buffer otherwise it will contain \"foo\" during the first call\n    // Then \"foofoo\" etc.\n    buf.Reset()\n\n    buf.WriteString(\"foo\")\n\n    return ioutil.WriteFile(filename, buf.Bytes(), 0644)\n}\n</code></pre> <p>Note: Since a pointer can be put into the interface value returned by Get() without any allocation, it is preferable to put pointers than structures in the pool.</p>"},{"location":"programming/golang/goroutine/#cond","title":"Cond","text":"<p>Cond implements a condition variable, a rendezvous point for goroutines waiting for or announcing the occurrence of an event.</p> <p>Creating a sync.Cond requires a sync.Locker object (either a sync.Mutex or a sync.RWMutex):</p> <pre><code>cond := sync.NewCond(&amp;sync.RWMutex{})\n</code></pre>"},{"location":"programming/golang/goroutine/#10-concurrency-patterns","title":"10. Concurrency patterns","text":""},{"location":"programming/golang/goroutine/#generator-function","title":"Generator function","text":"<p>Generator Pattern is used to generate a sequence of values which is used to produce some output. This pattern is widely used to introduce parallelism into loops. This allows the consumer of the data produced by the generator to run in parallel when the generator function is busy computing the next value.</p> <pre><code>package main\n\nimport \"fmt\"\n\n// Generator func which produces data which might be computationally expensive.\nfunc fib(n int) chan int {\n    c := make(chan int)\n    go func() {\n        for i, j := 0, 1; i &lt; n; i, j = i+j, i {\n            c &lt;- i\n        }\n        close(c)\n    }()\n    return c\n}\n\nfunc main() {\n    // fib returns the fibonacci numbers lesser than 1000\n    for i := range fib(1000) {\n        // Consumer which consumes the data produced by the generator, which further does some extra computations\n        v := i * i\n        fmt.Println(v)\n    }\n}\n</code></pre>"},{"location":"programming/golang/goroutine/#futures","title":"Futures","text":"<p>A Future indicates any data that is needed in future but its computation can be started in parallel so that it can be fetched from the background when needed.</p> <p>Example:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\ntype data struct {\n    Body  []byte\n    Error error\n}\n\nfunc futureData(url string) &lt;-chan data {\n    c := make(chan data, 1)\n\n    go func() {\n        var body []byte\n        var err error\n\n        resp, err := http.Get(url)\n        defer resp.Body.Close()\n\n        body, err = ioutil.ReadAll(resp.Body)\n\n        c &lt;- data{Body: body, Error: err}\n    }()\n\n    return c\n}\n\nfunc main() {\n    future := futureData(\"http://test.future.com\")\n\n    // do many other things\n\n    body := &lt;-future\n    fmt.Printf(\"response: %#v\", string(body.Body))\n    fmt.Printf(\"error: %#v\", body.Error)\n}\n</code></pre> <p>The actual http request is done asynchronously in a goroutine. The main function can continue doing other things. When the result is needed, we read the result from the channel.</p>"},{"location":"programming/golang/goroutine/#fan-in-fan-out","title":"Fan-in, Fan-out","text":"<p>Fan-in Fan-out is a way of Multiplexing and Demultiplexing in golang.</p> <ul> <li>Fan-in refers to processing multiple input data and combining into a single entity.</li> <li>Fan-out is the exact opposite, dividing the data into multiple smaller chunks, distributing the work amongst a group of workers to parallelize CPU use and I/O.</li> </ul> <p></p> <p>For example we have a following program:</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"math/rand\"\n    \"time\"\n)\n\nfunc updatePosition(name string) &lt;-chan string {\n    positionChannel := make(chan string)\n\n    go func() {\n        for i := 0; ; i++ {\n            positionChannel &lt;- fmt.Sprintf(\"%s %d\", name, i)\n            time.Sleep(time.Duration(rand.Intn(1e3)) * time.Millisecond)\n        }\n    }()\n\n    return positionChannel\n}\n\nfunc main() {\n    positionChannel1 := updatePosition(\"Huy\")\n    positionChannel2 := updatePosition(\"Duong\")\n\n    for i := 0; i &lt; 5; i++ {\n        fmt.Println(&lt;-positionChannel1)\n        fmt.Println(&lt;-positionChannel2)\n    }\n}\n</code></pre> <p>What if the data in <code>positionChannel2</code> come first? It needs to wait for the data in positionChannel1. What if we want to get position updates as soon as the data is ready? This is where <code>fan-in</code> comes into play. By using this technique, we'll combine the inputs from both channels and send them through a single channel.</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"math/rand\"\n    \"time\"\n)\n\nfunc updatePosition(name string) &lt;-chan string {\n    positionChannel := make(chan string)\n\n    go func() {\n        for i := 0; ; i++ {\n            positionChannel &lt;- fmt.Sprintf(\"%s %d\", name, i)\n            time.Sleep(time.Duration(rand.Intn(1e3)) * time.Millisecond)\n        }\n    }()\n\n    return positionChannel\n}\n\nfunc fanIn(chan1, chan2 &lt;-chan string) &lt;-chan string {\n    channel := make(chan string)\n    go func() {\n        for {\n            channel &lt;- &lt;-chan1\n        }\n    }()\n    go func() {\n        for {\n            channel &lt;- &lt;-chan2\n        }\n    }()\n\n    return channel\n}\n\nfunc main() {\n    positionChannel := fanIn(updatePosition(\"Huy\"), updatePosition(\"Duong\"))\n\n    for i := 0; i &lt; 10; i++ {\n        fmt.Println(&lt;-positionChannel)\n    }\n}\n</code></pre> <p>Another example using <code>generator</code>, <code>fan-in</code> and <code>fan-out</code> pattern</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n)\n\nfunc main() {\n    randomNumbers := []int{13, 44, 56, 99, 9, 45, 67, 90, 78, 23}\n    // generate the common channel with inputs\n    inputChan := generatePipeline(randomNumbers)\n\n    // Fan-out to 2 Go-routine\n    c1 := squareNumber(inputChan)\n    c2 := squareNumber(inputChan)\n\n    // Fan-in the resulting squared numbers\n    c := fanIn(c1, c2)\n    sum := 0\n\n    // Do the summation\n    for i := 0; i &lt; len(randomNumbers); i++ {\n        sum += &lt;-c\n    }\n    fmt.Printf(\"Total Sum of Squares: %d\", sum)\n}\n\nfunc generatePipeline(numbers []int) &lt;-chan int {\n    out := make(chan int)\n    go func() {\n        for _, n := range numbers {\n            out &lt;- n\n        }\n        close(out)\n    }()\n    return out\n}\n\nfunc squareNumber(in &lt;-chan int) &lt;-chan int {\n    out := make(chan int)\n    go func() {\n        for n := range in {\n            out &lt;- n * n\n        }\n        close(out)\n    }()\n    return out\n}\n\nfunc fanIn(input1, input2 &lt;-chan int) &lt;-chan int {\n    c := make(chan int)\n    go func() {\n        for {\n            select {\n            case s := &lt;-input1:\n                c &lt;- s\n            case s := &lt;-input2:\n                c &lt;- s\n            }\n        }\n    }()\n    return c\n}\n</code></pre>"},{"location":"programming/golang/goroutine/#sequencing","title":"Sequencing","text":"<p>Imagine a cooking competition. You are participating in it with your partner. The rule of the game are:</p> <ol> <li>There are 3 rounds in the competition.</li> <li>In each round both partners will have to to come up with their own dishes.</li> <li>A player can not move to the next round until their partner is done with their dish.</li> <li>The judge will decide the entry to the next round after tasting food from both the team members.</li> </ol> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"math/rand\"\n    \"time\"\n)\n\ntype CookInfo struct {\n    foodCooked     string\n    waitForPartner chan bool\n}\n\nfunc cookFood(name string) &lt;-chan CookInfo {\n    cookChannel := make(chan CookInfo)\n    waitForPartner := make(chan bool)\n\n    go func() {\n        for round := 0; round &lt; 3; round++ {\n            time.Sleep(time.Duration(rand.Intn(1e3)) * time.Millisecond)\n\n            cookChannel &lt;- CookInfo{fmt.Sprintf(\"%s %s\\n\", name, \"Done\"), waitForPartner}\n\n            &lt;-waitForPartner\n        }\n    }()\n\n    return cookChannel\n}\n\nfunc fanIn(input1, input2 &lt;-chan CookInfo) &lt;-chan CookInfo {\n    c := make(chan CookInfo)\n    go func() {\n        for {\n            select {\n            case s := &lt;-input1:\n                c &lt;- s\n            case s := &lt;-input2:\n                c &lt;- s\n            }\n        }\n    }()\n\n    return c\n}\n\nfunc main() {\n    channel := fanIn(cookFood(\"Player 1\"), cookFood(\"Player 2\"))\n    for round := 0; round &lt; 3; round++ {\n        food1 := &lt;-channel\n        fmt.Printf(food1.foodCooked)\n\n        food2 := &lt;-channel\n        fmt.Printf(food2.foodCooked)\n\n        food1.waitForPartner &lt;- true\n        food2.waitForPartner &lt;- true\n\n        fmt.Printf(\"Done with round %d\\n\", round+1)\n    }\n\n    fmt.Printf(\"Done with the competition\\n\")\n}\n</code></pre>"},{"location":"programming/kotlin/coroutine/basics/","title":"Coroutine basics","text":"<p>A coroutine is an instance of suspendable computation. It is conceptually similar to a thread. However, a coroutine is not bound to any particular thread. It may suspend its execution in one thread and resume in another one.</p> <p>Coroutines can be thought of as light-weight threads, but there is a number of important differences that make their real-life usage very different from threads.</p>"},{"location":"programming/kotlin/coroutine/basics/#coroutine-builders","title":"Coroutine Builders","text":"<p>Coroutine builders are a way of creating coroutines. Since they are not suspending themselves, they can be called from non-suspending code or any other piece of code. They act as a link between the suspending and non-suspending parts of our code.</p>"},{"location":"programming/kotlin/coroutine/basics/#launch","title":"launch","text":"<p><code>launch</code> Launches a new coroutine without blocking the current thread and returns a reference to the coroutine as a Job. The coroutine is cancelled when the resulting job is cancelled.</p> <pre><code>fun main() {\n  GlobalScope.launch {\n    delay(200)\n    println(\"World\")\n  }\n  println(\"Hello\")\n  Thread.sleep(400)\n}\n</code></pre> <p>The launch function is an extension function on the interface <code>CoroutineScope</code>. This is a part of an important mechanism called structured concurrency, with the purpose of building a relationship between parent coroutine and child coroutine. Coroutines are launched in coroutine builders and are bound by a coroutine scope. A lazy way to launch a coroutine would be to use the <code>GlobalScope</code>. This means that the coroutine would be running for as long as the application is running and, if there is no important reason to do this, I believe that it\u2019s a way to wrongly use resources.</p> <pre><code>public fun CoroutineScope.launch(\n    context: CoroutineContext = EmptyCoroutineContext,\n    start: CoroutineStart = CoroutineStart.DEFAULT,\n    block: suspend CoroutineScope.() -&gt; Unit\n): Job\n</code></pre> <p><code>CoroutineContext</code> is a persistent dataset of contextual information about the current coroutine <code>CoroutineStart</code> is the mode in which you can start a coroutine.</p> <ul> <li>DEFAULT: Immediately schedules a coroutine for execution according to its context.</li> <li>LAZY: Starts the coroutine lazily, only when it is needed.</li> <li>ATOMIC: Same as DEFAULT but cannot be cancelled before it starts.</li> <li>UNDISPATCHED: Runs the coroutine until its first suspension point.</li> </ul>"},{"location":"programming/kotlin/coroutine/basics/#runblocking","title":"runblocking","text":"<p><code>runBlocking</code> is a coroutine builder that blocks the current thread until all tasks of the coroutine it creates, finish</p> <pre><code>fun main() = runBlocking { // The method bridges the non-coroutine world and the code with coroutines\n    launch { // launch a new coroutine without blocking the current thread\n        delay(1000L) // non-blocking delay for 1 second (default time unit is ms)\n        println(\"World!\") // print after delay\n    }\n    println(\"Hello\") // main coroutine continues while a previous one is delayed\n}\n</code></pre>"},{"location":"programming/kotlin/coroutine/basics/#async","title":"async","text":"<p><code>async</code> is a coroutine builder which returns some value to the caller. async can be used to perform an asynchronous task which returns a value</p> <p><code>await</code> is a suspending function that is called upon the async builder to fetch the value of the Deferred object that is returned.</p> <pre><code>fun main() = runBlocking {\n    val res = async {\n        \"Hello world\"\n    }\n\n    println(res.await())\n}\n</code></pre>"},{"location":"programming/kotlin/coroutine/basics/#withcontext","title":"withContext","text":"<p>Calls the specified suspending block with a given coroutine context, suspends until it completes, and returns the result.</p>"},{"location":"programming/kotlin/coroutine/basics/#structured-concurrency","title":"Structured Concurrency","text":"<p>If a coroutine is started on <code>GlobalScope</code>, the program will not wait for it. For example:</p> <pre><code>fun main() = runBlocking {\n    GlobalScope.launch {\n        delay(1000L)\n        println(\"World!\")\n    }\n    println(\"Hello,\")\n}\n\n// Hello\n</code></pre> <p>Everything changes if we get rid of the <code>GlobalScope</code> here. <code>runBlocking</code> creates a <code>CoroutineScope</code> and provides it as its lambda expression receiver. Thus we can call <code>this.launch</code> or simply <code>launch</code>. As a result, <code>launch</code> becomes a child of <code>runBlocking</code>. As parents might recognize, parental responsibility is to wait for all its children, so <code>runBlocking</code> will suspend until all its children are finished.</p>"},{"location":"programming/kotlin/coroutine/basics/#scope-builder","title":"Scope builder","text":"<p><code>CoroutineScope</code> is an interface that has a single abstract property called <code>coroutineContext</code>.</p> <pre><code>public interface CoroutineScope {\n    public val coroutineContext: CoroutineContext\n}\n</code></pre> <p>Whenever a new coroutine scope is created, a new job gets created and gets associated with it. Every coroutine created using this scope becomes the child of this job.</p>"},{"location":"programming/kotlin/coroutine/basics/#globalscope","title":"GlobalScope","text":"<p>This is a singleton and not associated with any Job. This launches top-level coroutines and highly discouraged to use because if you launch coroutines in the global scope, you lose all the benefits you get from structured concurrency.</p> <p>Reference: https://elizarov.medium.com/the-reason-to-avoid-globalscope-835337445abc</p>"},{"location":"programming/kotlin/coroutine/basics/#mainscope","title":"MainScope","text":"<p>This is useful for UI components, it creates <code>SupervisorJob</code> and all the coroutines created with this scope runs on the Main thread.</p>"},{"location":"programming/kotlin/coroutine/basics/#coroutinescopectx","title":"CoroutineScope(ctx)","text":"<p>This creates a coroutine scope from provided coroutine context and makes sure that a job is associated with this scope.</p> <pre><code>public fun CoroutineScope(context: CoroutineContext): CoroutineScope = ContextScope(if (context[Job] != null) context else context + Job())\n</code></pre>"},{"location":"programming/kotlin/coroutine/basics/#coroutinescope","title":"coroutineScope","text":"<p>Kotlin avoid thread blocking functions inside coroutine so due to this the use of <code>runBlocking</code> is discouraged from inside coroutines ans allow suspending operation instead of this. The suspending function is equivalent of <code>runBlocking</code> is the <code>coroutineScope</code> builder.</p> <p><code>CoroutineScope</code> suspends the currently working coroutine until all it\u2019s child coroutines have finished their execution.</p> <pre><code>fun main() = runBlocking {\n    doWorld()\n}\n\nsuspend fun doWorld() = coroutineScope {  // this: CoroutineScope\n    launch {\n        delay(1000L)\n        println(\"World!\")\n    }\n    println(\"Hello\")\n}\n</code></pre>"},{"location":"programming/kotlin/coroutine/context_and_dispatchers/","title":"Coroutine context and dispatchers","text":""},{"location":"programming/kotlin/coroutine/context_and_dispatchers/#coroutine-context","title":"Coroutine context","text":"<p>Coroutines always execute in some context represented by a value of the <code>CoroutineContext</code> type</p> <p>The coroutine context is a set of various elements:</p> <ul> <li>Job: A cancellable piece of work, which has a defined lifecycle</li> <li>ContinuationInterceptor: A mechanism which listens to the continuation within a coroutine and intercepts its resumption.</li> <li>CoroutineExceptionHandler: A construct which handles exceptions in coroutines.</li> </ul> <pre><code>fun main() {\n    val defaultDispatcher = Dispatchers.Default\n    val emptyParentJob = Job()\n    val combinedContext = defaultDispatcher + emptyParentJob\n    GlobalScope.launch(context = combinedContext) {\n        println(Thread.currentThread().name)\n    }\n    Thread.sleep(50)\n}\n</code></pre>"},{"location":"programming/kotlin/coroutine/context_and_dispatchers/#dispatchers-and-threads","title":"Dispatchers and threads","text":"<p>The coroutine context includes a coroutine dispatcher that determines what thread or threads the corresponding coroutine uses for its execution.</p> <p>All coroutine builders like <code>launch</code> and <code>async</code> accept an optional <code>CoroutineContext</code> parameter that can be used to explicitly specify the dispatcher for the new coroutine and other context elements.</p> <pre><code>/**\n * When `launch { ... }` is used without parameters, it inherits the context from the `CoroutineScope` it is being launched from.\n * In this case, it inherits the context of the main `runBlocking` coroutine which runs in the main thread.\n */\nlaunch { // context of the parent, main runBlocking coroutine\n    println(\"main runBlocking      : I'm working in thread ${Thread.currentThread().name}\")\n}\n// Dispatchers.Unconfined is a special dispatcher that also appears to run in the main thread.\nlaunch(Dispatchers.Unconfined) { // not confined -- will work with main thread\n    println(\"Unconfined            : I'm working in thread ${Thread.currentThread().name}\")\n}\nlaunch(Dispatchers.Default) { // will get dispatched to DefaultDispatcher\n    println(\"Default               : I'm working in thread ${Thread.currentThread().name}\")\n}\nlaunch(newSingleThreadContext(\"MyOwnThread\")) { // will get its own new thread\n    println(\"newSingleThreadContext: I'm working in thread ${Thread.currentThread().name}\")\n}\n</code></pre> <p>It produces the following output (maybe in different order):</p> <pre><code>Unconfined            : I'm working in thread main\nDefault               : I'm working in thread DefaultDispatcher-worker-1\nnewSingleThreadContext: I'm working in thread MyOwnThread\nmain runBlocking      : I'm working in thread main\n</code></pre> Thread used Max number of threads Useful when Dispatchers.Default thread pool Number of CPU cores Executing CPU-bound code Dispatchers.IO thread pool Defined in IO_PARALLELISM_PROPERTY_NAME. The default value is max(64, number of cpu cores) Executing IO-heavy code Dispatchers.Main main 1 Working with UI elements executorService.asCoroutineDispatcher() thread pool Defined by ExecuteService config Need to limit number of threads that the coroutine can run in runBlocking {...} Currently thread 1 Bridging blocking and suspending code"},{"location":"programming/kotlin/coroutine/context_and_dispatchers/#unconfined-vs-confined-dispatcher","title":"Unconfined vs confined dispatcher","text":"<p>The Dispatchers.Unconfined coroutine dispatcher starts a coroutine in the caller thread, but only until the first suspension point. After suspension it resumes the coroutine in the thread that is fully determined by the suspending function that was invoked.</p> <pre><code>launch(Dispatchers.Unconfined) { // not confined -- will work with main thread\n    println(\"Unconfined      : I'm working in thread ${Thread.currentThread().name}\")\n    delay(500)\n    println(\"Unconfined      : After delay in thread ${Thread.currentThread().name}\")\n}\nlaunch { // context of the parent, main runBlocking coroutine\n    println(\"main runBlocking: I'm working in thread ${Thread.currentThread().name}\")\n    delay(1000)\n    println(\"main runBlocking: After delay in thread ${Thread.currentThread().name}\")\n}\n</code></pre> <p>Produces the output:</p> <pre><code>Unconfined      : I'm working in thread main\nmain runBlocking: I'm working in thread main\nUnconfined      : After delay in thread kotlinx.coroutines.DefaultExecutor\nmain runBlocking: After delay in thread main\n</code></pre> <p>So, the coroutine with the context inherited from <code>runBlocking {...}</code> continues to execute in the <code>main</code> thread, while the unconfined one resumes in the default executor thread that the <code>delay</code> function is using.</p>"},{"location":"programming/kotlin/coroutine/flow/","title":"Flow","text":"<p>In coroutines, a flow is a type that can emit multiple values sequentially, as opposed to suspend functions that return only a single value. For example, you can use a flow to receive live updates from a database.</p> <p>There are three entities involved in streams of data:</p> <ul> <li>A producer produces data that is added to the stream. Thanks to coroutines, flows can also produce data asynchronously.</li> <li>(Optional) Intermediaries can modify each value emitted into the stream or the stream itself.</li> <li>A consumer consumes the values from the stream.</li> </ul> <p></p> <p>Flows are cold streams similar to sequences \u2014 the code inside a flow builder does not run until the flow is collected. </p>"},{"location":"programming/kotlin/coroutine/flow/#creating-a-flow","title":"Creating a flow","text":"<p>To create flows, use the flow builder APIs. The <code>flow</code> builder function creates a new flow where you can manually emit new values into the stream of data using the <code>emit</code> function.</p> <pre><code>fun simple(): Flow&lt;Int&gt; = flow { // flow builder\n    for (i in 1..3) {\n        delay(100) // pretend we are doing something useful here\n        emit(i) // emit next value\n    }\n}\n</code></pre>"},{"location":"programming/kotlin/coroutine/flow/#collecting-from-a-flow","title":"Collecting from a flow","text":"<pre><code>fun main() = runBlocking&lt;Unit&gt; {\n    // Collect the flow\n    simple().collect { value -&gt; println(value) }\n}\n</code></pre>"},{"location":"programming/kotlin/coroutine/flow/#modifying-the-stream","title":"Modifying the stream","text":"<p>Intermediaries can use intermediate operators to modify the stream of data without consuming the value.</p> <pre><code>suspend fun performRequest(request: Int): String {\n    delay(1000) // imitate long-running asynchronous work\n    return \"response $request\"\n}\n\nfun main() = runBlocking&lt;Unit&gt; {\n    (1..3).asFlow() // a flow of requests\n        .filter { it != 2 }\n        .map { request -&gt; performRequest(request) }\n        .collect { response -&gt; println(response) }\n}\n</code></pre> <p>You can also modify the flow using <code>transform</code> builtin function</p> <pre><code>(1..3).asFlow() // a flow of requests\n    .transform { request -&gt;\n        emit(\"Making request $request\") \n        emit(performRequest(request)) \n    }\n    .collect { response -&gt; println(response) }\n</code></pre>"},{"location":"programming/kotlin/coroutine/flow/#executing-in-a-different-coroutinecontext","title":"Executing in a different CoroutineContext","text":"<p>By default, the producer of a flow builder executes in the <code>CoroutineContext</code> of the coroutine that collects from it and you can not <code>emit</code> values from different <code>CoroutineContext</code>. This behavior might be undesirable in some cases.</p> <p>To change the CoroutineContext of a flow, use the intermediate operator <code>flowOn</code>. <code>flowOn</code> changes the <code>CoroutineContext</code> of the upstream flow, meaning the producer and any intermediate operators applied before (or above) <code>flowOn</code>. The downstream flow (the intermediate operators after <code>flowOn</code> along with the consumer) is not affected and executes on the <code>CoroutineContext</code> used to collect from the flow.</p> <pre><code>class NewsRepository(\n    private val newsRemoteDataSource: NewsRemoteDataSource,\n    private val userData: UserData,\n    private val defaultDispatcher: CoroutineDispatcher\n) {\n    val favoriteLatestNews: Flow&lt;List&lt;ArticleHeadline&gt;&gt; =\n        newsRemoteDataSource.latestNews\n            .map { news -&gt; // Executes on the default dispatcher\n                news.filter { userData.isFavoriteTopic(it) }\n            }\n            .onEach { news -&gt; // Executes on the default dispatcher\n                saveInCache(news)\n            }\n            // flowOn affects the upstream flow \u2191\n            .flowOn(defaultDispatcher)\n            // the downstream flow \u2193 is not affected\n            .catch { exception -&gt; // Executes in the consumer's context\n                emit(lastCachedNews())\n            }\n}\n</code></pre>"},{"location":"programming/tips/debugging-laravel-sail-with-xdebug/","title":"Debugging Laravel Sail with XDebug","text":""},{"location":"programming/tips/debugging-laravel-sail-with-xdebug/#configure-laravel-application","title":"Configure Laravel application","text":"<ol> <li>Update <code>.env</code> file to add these values</li> </ol> <pre><code>SAIL_XDEBUG_MODE=debug,develop\nSAIL_XDEBUG_CONFIG=\"client_host=host.docker.internal idekey=docker\"\n</code></pre> <ol> <li>Update <code>docker-compose.yml</code> file to add <code>PHP_IDE_CONFIG: 'serverName=Docker'</code> environment variable under <code>laravel.test</code></li> </ol> <ol> <li>To ensure that our docker setup is configured correctly you can temporarily add a new route to your web.php files as the following</li> </ol> <pre><code>Route::get('/phpinfo', function () {\n    return phpinfo();\n});\n</code></pre> <p>Go to <code>http://{your-domain}/phpinfo</code></p> <p>You should see similar values as of this:</p> <p> </p> <p>Great! Everything is set up</p>"},{"location":"programming/tips/debugging-laravel-sail-with-xdebug/#references","title":"References","text":"<ul> <li>https://blog.stackademic.com/debugging-laravel-sail-with-xdebug-3-in-phpstorm-2023-a-detailed-guide-84a594c09586 </li> </ul>"},{"location":"security/cors/","title":"CORS","text":"<p>Cross-origin resource sharing (CORS) is a browser mechanism which enables controlled access to resources located outside of a given domain.</p>"},{"location":"security/cors/#same-origin-policy-sop","title":"Same-origin policy (SOP)","text":"<p>The same-origin policy is a web browser security mechanism that aims to prevent websites from attacking each other.</p> <p>The same-origin policy restricts scripts on one origin from accessing data from another origin.</p> <p>An origin consists of a URI scheme, domain and port number.</p> <p></p>"},{"location":"security/cors/#relaxation-of-the-same-origin-policy","title":"Relaxation of the same-origin policy","text":"<p>The same-origin policy is very restrictive and consequently various approaches have been devised to circumvent the constraints.</p> <p>Many websites interact with subdomains or third-party sites in a way that requires full cross-origin access.</p> <p>A controlled relaxation of the same-origin policy is possible using cross-origin resource sharing (CORS).</p> <p>The cross-origin resource sharing protocol uses a suite of HTTP headers that define trusted web origins and associated properties such as whether authenticated access is permitted. These are combined in a header exchange between a browser and the cross-origin web site that it is trying to access.</p>"},{"location":"security/cors/#how-does-cors-work","title":"How does CORS work?","text":""},{"location":"security/cors/#simple-requests","title":"Simple requests","text":"<p>A simple request is one that meets all the following conditions:</p> <ul> <li>One of the allowed methods: GET, HEAD, POST</li> <li>A CORS safe-listed header is used</li> <li>When using the <code>Content-Type</code> header, only the following values are allowed: <code>application/x-www-form-urlencoded</code>, <code>multipart/form-data</code>, or <code>text/plain</code></li> <li>No event listeners are registered on any XMLHttpRequestUpload object</li> <li>No ReadableStream object is used in the request</li> </ul> <p>Step 1: Client (browser) request</p> <p>When the browser is making a cross-origin request, the browser adds an Origin header with the current origin (scheme, host, and port).</p> <p>Step 2: Server response</p> <p>On the server side, when a server sees this header, and wants to allow access, it needs to add an Access-Control-Allow-Origin header to the response specifying the requesting origin (or * to allow any origin.)</p> <p>Step 3: Browser receives response</p> <p>When the browser sees this response with an appropriate Access-Control-Allow-Origin header, the browser allows the response data to be shared with the client site.</p>"},{"location":"security/cors/#preflighted-requests","title":"Preflighted requests","text":"<p>Unlike simple requests, for \"preflighted\" requests the browser first sends an HTTP request using the OPTIONS method to the resource on the other origin, in order to determine if the actual request is safe to send. Such cross-origin requests are preflighted since they may have implications for user data.</p> <p>For example, a client might be asking a server if it would allow a DELETE request, before sending a DELETE request, by using a preflight request:</p> <pre><code>OPTIONS /resource/foo\nAccess-Control-Request-Method: DELETE\nAccess-Control-Request-Headers: origin, x-requested-with\nOrigin: https://foo.bar.org\n</code></pre> <p>If the server allows it, then it will respond to the preflight request with an Access-Control-Allow-Methods response header, which lists DELETE:</p> <pre><code>HTTP/1.1 204 No Content\nConnection: keep-alive\nAccess-Control-Allow-Origin: https://foo.bar.org\nAccess-Control-Allow-Methods: POST, GET, OPTIONS, DELETE\nAccess-Control-Max-Age: 86400\n</code></pre> <p>The preflight response can be optionally cached for the requests created in the same URL using Access-Control-Max-Age header like in the above example.</p>"},{"location":"security/cors/#cors-headers","title":"CORS headers","text":"<p>Request headers</p> Header Name Example Value Description Used in preflight requests Used in CORS requests Origin https://www.test.com Combination of protocol, domain, and port of the browser tab opened YES YES Access-Control-Request-Method POST For the preflight request, specifies what method the original CORS request will use YES NO Access-Control-Request-Headers Authorization, X-PING For the preflight request, a comma separated list specifying what headers the original CORS request will send YES NO <p>Response Headers</p> Header Name Example Value Description Used in preflight requests Used in CORS requests Access-Control-Allow-Origin https://www.test.com Item3.1 YES YES Access-Control-Allow-Credentials true Item3.2 YES YES Access-Control-Expose-Headers Date, X-Device-Id Column1 NO YES Access-Control-Max-Age 600 Item3.1 YES NO Access-Control-Allow-Methods GET, POST, PUT, DELETE Item3.2 YES NO Access-Control-Allow-Headers Authorization, X-PING Column1 YES NO"},{"location":"security/cors/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Using * operator for Access-Control-Allow-Origin.</li> </ol> <p>CORS is a relaxation of same-origin policy while attempting to remain secure. Using * disables most security rules of CORS. There are use cases where wildcard is OK such as an open API that integrates into many 3rd party websites.</p> <ol> <li>Returning multiple domains for Access-Control-Allow-Origin.</li> </ol> <p>Unfortunately, the spec does not allow Access-Control-Allow-Origin: https://mydomain.com, https://www.mydomain.com. The server can only respond with one domain or *, but you can leverage the Origin request header.</p> <ol> <li>Using wildcard selection like *.mydomain.com.</li> </ol> <p>This is not part of the CORS spec, wildcard can only be used to imply all domains are allowed.</p> <ol> <li>Not including protocol or non-standard ports.</li> </ol> <p><code>Access-Control-Allow-Origin: mydomain.com</code> is not valid since the protocol is not included.</p> <p>In a similar way, you will have trouble with <code>Access-Control-Allow-Origin: http://localhost</code> unless the server is actually running on a standard HTTP port like :80.</p> <ol> <li>Not including Origin in the Vary response header</li> </ol> <p>Most CORS frameworks do this automatically, you must specify to clients that server responses will differ based on the request origin.</p> <ol> <li>Not specifying the Access-Control-Expose-Headers</li> </ol> <p>If a required header is not included, the CORS request will still pass, but response headers not whitelisted will be hidden from the browser tab. The default response headers always exposed for CORS requests are:</p> <ul> <li>Cache-Control</li> <li>Content-Language</li> <li>Content-Type</li> <li>Expires</li> <li>Last-Modified</li> <li> <p>Pragma</p> </li> <li> <p>Using wildcard when Access-Control-Allow-Credentials is set to true</p> </li> </ul> <p>This is a tricky case that catches many people. If response has <code>Access-Control-Allow-Credentials: true</code>, then the wildcard operator cannot be used on any of the response headers like <code>Access-Control-Allow-Origin</code>.</p>"},{"location":"security/jwe/","title":"JWE","text":"<p>By default, JSON Web Tokens (JWTs) are base64url encoded JSON objects signed using a digital signing algorithm, but it does not provide you with confidentiality. Anyone can read the payload, which can be an issue if the token holds any sort of sensitive data. So, how do you prevent this? How do you encrypt a JWT?</p> <p>This is where JSON Web Encryption (JWE) comes in. JWE allows you to encrypt a JWT payload so that only the intended recipient can read it while still providing integrity and authentication checks.</p>"},{"location":"security/jwe/#jwe-structure","title":"JWE structure","text":"<p>A JWE token is built with five key components, each separated by a period (<code>.</code>)</p> <p></p>"},{"location":"security/jwe/#jose-header","title":"JOSE header","text":"<p>The JOSE header is the very first element of the JWE token and it stands for JSON Object Signing and Encryption. It is a base64url-encoded JSON object that includes:</p> <ul> <li>alg: (Required) Specifies the algorithm used to encrypt the Content Encryption Key (CEK). Common algorithms include RSA-OAEP, RSA1_5, A128KW, A256KW, etc.</li> <li>enc: (Required) Indicates the encryption algorithm used to encrypt the payload (plaintext). Examples include A128GCM, A256GCM, A128CBC-HS256, etc.</li> <li>typ: (Optional) Indicates the type of the token, typically JWT.</li> <li>cty: (Optional) Indicates the content type of the encrypted payload if it is something other than the default <code>application/json</code>.</li> <li>...</li> </ul> <p>Example</p> <p><pre><code>{\n  \"alg\": \"RSA-OAEP\",\n  \"enc\": \"A256GCM\"\n}\n</code></pre> This header specifies that the content encryption key is encrypted using the RSA-OAEP algorithm and the payload is encrypted using AES GCM with a 256-bit key.</p>"},{"location":"security/jwe/#jwe-encrypted-key","title":"JWE Encrypted Key","text":"<p>The CEK is the key used to encrypt the JWE payload. This key is encrypted using the algorithm specified in the <code>alg</code> parameter of the JOSE Header.</p> <ul> <li>If the <code>alg</code> is <code>RSA-OAEP</code>, the <code>CEK</code> is encrypted using the <code>RSA-OAEP</code> algorithm with the recipient\u2019s public key.</li> <li>If the <code>alg</code> is <code>A128KW</code> or <code>A256KW</code>, a symmetric key wrap is used.</li> </ul> <p>The CEK is different for each token, generated for one-time use. It must never be re-used.</p>"},{"location":"security/jwe/#initialization-vector-iv","title":"Initialization Vector (IV)","text":"<p>IV is random value that is used along with the encryption algorithm to ensure that the same plaintext will encrypt differently each time. The IV prevents patterns in the encrypted data, enhancing security.</p>"},{"location":"security/jwe/#ciphertext","title":"Ciphertext","text":"<p>The Ciphertext is the result of encrypting the plaintext (the payload data) with the <code>CEK</code> and the <code>enc</code> algorithm.</p>"},{"location":"security/jwe/#authentication-tag","title":"Authentication Tag","text":"<p>The Authentication Tag (also known as the Tag) is a base64url-encoded value that provides integrity and authenticity to the Ciphertext, Initialization Vector (IV), and Additional Authenticated Data (AAD). It is generated during the encryption process using algorithms like AES GCM.</p> <p>If any part of the JWE structure is altered after encryption, the decryption process will fail because the Authentication Tag will not match.</p>"},{"location":"security/jwe/#example-of-a-jwe","title":"Example of a JWE","text":"<p>Consider a scenario where we want to encrypt a message <code>Hello, World!</code> using JWE. Here is a simplified breakdown:</p> <ul> <li>Protected Header: <code>{\"alg\":\"RSA-OAEP\",\"enc\":\"A256GCM\"}</code></li> <li>Encrypted Key: Base64Url(encrypt(symmetric key with recipient\u2019s public key)) using RSA-OAEP algorithm</li> <li>Initialization Vector (IV): Base64Url(randomly generated IV)</li> <li>Ciphertext: Base64Url(encrypt(\u201cHello, World!\" with the CEK))</li> <li>Authentication Tag: Base64Url(GCM Tag)</li> </ul> <p>The final JWE might look something like this:</p> <pre><code>eyJhbGciOiJSU0EtT0FFUCIsImVuYyI6IkEyNTZHQ00ifQ.\ng_hE3pPLiSs9C60_WFQ-VP_mQ1BU00Z7Xg.\n48V1_ALb6US04U3b.\n5eym8mytxoXCBlYkhjBtkmmI.\nXFBoMYUZodetZdvTiFvSkQ\n</code></pre>"},{"location":"security/jwe/#jwe-decryption-flow","title":"JWE decryption flow","text":""},{"location":"security/jwt/","title":"JWT","text":""},{"location":"security/jwt/#1-what-is-jwt","title":"1. What is JWT?","text":"<p>JWT, or JSON Web Token, is an open standard used to share security information between two parties \u2014 a client and a server. Each JWT contains encoded JSON objects, including a set of claims. JWTs are signed using a cryptographic algorithm to ensure that the claims cannot be altered after the token is issued.</p>"},{"location":"security/jwt/#2-what-is-the-json-web-token-structure","title":"2. What is the JSON Web Token structure?","text":"<p>A JSON Web Token consists of 3 parts separated by a period.</p> <p><code>header.payload.signature</code></p> <p></p>"},{"location":"security/jwt/#21-header","title":"2.1 Header","text":"<p>JWT header consists of token type and algorithm used for signing and encoding. Algorithms can be HMAC, SHA256, RSA, HS256 or RS256.</p> <p>Eg:</p> <pre><code>{\n  \"typ\": \"JWT\",\n  \"alg\": \"HS256\"\n}\n</code></pre> <p>Then, this JSON is Base64Url encoded to form the first part of the JWT.</p>"},{"location":"security/jwt/#22-payload","title":"2.2 Payload","text":"<p>Payload consists of the session data called as claims. Below are some of the the standard claims that we can use,</p> <ul> <li>Issuer(iss)</li> <li>Subject (sub)</li> <li>Audience (aud)</li> <li>Expiration time (exp)</li> <li>Issued at (iat)</li> </ul> <p>Eg:</p> <pre><code>{\n  \"sub\": \"user10001\",\n  \"iat\": 1569302116\n}\n</code></pre> <p>Custom claims can also be included in the claim set. When using custom claim sets,</p> <ul> <li>Do not put large data in claim sets. Claim sets meant to be compact.</li> <li>Do not put sensitive informations since, JWT can be decoded easily.</li> </ul> <pre><code>{\n  \"sub\": \"user10001\",\n  \"iat\": 1569302116,\n  \"role\": \"admin\",\n  \"user_id\": \"user10001\"\n}\n</code></pre> <p>The payload is then Base64Url encoded to form the second part of the JSON Web Token.</p>"},{"location":"security/jwt/#23-signature","title":"2.3 Signature","text":"<p>To create the signature part you have to take the encoded header, the encoded payload, a secret, the algorithm specified in the header, and sign that.</p> <p>For example if you want to use the HMAC SHA256 algorithm, the signature will be created in the following way:</p> <pre><code>HMACSHA256(\n  base64UrlEncode(header) + \".\" +\n  base64UrlEncode(payload),\n  secret)\n</code></pre> <p>The signature is used to verify the message wasn't changed along the way, and, in the case of tokens signed with a private key, it can also verify that the sender of the JWT is who it says it is.</p>"},{"location":"security/jwt/#3-jwt-validation","title":"3. JWT Validation","text":"<p>Here is the basic flow of JWT authentication:</p> <p></p>"},{"location":"security/jwt/#31-how-tokens-are-signed","title":"3.1 How tokens are signed","text":"<p>Symmetric Signatures</p> <p>When a JWT is signed using a secret key, then it is called a symmetric signature. This type of signature is done when there is only one server that signs and validates the token. The same secret key is used to generate and validate the token. The token is signed using HMAC.</p> <p>HMAC stands for Hashing for Message Authentication Code. It\u2019s a message authentication code obtained by running a cryptographic hash function (like MD5, SHA1, and SHA256) over the data (to be authenticated) and a shared secret key</p> <p>Asymmetric Signatures</p> <p>This signature is suitable for distributed scenarios. Suppose there are multiple applications that can validate a given JWT. If we use a secret key to sign a JWT, then these applications will need that key to validate the token.</p> <p>It is not possible to share the secret key amongst all the applications, as it may get leaked. To solve this issue, asymmetric signing is done.</p>"},{"location":"security/jwt/#32-how-is-token-validated","title":"3.2 How is token validated","text":"<p>When a server receives a token, it fetches the header and payload from that token. It then uses the secret key or the public key (in the case of asymmetric signing) to generate the signature from the header and payload.</p> <p>If the generated signature matches the signature provided in the JWT, then it is considered to be valid.</p>"},{"location":"security/jwt/#33-claims-validations","title":"3.3 Claims validations","text":"<p>It is not sufficient to just validate the signature of the token. There are a few other security properties that need to be validated as discussed below:</p> <ol> <li>Check if the token is still valid. This can be validated through <code>exp</code> claim.</li> <li>Validate that the token is actually meant for you through the <code>aud</code> claim.</li> <li>Check if the token can be used at this time using the <code>nbf</code> claim. NBF stands for not before which means that this token should not be used before a particular time.</li> </ol>"},{"location":"security/jwt/#4-where-to-store-your-jwts","title":"4. Where to store your JWTs","text":"<p>Web Storage (local storage or session storage)</p> <p>If tokens are stored inside local storage, they are accessible by any script inside your page and thus vulnerable to cross-site scripting (XSS) attacks. Therefore, it is advisable to not store any sensitive information in the web storage.</p> <p>Cookies</p> <p>When cookies are used with the HttpOnly flag, they are good candidates for storing JWTs as they are immune to XSS attacks. Additionally, we can use the Secure cookie flag to guarantee that the cookie is only sent over HTTPS.</p>"},{"location":"security/jwt/#5-how-to-invalidate-a-jwt","title":"5. How to invalidate a JWT","text":"<p>If our JWT is somehow compromised, then we need some mechanism to invalidate our token. In session-based authentication, we can just close the browser and the session is destroyed. Unfortunately, it is not that easy in case of token-based authentication.</p> <p>One option that we have is that the server should change its secret key. However, this will invalidate the tokens for all the users. This approach should be used if the application owner thinks that the JWTs of a large number of users are stolen.</p> <p>Another option is maintaining a blacklist of all the invalid tokens. If a client suspects that the token is stolen, then they can logout from the browser. On doing this, the token for that user will be deleted from the browser storage and will be added to the blacklist present on the server. When the hacker sends a request with the stolen JWT, the server will find it in the blacklist and throw an unauthorized error.</p>"},{"location":"security/oauth2/","title":"OAuth2","text":""},{"location":"security/oauth2/#1-authentication-vs-authorization","title":"1. Authentication vs Authorization","text":""},{"location":"security/oauth2/#authentication","title":"Authentication","text":"<p>Authentication is the process of proving your own identity to third party service.</p> <p>So when we are trying to log in to Facebook or Google, we are required to first enter the email and password to verify our identity. This is what Authentication is.</p>"},{"location":"security/oauth2/#authorization","title":"Authorization","text":"<p>Authorization is the process of giving someone permission to do something or have something. It is done after successful Authentication.</p>"},{"location":"security/oauth2/#2-oauth2","title":"2. OAuth2","text":"<p>OAuth 2.0 is the industry-standard protocol for authorization and anyone can implement it.</p> <p>OAuth 2.0 terminology:</p> <ul> <li>Resource Owner: The user who owns the data that the client application wants to access.</li> <li>Client: The application that wants access to the user\u2019s data.</li> <li>Authorization Server: The authorization server authorizes the client to access a user\u2019s data by granting permission from the user.</li> <li>Resource Server: The system that holds the data that the client wants to access. In some cases, the resource server and authorization server are the same.</li> <li>Access token: An access token is the only key that the client can use to access the granted data by the user on the resource server.</li> </ul>"},{"location":"security/oauth2/#3-oauth2-flow","title":"3. OAuth2 flow","text":""},{"location":"security/oauth2/#authorization-code-flow","title":"Authorization Code Flow","text":"<p>The client begins the authorization sequence by redirecting the user to the authorization server with <code>response_type</code> set to <code>code</code> this tells the authorization server to respond with an authorization code.</p> <pre><code>https://accounts.google.com/o/oauth2/v2/auth?\n response_type=code&amp;\n client_id=your_client_id&amp;\n scope=profile%20contacts&amp;\n redirect_uri=https%3A//oauth2.example.com/code\n</code></pre> <p>The result of this request is an authorization code, which the client can exchange for an access token. The authorization code looks like this:</p> <pre><code>4/W7q7P51a-iMsCeLvIaQc6bYrgtp9\n</code></pre> <p>Why exchange code for a token?</p> <p>An access token is a secret piece of information that we don\u2019t want someone to access. If the client requests an access token directly and stores it in the browser, it can be stolen because browsers are not fully secure. Anyone can see the page source or potentially use dev tools to acquire the access token.</p> <p>To avoid exposure of the access token in the browser, the front end channel of the client gets the application code from the authorization server, then it sends the application code to the client\u2019s back end channel.</p> <p>Now to exchange this application code for an access token, a thing called client_secret is needed. The client_secret is only known by the client\u2019s back-end channel</p> <p>The request might look something like this:</p> <pre><code>POST /token HTTP/1.1\nHost: oauth2.googleapis.com\nContent-Type: application/x-www-form-urlencoded\n\ncode=4/W7q7P51a-iMsCeLvIaQc6bYrgtp9&amp;\nclient_id=your_client_id&amp;\nclient_secret=your_client_secret_only_known_by_server&amp;\nredirect_uri=https%3A//oauth2.example.com/code\n</code></pre> <p></p>"},{"location":"security/oauth2/#implicit-flow","title":"Implicit flow","text":"<p>The OAuth 2.0 implicit flow is used when you don\u2019t have a back end channel and your website is a static site that uses only the browser.</p> <p></p> <p>The client redirects the browser to the authorization server URI to start the authorization flow with <code>response_type</code> set to <code>token</code></p> <p>Implicit flow is considered less secure because the browser is responsible for managing the access token, so it could potentially be stolen. Still, it\u2019s widely used for single-page applications.</p>"},{"location":"security/openid_connect/","title":"OpenId Connect","text":"<p>OAuth 2.0 is designed only for authorization. It is used for granting access to data and features from one application to another. In OAuth, the client is given a token which it uses to access the data on the resource server, but it doesn\u2019t get to know anything about the user. OAuth was used for authentication as well, but since it was not designed for authentication it was extended further to support authentication.</p> <p>OpenID Connect is an identity layer on top of the OAuth 2.0 protocol. It extends OAuth 2.0 to standardize a way for authentication.</p> <ul> <li>OpenID is about verifying a person's identity (authentication).</li> <li>OAuth is about accessing a person's stuff (authorization).</li> <li>OpenID Connect does both.</li> </ul>"},{"location":"security/openid_connect/#identity-token","title":"Identity token","text":"<p>While discussing OAuth, we discussed the authorization code and access token. In the case of OpenId Connect, there is one more token that we can request. This token is called the identity token, which encodes the user\u2019s authentication information.</p> <p>In contrast to access tokens, which are only intended to be understood by the resource server, ID tokens are intended to be understood by the client application. The ID token contains the user information in JSON format. The JSON is wrapped into a JWT.</p> <p>An ID token is an artifact that proves that the user has been authenticated</p> <p>An ID token is encoded as a JSON Web Token (JWT)</p> <p>When a client receives the identity token, it should validate it first. The client must validate the following fields:</p> <ol> <li><code>iss</code> - Client must validate that the issuer of this token is the Authorization Server</li> <li><code>aud</code> - Client must validate that the token is meant for the client itself.</li> <li><code>exp</code> - Client must validate that the token is not expired.</li> </ol> <p>Here is some sample user information in the form of JSON present in an identity token.</p> <pre><code>{\n  \"iss\": \"https://server.example.com\",\n  \"sub\": \"24400320\",\n  \"aud\": \"s6BhdRkqt3\",\n  \"nonce\": \"n-0S6_WzA2Mj\",\n  \"exp\": 1311281970,\n  \"iat\": 1311280970,\n  \"auth_time\": 1311280969,\n  \"acr\": \"urn:mace:incommon:iap:silver\"\n}\n</code></pre> <ul> <li><code>iat</code>: The iat claim identifies the time at which the JWT was issued.</li> <li><code>auth_time</code>: Time when the End-User authentication occurred.</li> <li><code>nonce</code>: ID token requests may come with a nonce request parameter to protect from replay attacks.</li> </ul> <p>The ID token may have additional information about the user, such as their email address, picture, birthday, and so on.</p> <p>ID token vs Access token</p> <p></p>"},{"location":"security/openid_connect/#authorization-code-flow-for-authentication","title":"Authorization Code Flow for Authentication","text":"<p>The Authorization code flow for OpenID Connect is similar to the Authorization Code Flow. The only difference is the change in the value of the scope field. It must contain openid as one of the values, followed by other scope values based on what type of user data the client wants.</p> <p></p>"},{"location":"security/openid_connect/#openid-connect-vs-oauth2","title":"OpenId Connect vs OAuth2","text":"<ul> <li>OpenID Connect are authentication protocols while OAuth2 is an authorization protocol.</li> <li>The most significant difference between OpenID Connect and OAuth2 is the id_token. OIDC contains an id_token while OAuth2 does not.</li> <li>In OpenID Connect it contains user info endpoint while OAuth2 does not.</li> <li>OpenID Connect defines how to send a signed and encrypted request object where OAuth2 does not.</li> </ul>"},{"location":"security/web_security/","title":"Web Security","text":""},{"location":"security/web_security/#clickjacking","title":"Clickjacking","text":"<p>Clickjacking is a method of tricking website users into clicking on a harmful link, by disguising the link as something else.</p> <p>Demo</p>"},{"location":"security/web_security/#protection","title":"Protection","text":"<p>X-Frame-Options</p> <p>The <code>X-Frame-Options</code> HTTP header is an older standard that can be used to indicate whether or not a browser should be allowed to render a page in a <code>&lt;frame&gt;</code>, <code>&lt;iframe&gt;</code> or <code>&lt;object&gt;</code> tag. It was designed specifically to help protect against clickjacking, but has since between made obsolete by content security polices.</p> <p>The supported values are:</p> <ul> <li><code>DENY</code>: This web page cannot be embedded anywhere. This is the highest level of protection as it doesn\u2019t allow anyone to embed our content.</li> <li><code>SAMEORIGIN</code>: Only pages from the same domain as the current one can embed this page. This means that <code>example.com/embedder</code> can load <code>example.com/embedded</code> so long as its policy is set to <code>SAMEORIGIN</code>. This is a more relaxed policy that allows owners of a particular website to embed their own pages across their application.</li> <li><code>ALLOW-FROM uri</code>: Embedding is allowed from the specified URI. We could, for example, let an external, authorized website embed our content by using <code>ALLOW-FROM https://external.com</code>. This is generally used when you intend to allow a third party to embed your content through an iframe.</li> </ul> <p>Content Security Policy</p> <p>The <code>Content-Security-Policy</code> HTTP header is part of the HTML5 standard, and provides a broader range of protection than the <code>X-Frame-Options</code> header (which it replaces). It is designed in such a way that website authors can enumerate individual domains from which resources (like scripts, stylesheets, and fonts) can be loaded, and also domains that are permitted to embed a page.</p> <p>To control where your site can be embedded, use the <code>frame-ancestors</code> directive:</p> <ul> <li><code>Content-Security-Policy: frame-ancestors 'none'</code></li> <li><code>Content-Security-Policy: frame-ancestors 'self'</code></li> <li><code>Content-Security-Policy: frame-ancestors *uri*</code></li> </ul>"},{"location":"security/web_security/#cross-site-scripting-xss-and-reflected-xss","title":"Cross-site scripting (XSS) and Reflected XSS","text":"<p>XSS</p> <p>Reflected XSS</p>"},{"location":"security/web_security/#protection_1","title":"Protection","text":"<p>Escape Dynamic Content</p> <p>Content Security Policy</p> <p>XSS attacks rely on the attacker being able to run malicious scripts on a user\u2019s web page - either by injecting inline"}]}